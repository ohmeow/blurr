{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_slow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data.core\n",
    "\n",
    "> This module contains the core bits required to use the fastai DataBlock API and/or mid-level data processing pipelines to organize your data in a way modelable by Hugging Face transformer implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os, inspect\n",
    "from dataclasses import dataclass\n",
    "from functools import reduce, partial\n",
    "from typing import Any, Callable, List, Optional, Union, Type\n",
    "\n",
    "from fastcore.all import *\n",
    "from fastai.data.block import TransformBlock\n",
    "from fastai.data.core import Datasets, DataLoader, DataLoaders, TfmdDL\n",
    "from fastai.imports import *\n",
    "from fastai.losses import CrossEntropyLossFlat\n",
    "from fastai.text.data import SortedDL\n",
    "from fastai.torch_core import *\n",
    "from fastai.torch_imports import *\n",
    "from transformers import DataCollatorWithPadding, logging, PretrainedConfig, PreTrainedTokenizerBase, PreTrainedModel\n",
    "\n",
    "from blurr.utils import BLURR\n",
    "\n",
    "logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we're running with at the time this documentation was generated:\n",
      "torch: 1.10.1+cu111\n",
      "fastai: 2.5.3\n",
      "transformers: 4.15.0\n"
     ]
    }
   ],
   "source": [
    "# hide_input\n",
    "import pdb\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from fastai.data.block import CategoryBlock, ColReader, ColSplitter, DataBlock\n",
    "from fastai.data.external import untar_data, URLs\n",
    "from fastcore.test import *\n",
    "from nbverbose.showdoc import show_doc\n",
    "\n",
    "from blurr.utils import print_versions\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(\"What we're running with at the time this documentation was generated:\")\n",
    "print_versions(\"torch fastai transformers\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "# cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f\"Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid-level API: Base tokenization, batch transform, and DataBlock methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class HF_BaseInput(TensorBase):\n",
    "    \"\"\"The base represenation of your inputs; used by the various fastai `show` methods\"\"\"\n",
    "\n",
    "    def show(\n",
    "        self,\n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # The \"context\" associated to the current `show_batch/results` call\n",
    "        ctx=None,\n",
    "        # Any truncation you want to apply to the decoded tokenized inputs\n",
    "        trunc_at: int = None,\n",
    "        # A decoded string of your tokenized inputs (input_ids)\n",
    "    ) -> str:\n",
    "        input_ids = self.cpu().numpy()\n",
    "        decoded_input = str(hf_tokenizer.decode(input_ids, skip_special_tokens=True))[:trunc_at]\n",
    "\n",
    "        return show_title(decoded_input, ctx=ctx, label=\"text\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `HF_BaseInput` object is returned from the decodes method of `HF_AfterBatchTransform` as a means to customize @typedispatched functions like `DataLoaders.show_batch` and `Learner.show_results`. It uses the \"input_ids\" of a Hugging Face object as the representative tensor for `show` methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class HF_BeforeBatchTransform(Transform):\n",
    "    \"\"\"\n",
    "    Handles everything you need to assemble a mini-batch of inputs and targets, as well as \n",
    "    decode the dictionary produced as a byproduct of the tokenization process in the `encodes` method.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (e.b., bert, bart, etc..)\n",
    "        hf_arch: str,\n",
    "        # A specific configuration instance you want to use\n",
    "        hf_config: PretrainedConfig,\n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # A Hugging Face model\n",
    "        hf_model: PreTrainedModel,\n",
    "        # The token ID that should be ignored when calculating the loss\n",
    "        ignore_token_id=CrossEntropyLossFlat().ignore_index,\n",
    "        # To control the length of the padding/truncation. It can be an integer or None,\n",
    "        # in which case it will default to the maximum length the model can accept. If the model has no\n",
    "        # specific maximum input length, truncation/padding to max_length is deactivated.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        max_length: int = None,\n",
    "        # To control the `padding` applied to your `hf_tokenizer` during tokenization. If None, will default to\n",
    "        # `False` or `'do_not_pad'.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        padding: Union[bool, str] = True,\n",
    "        # To control `truncation` applied to your `hf_tokenizer` during tokenization. If None, will default to\n",
    "        # `False` or `do_not_truncate`.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        truncation: Union[bool, str] = True,\n",
    "        # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. Set this to `True`\n",
    "        # if your inputs are pre-tokenized (not numericalized)\n",
    "        is_split_into_words: bool = False,\n",
    "        # Any other keyword arguments you want included when using your `hf_tokenizer` to tokenize your inputs\n",
    "        tok_kwargs: dict = {},\n",
    "        # Keyword arguments to apply to `HF_BeforeBatchTransform`\n",
    "        **kwargs\n",
    "    ):\n",
    "        store_attr(self=self, names=\"hf_arch, hf_config, hf_tokenizer, hf_model\")\n",
    "        store_attr(self=self, names=\"max_length, padding, truncation, is_split_into_words, ignore_token_id, tok_kwargs\")\n",
    "        store_attr(self=self, names=\"kwargs\")\n",
    "\n",
    "    def encodes(self, samples, return_batch_encoding=False):\n",
    "        \"\"\"\n",
    "        This method peforms on-the-fly, batch-time tokenization of your data. In other words, your raw inputs\n",
    "        are tokenized as needed for each mini-batch of data rather than requiring pre-tokenization of your full\n",
    "        dataset ahead of time.\n",
    "        \"\"\"\n",
    "        samples = L(samples)\n",
    "\n",
    "        # grab inputs\n",
    "        if is_listy(samples[0][0]) and not self.is_split_into_words:\n",
    "            inps = list(zip(samples.itemgot(0, 0), samples.itemgot(0, 1)))\n",
    "        else:\n",
    "            inps = samples.itemgot(0).items\n",
    "\n",
    "        # tokenize\n",
    "        tok_d = self.hf_tokenizer(\n",
    "            inps,\n",
    "            max_length=self.max_length,\n",
    "            padding=self.padding,\n",
    "            truncation=self.truncation,\n",
    "            is_split_into_words=self.is_split_into_words,\n",
    "            return_tensors=\"pt\",\n",
    "            **self.tok_kwargs\n",
    "        )\n",
    "\n",
    "        # update the samples with tokenized inputs (e.g. input_ids, attention_mask, etc...), ensureing that if\n",
    "        # \"overflow_to_sample_mapping = True\" we include each sample chunk\n",
    "        d_keys = tok_d.keys()\n",
    "        updated_samples = []\n",
    "        if \"overflow_to_sample_mapping\" in d_keys:\n",
    "            for idx, seq_idx in enumerate(tok_d[\"overflow_to_sample_mapping\"]):\n",
    "                s = (*[{k: tok_d[k][idx] for k in d_keys}], *samples[seq_idx][1:])\n",
    "                updated_samples.append(s)\n",
    "        else:\n",
    "            updated_samples = [(*[{k: tok_d[k][idx] for k in d_keys}], *sample[1:]) for idx, sample in enumerate(samples)]\n",
    "\n",
    "        if return_batch_encoding:\n",
    "            return updated_samples, tok_d\n",
    "\n",
    "        return updated_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`HF_BeforeBatchTransform` was inspired by this [article](https://docs.fast.ai/tutorial.transformers.html).\n",
    "\n",
    "Inputs can come in as a string or a list of tokens, the later being for tasks like Named Entity Recognition (NER), where you want to predict the label of each token.\n",
    "\n",
    "**On-the-fly Batch-Time Tokenization**: \n",
    "\n",
    "The previous version of the library performed the tokenization/numericalization as a type transform when the raw data was read, and included a couple batch transforms to prepare the data for collation (e.g., to be made into a mini-batch). With this update, everything is done in a single batch transform.  \n",
    "\n",
    "Why?  Part of the inspiration had to do with the mechanics of the huggingrace tokenizer, in particular how by default it returns a collated mini-batch of data given a list of sequences. And where do we get a list of examples with fastai? In the batch transforms!  So I thought, hey, why not do everything dynamically at batch time?  And with a bit of tweaking, I got everything to work pretty well.  The result is *less code*, *faster mini-batch creation*, *less RAM utilization* and time spent tokenizing (really helps with very large datasets), and *more flexibility*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class HF_AfterBatchTransform(Transform):\n",
    "    \"\"\"A class used to cast your inputs into something understandable in fastai `show` methods\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # The return type your decoded inputs should be cast too (used by methods such as `show_batch`)\n",
    "        input_return_type: Type = HF_BaseInput,\n",
    "    ):\n",
    "        store_attr(self=self, names=\"hf_tokenizer, input_return_type\")\n",
    "\n",
    "    def decodes(\n",
    "        self,\n",
    "        # The encoded samples for your batch. `input_ids` will be pulled out of your dictionary of Hugging Face\n",
    "        # inputs, cast to `self.input_return_type` and returned for methods such as `show_batch`\n",
    "        encoded_samples: Type,\n",
    "    ):\n",
    "        \"\"\"Returns the proper object and data for show related fastai methods\"\"\"\n",
    "        if isinstance(encoded_samples, dict):\n",
    "            return self.input_return_type(encoded_samples[\"input_ids\"], hf_tokenizer=self.hf_tokenizer)\n",
    "        return encoded_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With fastai 2.1.5, before batch transforms no longer have a `decodes` method ... and so, I've introduced a standard batch transform here, `HF_AfterBatchTransform`, that will do the decoding for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def blurr_sort_func(\n",
    "    example,\n",
    "    # A Hugging Face tokenizer\n",
    "    hf_tokenizer: PreTrainedTokenizerBase,\n",
    "    # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. Set this to `True`\n",
    "    # if your inputs are pre-tokenized (not numericalized)\n",
    "    is_split_into_words: bool = False,\n",
    "    # Any other keyword arguments you want to include during tokenization\n",
    "    tok_kwargs: dict = {},\n",
    "):\n",
    "    \"\"\"This method is used by the `SortedDL` to ensure your dataset is sorted *after* tokenization\"\"\"\n",
    "    if is_split_into_words:\n",
    "        return len(example[0])\n",
    "    return len(hf_tokenizer.tokenize(example[0], **tok_kwargs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@delegates(TfmdDL)\n",
    "class OverflowDL(SortedDL):\n",
    "    def __init__(self, dataset, sort_func=None, res=None, overflow_map_key=\"overflow_to_sample_mapping\", **kwargs):\n",
    "        super().__init__(dataset, sort_func=sort_func, res=res, **kwargs)\n",
    "        self.overflow_map_key = overflow_map_key\n",
    "        self.batch_items = None\n",
    "\n",
    "    def create_batches(self, samps):\n",
    "        if self.dataset is not None:\n",
    "            self.it = iter(self.dataset)\n",
    "        res = filter(lambda o: o is not None, map(self.do_item, samps))\n",
    "\n",
    "        for b in map(self.do_batch, self.chunkify(res)):\n",
    "            while self._n_batch_items() >= self.bs:\n",
    "                yield self._get_batch()\n",
    "\n",
    "    def do_batch(self, b):\n",
    "        b = super().do_batch(b)\n",
    "        self._add_batch(b)\n",
    "\n",
    "    def _add_batch(self, b):\n",
    "        if not self.batch_items:\n",
    "            self.batch_items = b\n",
    "        else:\n",
    "            for i in range(len(b)):\n",
    "                if isinstance(b[i], dict):\n",
    "                    for k in self.batch_items[i].keys():\n",
    "                        self.batch_items[i][k] = torch.cat([self.batch_items[i][k], b[i][k]])\n",
    "                else:\n",
    "                    self.batch_items[i].data = torch.cat([self.batch_items[i], b[i]])\n",
    "\n",
    "        # update \"n\" to reflect the additional samples\n",
    "        overflow_map = b[0][self.overflow_map_key].numpy()\n",
    "        self.n += np.sum([i - 1 for i in Counter(overflow_map).values()])\n",
    "\n",
    "    def _get_batch(self):\n",
    "        chunked_batch = []\n",
    "\n",
    "        for i in range(len(self.batch_items)):\n",
    "            if isinstance(self.batch_items[i], dict):\n",
    "                chunked_d = {}\n",
    "                for k in self.batch_items[i].keys():\n",
    "                    chunked_d[k] = self.batch_items[i][k][: self.bs]\n",
    "                    self.batch_items[i][k] = self.batch_items[i][k][self.bs :]\n",
    "\n",
    "                chunked_batch.append(chunked_d)\n",
    "            else:\n",
    "                chunked_batch.append(self.batch_items[i][: self.bs])\n",
    "                self.batch_items[i].data = self.batch_items[i][self.bs :]\n",
    "\n",
    "        return tuplify(chunked_batch)\n",
    "\n",
    "    def _n_batch_items(self):\n",
    "        return len(self.batch_items[0][self.overflow_map_key]) if self.batch_items else 0\n",
    "\n",
    "    def _one_pass(self):\n",
    "        self.do_batch([self.do_item(0)])\n",
    "        b = self._get_batch()\n",
    "        if self.device is not None:\n",
    "            b = to_device(b, self.device)\n",
    "        its = self.after_batch(b)\n",
    "        self._n_inp = 1 if not isinstance(its, (list, tuple)) or len(its) == 1 else len(its) - 1\n",
    "        self._types = explode_types(its)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class HF_TextBlock(TransformBlock):\n",
    "    \"\"\"The core `TransformBlock` to prepare your data for training in Blurr with fastai's `DataBlock` API\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (not required if passing in an\n",
    "        # instance of `HF_BeforeBatchTransform` to `before_batch_tfm`)\n",
    "        hf_arch: str = None,\n",
    "        # A Hugging Face configuration object (not required if passing in an\n",
    "        # instance of `HF_BeforeBatchTransform` to `before_batch_tfm`)\n",
    "        hf_config: PretrainedConfig = None,\n",
    "        # A Hugging Face tokenizer (not required if passing in an\n",
    "        # instance of `HF_BeforeBatchTransform` to `before_batch_tfm`)\n",
    "        hf_tokenizer: PreTrainedTokenizerBase = None,\n",
    "        # A Hugging Face model (not required if passing in an\n",
    "        # instance of `HF_BeforeBatchTransform` to `before_batch_tfm`)\n",
    "        hf_model: PreTrainedModel = None,\n",
    "        # The token ID that should be ignored when calculating the loss\n",
    "        ignore_token_id=CrossEntropyLossFlat().ignore_index,\n",
    "        # The before batch transform you want to use to tokenize your raw data on the fly\n",
    "        # (defaults to an instance of `HF_BeforeBatchTransform` created using the Hugging Face objects defined above)\n",
    "        before_batch_tfm: HF_BeforeBatchTransform = None,\n",
    "        # The batch_tfms to apply to the creation of your DataLoaders,\n",
    "        # (defaults to HF_AfterBatchTransform created using the Hugging Face objects defined above)\n",
    "        after_batch_tfm: HF_AfterBatchTransform = None,\n",
    "        # To control the length of the padding/truncation. It can be an integer or None,\n",
    "        # in which case it will default to the maximum length the model can accept. If the model has no\n",
    "        # specific maximum input length, truncation/padding to max_length is deactivated.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        max_length: int = None,\n",
    "        # To control the `padding` applied to your `hf_tokenizer` during tokenization. If None, will default to\n",
    "        # `False` or `'do_not_pad'.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        padding: Union[bool, str] = True,\n",
    "        # To control `truncation` applied to your `hf_tokenizer` during tokenization. If None, will default to\n",
    "        # `False` or `do_not_truncate`.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        truncation: Union[bool, str] = True,\n",
    "        # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. Set this to `True`\n",
    "        # if your inputs are pre-tokenized (not numericalized)\n",
    "        is_split_into_words: bool = False,\n",
    "        # The return type your decoded inputs should be cast too (used by methods such as `show_batch`)\n",
    "        input_return_type: Type = HF_BaseInput,\n",
    "        # The type of `DataLoader` you want created (defaults to `SortedDL`)\n",
    "        dl_type: DataLoader = None,\n",
    "        # Any keyword arguments you want applied to your before batch tfm\n",
    "        before_batch_kwargs: dict = {},\n",
    "        # Any keyword arguments you want applied to your after batch tfm (or referred to in fastai as `batch_tfms`)\n",
    "        after_batch_kwargs: dict = {},\n",
    "        # Any keyword arguments you want your Hugging Face tokenizer to use during tokenization\n",
    "        tok_kwargs: dict = {},\n",
    "        # Any keyword arguments you want to have applied with generating text\n",
    "        text_gen_kwargs: dict = {},\n",
    "        # Any keyword arguments you want applied to `HF_TextBlock`\n",
    "        **kwargs\n",
    "    ):\n",
    "        if (not all([hf_arch, hf_config, hf_tokenizer, hf_model])) and before_batch_tfm is None:\n",
    "            raise ValueError(\n",
    "                \"\"\"You must supply the Hugging Face architecture, config, tokenizer, and model\n",
    "                - or - an instances of HF_BeforeBatchTransform\"\"\"\n",
    "            )\n",
    "\n",
    "        if before_batch_tfm is None:\n",
    "            # if allowing overflow, if we have to ensure mixed batch items are the same shape\n",
    "            if \"return_overflowing_tokens\" in tok_kwargs:\n",
    "                padding = \"max_length\"\n",
    "\n",
    "            before_batch_tfm = HF_BeforeBatchTransform(\n",
    "                hf_arch,\n",
    "                hf_config,\n",
    "                hf_tokenizer,\n",
    "                hf_model,\n",
    "                ignore_token_id=ignore_token_id,\n",
    "                max_length=max_length,\n",
    "                padding=padding,\n",
    "                truncation=truncation,\n",
    "                is_split_into_words=is_split_into_words,\n",
    "                tok_kwargs=tok_kwargs.copy(),\n",
    "                **before_batch_kwargs.copy()\n",
    "            )\n",
    "\n",
    "        if after_batch_tfm is None:\n",
    "            after_batch_tfm = HF_AfterBatchTransform(\n",
    "                hf_tokenizer=before_batch_tfm.hf_tokenizer, input_return_type=input_return_type, **after_batch_kwargs.copy()\n",
    "            )\n",
    "\n",
    "        if dl_type is None:\n",
    "            dl_sort_func = partial(\n",
    "                blurr_sort_func,\n",
    "                hf_tokenizer=before_batch_tfm.hf_tokenizer,\n",
    "                is_split_into_words=before_batch_tfm.is_split_into_words,\n",
    "                tok_kwargs=before_batch_tfm.tok_kwargs.copy(),\n",
    "            )\n",
    "\n",
    "            # `OverflowDL` is a `DataLoader` that knows how to serve batches of items that are created on the fly as a result\n",
    "            # of asking the tokenizer to return an input in chunks if the lenght > max_length\n",
    "            if \"return_overflowing_tokens\" in before_batch_tfm.tok_kwargs:\n",
    "                dl_type = partial(OverflowDL, sort_func=dl_sort_func)\n",
    "            else:\n",
    "                partial(SortedDL, sort_func=dl_sort_func)\n",
    "\n",
    "        # set the TransformBlock's Hugging Face face objects\n",
    "        self.hf_arch = before_batch_tfm.hf_arch\n",
    "        self.hf_config = before_batch_tfm.hf_config\n",
    "        self.hf_tokenizer = before_batch_tfm.hf_tokenizer\n",
    "        self.hf_model = before_batch_tfm.hf_model\n",
    "\n",
    "        return super().__init__(dl_type=dl_type, dls_kwargs={\"before_batch\": before_batch_tfm}, batch_tfms=after_batch_tfm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic wrapper that links defaults transforms for the data block API\n",
    "\n",
    "`HF_TextBlock` is designed with sensible defaults to minimize user effort in defining their transforms pipeline. It handles setting up your `HF_BeforeBatchTransform` and `HF_AfterBatchTransform` transforms regardless of data source (e.g., this will work with files, DataFrames, whatever). \n",
    "\n",
    "You must either pass in your own instance of a `HF_BeforeBatchTransform` class or the Hugging Face architecture and tokenizer via the `hf_arch` and `hf_tokenizer` (the other args are optional)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-level API: For working with PyTorch and/or fast.ai Datasets & DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a low-level API for working with basic PyTorch/Hugging Face Datasets and DataLoaders that allows you to get back fast.ai specific features such as `show_batch`, `show_results`, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@dataclass\n",
    "class BlurrBatchCreator:\n",
    "    \"\"\"\n",
    "    A class that can be assigned to a `TfmdDL.create_batch` method; used to in Blurr's low-level API\n",
    "    to create batches that can be used in the Blurr library\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Your Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # Defaults to use Hugging Face's DataCollatorWithPadding(tokenizer=hf_tokenizer)\n",
    "        data_collator: Type = None,\n",
    "    ):\n",
    "        self.hf_tokenizer = hf_tokenizer\n",
    "        self.data_collator = data_collator if (data_collator) else DataCollatorWithPadding(tokenizer=hf_tokenizer)\n",
    "\n",
    "    def __call__(self, features):  # A mini-batch (list of examples to run through your model)\n",
    "        \"\"\"This method will collate your data using `self.data_collator` and add a target element to the\n",
    "        returned tuples if `labels` are defined as is the case when most Hugging Face datasets\n",
    "        \"\"\"\n",
    "        batch = self.data_collator(features)\n",
    "        if isinstance(features[0], dict):\n",
    "            return dict(batch), batch[\"labels\"] if (\"labels\" in features[0]) else dict(batch)\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BlurrBatchTransform(HF_AfterBatchTransform):\n",
    "    \"\"\"A class used to cast your inputs into something understandable in fastai `show` methods\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (not required if passing in an\n",
    "        # instance of `HF_BeforeBatchTransform` to `before_batch_tfm`)\n",
    "        hf_arch: str = None,\n",
    "        # A Hugging Face configuration object (not required if passing in an\n",
    "        # instance of `HF_BeforeBatchTransform` to `before_batch_tfm`)\n",
    "        hf_config: PretrainedConfig = None,\n",
    "        # A Hugging Face tokenizer (not required if passing in an\n",
    "        # instance of `HF_BeforeBatchTransform` to `before_batch_tfm`)\n",
    "        hf_tokenizer: PreTrainedTokenizerBase = None,\n",
    "        # A Hugging Face model (not required if passing in an\n",
    "        # instance of `HF_BeforeBatchTransform` to `before_batch_tfm`)\n",
    "        hf_model: PreTrainedModel = None,\n",
    "        # The token ID to ignore when calculating loss/metrics\n",
    "        ignore_token_id: int = CrossEntropyLossFlat().ignore_index,\n",
    "        # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. Set this to `True`\n",
    "        # if your inputs are pre-tokenized (not numericalized)\n",
    "        is_split_into_words: bool = False,\n",
    "        \n",
    "        # Any other keyword arguments you want included when using your `hf_tokenizer` to tokenize your inputs\n",
    "        tok_kwargs: dict = {},\n",
    "        # Any text generation keyword arguments\n",
    "        text_gen_kwargs: dict = {},\n",
    "        # The return type your decoded inputs should be cast too (used by methods such as `show_batch`)\n",
    "        input_return_type: Type = HF_BaseInput,\n",
    "        # Any other keyword arguments you need to pass to `HF_AfterBatchTransform`\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(hf_tokenizer=hf_tokenizer, input_return_type=input_return_type)\n",
    "\n",
    "        store_attr(self=self, names=\"hf_arch, hf_config, hf_model, ignore_token_id\")\n",
    "        store_attr(self=self, names=\"tok_kwargs, text_gen_kwargs, is_split_into_words, kwargs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@delegates()\n",
    "class BlurrDataLoader(TfmdDL):\n",
    "    \"\"\"A class that makes creating a fast.ai `DataLoader` that works with Blurr\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # A standard PyTorch Dataset\n",
    "        dataset: Union[torch.utils.data.dataset.Dataset, Datasets],\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (not required if passing in an\n",
    "        # instance of `HF_BeforeBatchTransform` to `before_batch_tfm`)\n",
    "        hf_arch: str,\n",
    "        # A Hugging Face configuration object (not required if passing in an instance of `HF_BeforeBatchTransform`\n",
    "        # to `before_batch_tfm`)\n",
    "        hf_config: PretrainedConfig,\n",
    "        # A Hugging Face tokenizer (not required if passing in an instance of `HF_BeforeBatchTransform` to\n",
    "        # `before_batch_tfm`)\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # A Hugging Face model (not required if passing in an instance of `HF_BeforeBatchTransform` to\n",
    "        # `before_batch_tfm`)\n",
    "        hf_model: PreTrainedModel,\n",
    "        # An instance of `BlurrBatchCreator` or equivalent\n",
    "        batch_creator: BlurrBatchCreator = None,\n",
    "        # The batch_tfm used to decode Blurr batches (default: HF_AfterBatchTransform)\n",
    "        batch_tfm: BlurrBatchTransform = None,\n",
    "        # (optional) A preprocessing function that will be applied to your dataset\n",
    "        preproccesing_func: Callable[\n",
    "            [Union[torch.utils.data.dataset.Dataset, Datasets], PreTrainedTokenizerBase, PreTrainedModel],\n",
    "            Union[torch.utils.data.dataset.Dataset, Datasets],\n",
    "        ] = None,\n",
    "        # Keyword arguments to be applied to your `batch_tfm`\n",
    "        batch_tfm_kwargs: dict = {},\n",
    "        # Keyword arguments to be applied to `BlurrDataLoader`\n",
    "        **kwargs\n",
    "    ):\n",
    "        if preproccesing_func:\n",
    "            dataset = preproccesing_func(dataset, hf_tokenizer, hf_model)\n",
    "\n",
    "        if \"create_batch\" in kwargs:\n",
    "            kwargs.pop(\"create_batch\")\n",
    "        if not batch_creator:\n",
    "            batch_creator = BlurrBatchCreator(hf_tokenizer=hf_tokenizer)\n",
    "\n",
    "        if \"after_batch\" in kwargs:\n",
    "            kwargs.pop(\"after_batch\")\n",
    "        if not batch_tfm:\n",
    "            batch_tfm = BlurrBatchTransform(hf_arch, hf_config, hf_tokenizer, hf_model, **batch_tfm_kwargs.copy())\n",
    "\n",
    "        super().__init__(dataset=dataset, create_batch=batch_creator, after_batch=batch_tfm, **kwargs)\n",
    "        store_attr(self=self, names=\"hf_arch, hf_config, hf_tokenizer, hf_model\")\n",
    "\n",
    "    def new(\n",
    "        self,\n",
    "        # A standard PyTorch and fastai dataset\n",
    "        dataset: Union[torch.utils.data.dataset.Dataset, Datasets] = None,\n",
    "        # The class you want to create an instance of (will be \"self\" if None)\n",
    "        cls: Type = None,\n",
    "        #  Any additional keyword arguments you want to pass to the __init__ method of `cls`\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"We have to override the new method in order to add back the Hugging Face objects in this factory \n",
    "        method (called for example in places like `show_results`). With the exception of the additions to the kwargs\n",
    "        dictionary, the code below is pulled from the `DataLoaders.new` method as is.\n",
    "        \"\"\"\n",
    "        if dataset is None:\n",
    "            dataset = self.dataset\n",
    "        if cls is None:\n",
    "            cls = type(self)\n",
    "\n",
    "        cur_kwargs = dict(\n",
    "            dataset=dataset,\n",
    "            num_workers=self.fake_l.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            timeout=self.timeout,\n",
    "            bs=self.bs,\n",
    "            shuffle=self.shuffle,\n",
    "            drop_last=self.drop_last,\n",
    "            indexed=self.indexed,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "        for n in self._methods:\n",
    "            o = getattr(self, n)\n",
    "            if not isinstance(o, MethodType):\n",
    "                cur_kwargs[n] = o\n",
    "\n",
    "        # we need to add these arguments back in (these, after_batch, and create_batch will go in as kwargs)\n",
    "        kwargs[\"hf_arch\"] = self.hf_arch\n",
    "        kwargs[\"hf_config\"] = self.hf_config\n",
    "        kwargs[\"hf_tokenizer\"] = self.hf_tokenizer\n",
    "        kwargs[\"hf_model\"] = self.hf_model\n",
    "\n",
    "        return cls(**merge(cur_kwargs, kwargs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility methods for getting blurr transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_blurr_tfm(\n",
    "    # A list of transforms (e.g., dls.after_batch, dls.before_batch, etc...)\n",
    "    tfms_list: Pipeline,\n",
    "    # The transform to find\n",
    "    tfm_class: Transform = HF_BeforeBatchTransform,\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a fastai DataLoaders batch transforms, this method can be used to get at a transform\n",
    "    instance used in your Blurr DataBlock\n",
    "    \"\"\"\n",
    "    return next(filter(lambda el: issubclass(type(el), tfm_class), tfms_list), None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"get_blurr_tfm\" class=\"doc_header\"><code>get_blurr_tfm</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>get_blurr_tfm</code>(**`tfms_list`**:`Pipeline`, **`tfm_class`**:`Transform`=*`HF_BeforeBatchTransform`*)\n",
       "\n",
       "Given a fastai DataLoaders batch transforms, this method can be used to get at a transform\n",
       "instance used in your Blurr DataBlock\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`tfms_list`** : *`<class 'fastcore.transform.Pipeline'>`*\t<p>A list of transforms (e.g., dls.after_batch, dls.before_batch, etc...)</p>\n",
       "\n",
       "\n",
       " - **`tfm_class`** : *`<class 'fastcore.transform.Transform'>`*, *optional*\t<p>The transform to find</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(get_blurr_tfm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def first_blurr_tfm(\n",
    "    dls: DataLoaders,  # Your fast.ai `DataLoaders\n",
    "    before_batch_tfm_class: Transform = HF_BeforeBatchTransform,  # The before_batch transform to look for\n",
    "    blurr_batch_tfm_class: Transform = BlurrBatchTransform,  # The after_batch (or batch_tfm) to look for\n",
    "):\n",
    "    \"\"\"\n",
    "    This convenience method will find the first Blurr transform required for methods such as \n",
    "    `show_batch` and `show_results`. The returned transform should have everything you need to properly\n",
    "    decode and 'show' your Hugging Face inputs/targets\n",
    "    \"\"\"\n",
    "    # try our befor_batch tfms (this will be used if you're using the mid-level DataBlock API)\n",
    "    tfm = get_blurr_tfm(dls.before_batch, tfm_class=before_batch_tfm_class)\n",
    "    if tfm:\n",
    "        return tfm\n",
    "\n",
    "    # try our after_batch tfms (this will be used if you're using the low-level Blurr data API)\n",
    "    return get_blurr_tfm(dls.after_batch, tfm_class=blurr_batch_tfm_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"first_blurr_tfm\" class=\"doc_header\"><code>first_blurr_tfm</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>first_blurr_tfm</code>(**`dls`**:`DataLoaders`, **`before_batch_tfm_class`**:`Transform`=*`HF_BeforeBatchTransform`*, **`blurr_batch_tfm_class`**:`Transform`=*`BlurrBatchTransform`*)\n",
       "\n",
       "This convenience method will find the first Blurr transform required for methods such as \n",
       "`show_batch` and `show_results`. The returned transform should have everything you need to properly\n",
       "decode and 'show' your Hugging Face inputs/targets\n",
       "\n",
       "**Parameters:**\n",
       "\n",
       "\n",
       " - **`dls`** : *`<class 'fastai.data.core.DataLoaders'>`*\t<p>Your fast.ai `DataLoaders</p>\n",
       "\n",
       "\n",
       " - **`before_batch_tfm_class`** : *`<class 'fastcore.transform.Transform'>`*, *optional*\t<p>The before_batch transform to look for</p>\n",
       "\n",
       "\n",
       " - **`blurr_batch_tfm_class`** : *`<class 'fastcore.transform.Transform'>`*, *optional*\t<p>The after_batch (or batch_tfm) to look for</p>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(first_blurr_tfm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base `show_batch` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@typedispatch\n",
    "def show_batch(\n",
    "    # This typedispatched `show_batch` will be called for `HF_BaseInput` typed inputs\n",
    "    x: HF_BaseInput,\n",
    "    # Your targets\n",
    "    y,\n",
    "    # Your raw inputs/targets\n",
    "    samples,\n",
    "    # Your `DataLoaders`. This is required so as to get at the Hugging Face objects for\n",
    "    # decoding them into something understandable\n",
    "    dataloaders,\n",
    "    # Your `show_batch` context\n",
    "    ctxs=None,\n",
    "    # The maximum number of items to show\n",
    "    max_n=6,\n",
    "    # Any truncation your want applied to your decoded inputs\n",
    "    trunc_at=None,\n",
    "    # Any other keyword arguments you want applied to `show_batch`\n",
    "    **kwargs,\n",
    "):\n",
    "    # grab our tokenizer\n",
    "    tfm = first_blurr_tfm(dataloaders)\n",
    "    hf_tokenizer = tfm.hf_tokenizer\n",
    "\n",
    "    # if we've included our labels list, we'll use it to look up the value of our target(s)\n",
    "    trg_labels = tfm.kwargs['labels'] if ('labels' in tfm.kwargs) else None\n",
    "\n",
    "    res = L()\n",
    "    n_inp = dataloaders.n_inp\n",
    "\n",
    "    for idx, (input_ids, label, sample) in enumerate(zip(x, y, samples)):\n",
    "        if idx >= max_n:\n",
    "            break\n",
    "\n",
    "        rets = [hf_tokenizer.decode(input_ids, skip_special_tokens=True)[:trunc_at]]\n",
    "        for item in sample[n_inp:]:\n",
    "            if not torch.is_tensor(item):\n",
    "                trg = trg_labels[int(item)] if trg_labels else item\n",
    "            elif is_listy(item.tolist()):\n",
    "                trg = [trg_labels[idx] for idx, val in enumerate(label.numpy().tolist()) if (val == 1)] if (trg_labels) else label.item()\n",
    "            else:\n",
    "                trg = trg_labels[label.item()] if (trg_labels) else label.item()\n",
    "\n",
    "            rets.append(trg)\n",
    "        res.append(tuplify(rets))\n",
    "\n",
    "    cols = [\"text\"] + [\"target\" if (i == 0) else f\"target_{i}\" for i in range(len(res[0]) - n_inp)]\n",
    "    display_df(pd.DataFrame(res, columns=cols)[:max_n])\n",
    "    return ctxs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence classification\n",
    "\n",
    "The following eamples demonstrate several approaches to construct your `DataBlock` for sequence classication tasks using the mid-level API, and also an example on how to accomplish the same using the low-level API and standard PyTorch/Hugging Face/fast.ai Datasets and DataLoaders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the mid-level API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/wgilliam/.cache/huggingface/datasets/imdb/plain_text/1.0.0/4ea52f2e58a08dbc12c2bd52d0d92b30b88c00230b4522801b3636782f625c5b)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>While the original 1932 version, with Preston Foster, was good, there's no remake more worthy than this 1959 one, or more impossible to find anywhere, just as I strongly suspect Mickey Rooney to have had something to do with that. Never could a mere performance have ever been so masterfully brilliant, or a script more thought-provoking, as well as an improvement upon the original. Many years after the last of my several viewings of this film, in 1970, I read an article in which Mickey Rooney was recounting a visit he'd made to death row, and which had apparently very drastically eliminated...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ironically the most talked-about American film in the 2008 New York Film Festival is 98% in Spanish. The extra-long film's controversy began at the Cannes Festival. There were love-hate notices, and considerable doubts about commercial prospects. As consolation the star, Benicio Del Toro, got the Best Actor award there. I'm talking about Steven Soderbergh's 'Che,' of course. That's the name it's going by in this version, shown in New York as at Cannes in two 2-hour-plus segments without opening title or end credits. 'Che' is certainly appropriate since Ernesto \"Che\" Guevara is in almost ev...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Bingo is the game, bullshit is the name. Rarely has the screen been smeared with such a blown-up hodgepodge of half-baked conspiracy theories, puritan prudery, and new-age gibberish. The bulk of the story is set at Viciente, a Cristian resort in the Peruvian jungle. Think Tolkien's Rivendell meets Star Trek's Planet Baku, inhabited by dimwitted followers of a not-so-mysterious, but surprisingly narrow-minded cult of love and peace. Thanks to gruesome acting and tacky production design (the rainbow-colored visualization of the mysterious all-healing \"energy\" is particularly hideous), \"The C...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>There are so many reasons as to why I rate the sopranos so highly, one of its biggest triumphs being the cast and character building. Each character unfolds more and more each series. Also each series has an array of different 'small time characters' as well as the main. A good example of a character (who was only in three episodes) who you can feel for is David the compulsive gambler played brilliantly by Robert Patrick. Every little detail builds the perfect TV series. The show revolves round mob boss Tony Soprano (James Gandolfini) who attempts to balance his life of crime with his role...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I thought watching employment videos on corporate compliance was tedious. This movie went nowhere fast. What could have been a somewhat cheesy half hour twilight zone episode turned into a seemingly endless waste of film on people parking their cars, a picture of some dude's swimming pool (he really needs to answer his phone by the way) a dot matrix printer doing its job, and Heuy and Louey sitting in a yellow lighted control room repeating \"T minus 10 and counting\" as if something exciting is going to happen. It doesn't so don't get your hopes up. The best thing about this movie is to see...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  \\\n",
       "0      1   \n",
       "1      1   \n",
       "2      0   \n",
       "3      1   \n",
       "4      0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0  While the original 1932 version, with Preston Foster, was good, there's no remake more worthy than this 1959 one, or more impossible to find anywhere, just as I strongly suspect Mickey Rooney to have had something to do with that. Never could a mere performance have ever been so masterfully brilliant, or a script more thought-provoking, as well as an improvement upon the original. Many years after the last of my several viewings of this film, in 1970, I read an article in which Mickey Rooney was recounting a visit he'd made to death row, and which had apparently very drastically eliminated...   \n",
       "1  Ironically the most talked-about American film in the 2008 New York Film Festival is 98% in Spanish. The extra-long film's controversy began at the Cannes Festival. There were love-hate notices, and considerable doubts about commercial prospects. As consolation the star, Benicio Del Toro, got the Best Actor award there. I'm talking about Steven Soderbergh's 'Che,' of course. That's the name it's going by in this version, shown in New York as at Cannes in two 2-hour-plus segments without opening title or end credits. 'Che' is certainly appropriate since Ernesto \"Che\" Guevara is in almost ev...   \n",
       "2  Bingo is the game, bullshit is the name. Rarely has the screen been smeared with such a blown-up hodgepodge of half-baked conspiracy theories, puritan prudery, and new-age gibberish. The bulk of the story is set at Viciente, a Cristian resort in the Peruvian jungle. Think Tolkien's Rivendell meets Star Trek's Planet Baku, inhabited by dimwitted followers of a not-so-mysterious, but surprisingly narrow-minded cult of love and peace. Thanks to gruesome acting and tacky production design (the rainbow-colored visualization of the mysterious all-healing \"energy\" is particularly hideous), \"The C...   \n",
       "3  There are so many reasons as to why I rate the sopranos so highly, one of its biggest triumphs being the cast and character building. Each character unfolds more and more each series. Also each series has an array of different 'small time characters' as well as the main. A good example of a character (who was only in three episodes) who you can feel for is David the compulsive gambler played brilliantly by Robert Patrick. Every little detail builds the perfect TV series. The show revolves round mob boss Tony Soprano (James Gandolfini) who attempts to balance his life of crime with his role...   \n",
       "4  I thought watching employment videos on corporate compliance was tedious. This movie went nowhere fast. What could have been a somewhat cheesy half hour twilight zone episode turned into a seemingly endless waste of film on people parking their cars, a picture of some dude's swimming pool (he really needs to answer his phone by the way) a dot matrix printer doing its job, and Heuy and Louey sitting in a yellow lighted control room repeating \"T minus 10 and counting\" as if something exciting is going to happen. It doesn't so don't get your hopes up. The best thing about this movie is to see...   \n",
       "\n",
       "   is_valid  \n",
       "0     False  \n",
       "1     False  \n",
       "2     False  \n",
       "3     False  \n",
       "4     False  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"imdb\", split=['train', 'test'])\n",
    "raw_datasets[0] = raw_datasets[0].add_column('is_valid', [False] * len(raw_datasets[0]))\n",
    "raw_datasets[1] = raw_datasets[1].add_column('is_valid', [True] * len(raw_datasets[1]))\n",
    "\n",
    "final_ds = concatenate_datasets([raw_datasets[0].shuffle().select(range(1000)), raw_datasets[1].shuffle().select(range(200))])\n",
    "imdb_df = pd.DataFrame(final_ds)\n",
    "imdb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = raw_datasets[0].features['label'].names\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic use\n",
    "\n",
    "Step 1: Get your Hugging Face objects.\n",
    "\n",
    "There are a bunch of ways we can get at the four Hugging Face elements we need (e.g., architecture name, tokenizer, config, and model).  We can just create them directly, or we can use one of the helper methods available via `BLURR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_output\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_cls = AutoModelForSequenceClassification\n",
    "\n",
    "pretrained_model_name = \"distilroberta-base\"  # \"distilbert-base-uncased\" \"bert-base-uncased\"\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name, model_cls=model_cls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Create your `DataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = (HF_TextBlock(hf_arch, hf_config, hf_tokenizer, hf_model, before_batch_kwargs={\"labels\": labels}), CategoryBlock)\n",
    "dblock = DataBlock(blocks=blocks, get_x=ColReader(\"text\"), get_y=ColReader(\"label\"), splitter=ColSplitter())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Build your `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(imdb_df, bs=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, torch.Size([4, 327]), 4)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "len(b), len(b[0][\"input_ids\"]), b[0][\"input_ids\"].shape, len(b[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the actual types represented by our batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{tuple: [dict, fastai.torch_core.TensorCategory]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explode_types(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Went to see this as Me and my Lady had little else to do on a sunday afternoon I like films that deal with sleazy,loser characters and this is full of em. After a slow start we get some good turns from the cast but it is the actual 'Bellini' that both makes and lets the film down. The 'Bellini' is one of the funniest scenes I have seen in a film for a long while but is too short and could have made this a masterpiece overall 71/2 out of 10</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Much like the early horror film The Boogens, the devious unseen killer is quite a letdown when it finally becomes seen. Although Animal House's Stephen Furst obviously had fun in the role as a product of incest, his performance is more comedy than horror.&lt;br /&gt;&lt;br /&gt;The plot, an extremely tired one, has three sexy women(Bach, Lamm and Lois Young) unable to find a hotel for the evening, so they willingly accept to stay with a seemingly kind museum curator, exceptionally played by the deceased Sy</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with long documents\n",
    "\n",
    "There are two options when dealing with texts longer than your model can handle.\n",
    "\n",
    "First, as illustrated above, you can simply provide a `truncation` strategy to ensure they are <= the maximum length your model can handle.\n",
    "\n",
    "Second, in the case we want to process the entirety of each document regardless of length, we can split text greater than the max length allowed by our model and then treat each of these chunks as separate examples. This approach is accomplished by setting `\"return_overflowing_tokens\": True` into our tokenizer function's via `tok_kwargs`. \n",
    "\n",
    "While the second approach is traditionaly performed as part of the data preprocessing, blurr can do this on-the-fly when using it's `OverflowDL` DataLoader (which is automatically used by blurr when you pass  `\"return_overflowing_tokens\": True` in the `tok_kwargs` argument of  `HF_TextBlock`.  Below is an example of how this works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Get your Hugging Face objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name, model_cls=model_cls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Create your `DataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = (\n",
    "    HF_TextBlock(\n",
    "        hf_arch,\n",
    "        hf_config,\n",
    "        hf_tokenizer,\n",
    "        hf_model,\n",
    "        max_length=128,\n",
    "        tok_kwargs={\"return_overflowing_tokens\": True, \"stride\": 2},\n",
    "        before_batch_kwargs={\"labels\": labels},\n",
    "    ),\n",
    "    CategoryBlock,\n",
    ")\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, get_x=ColReader(\"text\"), get_y=ColReader(\"label\"), splitter=ColSplitter())\n",
    "\n",
    "dls = dblock.dataloaders(imdb_df, bs=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Build your `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'d become the professing Christian he now makes it a special point, whenever possible, to emphasize that he is; but, as anybody should be well-aware, this particular category of people tends to be the most vehemently out for blood, when it comes to extracting an eye for an eye. However, I have no particular bone of contention concerning that, per se, just as there's no doubt, scripturally speaking, that not all, and perhaps not even most, shall be spared the same ultimate fate, at the hands of the Lord Himself, as a result of His sacrifice on the cross. However, there is a</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is a problem, for me, about the spirit or attitude with which most professing Christians emphasize their enthusiasm for capital punishment; for, contrary to the Lord Himself, who would love to see everybody saved (Ezekiel 18:32) (II Peter 3:9), they seem to go vindictively out of their way to find reasons to condemn!... What most people, on either side of this superlatively ever-burning issue, cannot appear to sufficiently appreciate, is that the Lord is as dynamically and elusively soft in nature as He is hard. The two sides of His nature appear to be</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to be so inherently incompatible as to render Him mentally deranged, at least by any strictly human reckoning. Yet, regardless of how harrowingly ungraspable this miraculously dynamic blending of the water and oil in His nature surely is, there can be no doubt that anything short of it, or anything fanatically and characteristically on either one side or the other of this equation, falls inadequately and unacceptably short of the entire judicial truth. Indeed, I've seen the most blood-curdling thirst for the same come out, self-contradictorily enough, on far-too-many</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-many occasions, whenever the categorically anti-death penalty advocates are confronted, even in the most rationally well-balanced ways, with the fact that, although the Lord died for everybody, not all are thereby going to be saved. After-all, in order to receive absolution, one must, to repeat the same term, reach out and receive it, that is, repent (Luke 13:3-5). Could anything make more sense?... But, then, what about the Lord's command to forgive, even in the case of one's enemies, of those who despise and persecute you</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the low-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Build your datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/wgilliam/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "    num_rows: 3668\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"].features\n",
    "raw_datasets[\"train\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ed45f26a5d4a99bd994c976fa9d7b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c136590f179472cbec982b9178b9268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aca69818b954449af57d81d828247a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    return hf_tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Define any pre-processing that needs to be done to your datasets (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def preproc_hf_dataset(\n",
    "    # A standard PyTorch Dataset or fast.ai Datasets\n",
    "    dataset: Union[torch.utils.data.dataset.Dataset, Datasets],\n",
    "    # A Hugging Face tokenizer\n",
    "    hf_tokenizer: PreTrainedTokenizerBase,\n",
    "    # A Hugging Face model\n",
    "    hf_model: PreTrainedModel,\n",
    "):\n",
    "    \"\"\"This method can be used to preprocess most Hugging Face Datasets for use in Blurr and other training\n",
    "    libraries\n",
    "    \"\"\"\n",
    "    if (\"label\") in dataset.column_names:\n",
    "        dataset = dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "    hf_model_fwd_args = list(inspect.signature(hf_model.forward).parameters.keys())\n",
    "    bad_cols = set(dataset.column_names).difference(hf_model_fwd_args)\n",
    "    dataset = dataset.remove_columns(bad_cols)\n",
    "\n",
    "    dataset.set_format(\"torch\")\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Build your `DataLoaders`.\n",
    "\n",
    "Use `BlurrDataLoader` to build Blurr friendly dataloaders from your datasets. Passing `{'labels': label_names}` to your `batch_tfm_kwargs` will ensure that your lable/target names will be displayed in methods like `show_batch` and `show_results` (just as it works with the mid-level API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = raw_datasets[\"train\"].features[\"label\"].names\n",
    "\n",
    "trn_dl = BlurrDataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    hf_arch,\n",
    "    hf_config,\n",
    "    hf_tokenizer,\n",
    "    hf_model,\n",
    "    preproccesing_func=preproc_hf_dataset,\n",
    "    batch_tfm_kwargs={'labels': label_names},\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "val_dl = BlurrDataLoader(\n",
    "    tokenized_datasets[\"validation\"],\n",
    "    hf_arch,\n",
    "    hf_config,\n",
    "    hf_tokenizer,\n",
    "    hf_model,\n",
    "    preproccesing_func=preproc_hf_dataset,\n",
    "    batch_tfm_kwargs={'labels': label_names},\n",
    "    batch_size=16,\n",
    ")\n",
    "\n",
    "dls = DataLoaders(trn_dl, val_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 62])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "b[0][\"input_ids\"].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IBM said it believes that the investigation arose from a separate SEC investigation of a customer of IBM's Retail Store Solutions unit, which markets and sells point-of-sale products. The company added : \" IBM believes that the investigation arises from a separate investigation by the SEC of a customer of IBM's Retail Store Solutions unit.</td>\n",
       "      <td>equivalent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We have already found two trailers, both of which we believe were used for the manufacture of biological weapons. We have already found two trailers that we and the Americans believe were used for chemical and biological weapons. \"</td>\n",
       "      <td>equivalent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=800)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "The tests below to ensure the core DataBlock code above works for **all** pretrained sequence classification models available in Hugging Face.  These tests are excluded from the CI workflow because of how long they would take to run and the amount of data that would be required to download.\n",
    "\n",
    "**Note**: Feel free to modify the code below to test whatever pretrained classification models you are working with ... and if any of your pretrained sequence classification models fail, please submit a github issue *(or a PR if you'd like to fix it yourself)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AlbertForSequenceClassification',\n",
       " 'BartForSequenceClassification',\n",
       " 'BertForSequenceClassification',\n",
       " 'BigBirdForSequenceClassification',\n",
       " 'BigBirdPegasusForSequenceClassification',\n",
       " 'CTRLForSequenceClassification',\n",
       " 'CamembertForSequenceClassification',\n",
       " 'CanineForSequenceClassification',\n",
       " 'ConvBertForSequenceClassification',\n",
       " 'DebertaForSequenceClassification',\n",
       " 'DebertaV2ForSequenceClassification',\n",
       " 'DistilBertForSequenceClassification',\n",
       " 'ElectraForSequenceClassification',\n",
       " 'FNetForSequenceClassification',\n",
       " 'FlaubertForSequenceClassification',\n",
       " 'FunnelForSequenceClassification',\n",
       " 'GPT2ForSequenceClassification',\n",
       " 'GPTJForSequenceClassification',\n",
       " 'GPTNeoForSequenceClassification',\n",
       " 'HubertForSequenceClassification',\n",
       " 'IBertForSequenceClassification',\n",
       " 'LEDForSequenceClassification',\n",
       " 'LayoutLMForSequenceClassification',\n",
       " 'LayoutLMv2ForSequenceClassification',\n",
       " 'LongformerForSequenceClassification',\n",
       " 'MBartForSequenceClassification',\n",
       " 'MPNetForSequenceClassification',\n",
       " 'MegatronBertForSequenceClassification',\n",
       " 'MobileBertForSequenceClassification',\n",
       " 'OpenAIGPTForSequenceClassification',\n",
       " 'PerceiverForSequenceClassification',\n",
       " 'ReformerForSequenceClassification',\n",
       " 'RemBertForSequenceClassification',\n",
       " 'RoFormerForSequenceClassification',\n",
       " 'RobertaForSequenceClassification',\n",
       " 'SEWDForSequenceClassification',\n",
       " 'SEWForSequenceClassification',\n",
       " 'SqueezeBertForSequenceClassification',\n",
       " 'TransfoXLForSequenceClassification',\n",
       " 'UniSpeechForSequenceClassification',\n",
       " 'UniSpeechSatForSequenceClassification',\n",
       " 'Wav2Vec2ForSequenceClassification',\n",
       " 'WavLMForSequenceClassification',\n",
       " 'XLMForSequenceClassification',\n",
       " 'XLMRobertaForSequenceClassification',\n",
       " 'XLNetForSequenceClassification']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "[model_type for model_type in BLURR.get_models(task=\"SequenceClassification\") if (not model_type.startswith(\"TF\"))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "pretrained_model_names = [\n",
    "    \"hf-internal-testing/tiny-albert\",\n",
    "    \"hf-internal-testing/tiny-random-bart\",\n",
    "    \"hf-internal-testing/tiny-bert\",\n",
    "    \"hf-internal-testing/tiny-random-big_bird\",\n",
    "    \"hf-internal-testing/tiny-random-bigbird_pegasus\",\n",
    "    \"hf-internal-testing/tiny-random-ctrl\",\n",
    "    \"camembert-base\",\n",
    "    \"hf-internal-testing/tiny-random-canine\",\n",
    "    \"YituTech/conv-bert-base\",\n",
    "    \"hf-internal-testing/tiny-deberta\",\n",
    "    \"hf-internal-testing/tiny-random-deberta-v2\",\n",
    "    \"hf-internal-testing/tiny-random-distilbert\",\n",
    "    \"hf-internal-testing/tiny-electra\",\n",
    "    \"google/fnet-base\",\n",
    "    \"hf-internal-testing/tiny-random-flaubert\",\n",
    "    \"hf-internal-testing/tiny-random-funnel\",\n",
    "    \"hf-internal-testing/tiny-random-gpt2\",\n",
    "    \"hf-internal-testing/tiny-random-gptj\",\n",
    "    \"hf-internal-testing/tiny-random-gpt_neo\",\n",
    "    \"hf-internal-testing/tiny-random-hubert\",\n",
    "    \"kssteven/ibert-roberta-base\",\n",
    "    \"hf-internal-testing/tiny-random-led\",\n",
    "    \"hf-internal-testing/tiny-layoutlm\",\n",
    "    \"microsoft/layoutlmv2-base-uncased\",\n",
    "    \"hf-internal-testing/tiny-random-longformer\",\n",
    "    \"hf-internal-testing/tiny-random-mbart\", \n",
    "    \"hf-internal-testing/tiny-random-mpnet\",\n",
    "    \"anton-l/megatron-11b\",                            \n",
    "    \"hf-internal-testing/tiny-random-mobilebert\",\n",
    "    \"openai-gpt\",\n",
    "    \"hf-internal-testing/tiny-random-reformer\",\n",
    "    \"google/rembert\",\n",
    "    \"hf-internal-testing/tiny-random-roformer\",\n",
    "    'google/reformer-enwik8',    \n",
    "    \"hf-internal-testing/tiny-random-roberta\",\n",
    "    \"hf-internal-testing/tiny-random-squeezebert\",\n",
    "    \"hf-internal-testing/tiny-random-transfo-xl\",\n",
    "    \"hf-internal-testing/tiny-random-xlm\",\n",
    "    \"hf-internal-testing/tiny-xlm-roberta\",\n",
    "    \"hf-internal-testing/tiny-random-xlnet\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# for model_name in pretrained_model_names:\n",
    "#     tok = AutoTokenizer.from_pretrained(model_name)\n",
    "#     print(f'=== {model_name} ===')\n",
    "#     print(f'=== {tok.padding_side} ===')\n",
    "#     print(f'=== {tok.pad_token_id} ===')\n",
    "#     print(tok(['hi', 'hello everyone. its good to be here'], ['yo', 'yo'], padding='max_length', max_length=128))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/wgilliam/.cache/huggingface/datasets/imdb/plain_text/1.0.0/4ea52f2e58a08dbc12c2bd52d0d92b30b88c00230b4522801b3636782f625c5b)\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "raw_datasets = load_dataset(\"imdb\", split=['train', 'test'])\n",
    "raw_datasets[0] = raw_datasets[0].add_column('is_valid', [False] * len(raw_datasets[0]))\n",
    "raw_datasets[1] = raw_datasets[1].add_column('is_valid', [True] * len(raw_datasets[1]))\n",
    "\n",
    "final_ds = concatenate_datasets([raw_datasets[0].shuffle().select(range(1000)), raw_datasets[1].shuffle().select(range(200))])\n",
    "imdb_df = pd.DataFrame(final_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-albert ===\n",
      "\n",
      "architecture:\talbert\n",
      "tokenizer:\tAlbertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"the last big thing\" is a wonderful satirical film that sardonically whips pop culture to the point of humorous self-desctruction. the characters are so interesting and fun to laugh at/sympathize with. which brings me to an introduction to the characters i liked best...br /br /simon geist is a man in his late 30s/early 40s who creates a pop-culture driven editorial magazine called \"the next big</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>any film with a title as ridiculous as \"the bagman\" should automatically attract the attention of any bad movie lover, but the plot is far different than what one may expect after viewing the dvd cover. the bagman is by no means a good movie. it falls into the category of films that seem to have been (and probably were) filmed on a home video camera. the acting is awful. i haven't heard and seen such wooden acting since troll 2. there are plenty of scenes with nudity and sex</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-bart ===\n",
      "\n",
      "architecture:\tbart\n",
      "tokenizer:\tBartTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This very good movie crackles with tension. The stakes are, of course, high--will the police and Widmark, the public health doctor, apprehend the criminals who don't know that they are carrying plague before a full-scale epidemic breaks out? But smaller plot elements introduce tension in every scene--between</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There is no story! The plot is hopeless! A filmed based on a car with a stuck accelerator, no brakes, and a stuck automatic transmission gear lever cannot be good! I would have stopped that car within one minute whether I was in it or in the police car constantly following it. I feel sorry for the actors that had to put up with such a poor scri</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-bert ===\n",
      "\n",
      "architecture:\tbert\n",
      "tokenizer:\tBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i don't like using the word \" awful \" to describe any work of the cinema for which a great deal of time, effort, talent and money is spent in its creation but zefferelli's attempt to adapt charlotte bronte's novel'jane eyre'is a total waste of time. &lt; br / &gt; &lt; br / &gt; the script is lacking in finesse and power, everything explained to the viewer in no uncertain terms, leaving little to the imagination. the lead actors are woefully miscast, clearly hired for their star names, and the musical score drippy and dull. charlotte gainsbourg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i'm not a writer or an critic... i'm just a student that has seen this movie few minutes ago.... and i want to thank people that worked on creating this movie! it is not the best or the most.... but it touched my heart... why??? i would like to understand it myself... it is easy and accessible.. it is a movie that makes you feel good after a bad day without any regret about the time wasted on watching it! it is about love and caring, about the life that we have but we miss it sometimes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-big_bird ===\n",
      "\n",
      "architecture:\tbig_bird\n",
      "tokenizer:\tBigBirdTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For months preceding the release of this movie you saw it advertised in all sorts of print media, so I patiently waited for its video release to see what all the hype was about. After it was over I had to apo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I really did like this show, once upon a time. That is, until I realized all the faults in it. It's so unrealistic. I know it's fiction, but it isn't even the slightest bit believable. Here's why. **Spoil</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-bigbird_pegasus ===\n",
      "\n",
      "architecture:\tbigbird_pegasus\n",
      "tokenizer:\tPegasusTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The plot of Corpse Grinders 2 is very much similar to the original Corpse Grinders, what is left that is different from the other film consists of</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Well I gave this movie a 7. It was better than \"Thirdspace\" but not as good as \"In the Beginning\" as far as the B5 movies go. I really t</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-ctrl ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/miniconda3/envs/blurr/lib/python3.9/site-packages/transformers/models/ctrl/modeling_ctrl.py:45: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  angle_rates = 1 / torch.pow(10000, (2 * (i // 2)) / d_model_size)\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tctrl\n",
      "tokenizer:\tCTRLTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This 1919 to 1933 Germany looks hardly like a post WWII Czech capitol. Oh sorry, it is the Czech capitol and it is 2003, how funny.&lt;br /&gt;&lt;br /&gt;This is one of the most awful history movies in the nearest past. Rhm is a head higher than Adolf and looks so damned good, Gring looks like 40 when he just is 23 and the \"Fhrer\" always seems to look like 56. And the buildings, folks, even buildings have been young, sometimes. Especially 1919 were a lot of houses in Germany nearly new (the WWI does not reach German cities!). No crumbling plaster! Then the Reichstagsbuilding. There have never been urban canyons around</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Schlocky '70s horror films...ya gotta love 'em. In contrast to today's boring slasher flicks, these K-tel specials actually do something scary and do not resort to a tired formula.&lt;br /&gt;&lt;br /&gt;This is a B movie about the making of a B movie...that went horribly wrong. Faith Domergue (This Island Earth) stars as an over-the-hill, B movie queen making a movie about a series of grisly murders that befell a family in their home. Her boyfriend/director, who looks and acts like Gordon Jump with an attitude, is filming on location and on a tight schedule. The Ken doll co-star discovers a book of Tibetian chants</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== camembert-base ===\n",
      "\n",
      "architecture:\tcamembert\n",
      "tokenizer:\tCamembertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It is always satisfying when a detective wraps up a case and the criminal is brought to book. In this case the climax gives me even greater pleasure. To see the smug grin wiped off the face of Abigail Mitchell when she realises her victim has left \"deathbed testimony\" which leaves no doubt about her guilt is very satisfying.&lt;br /&gt;&lt;br /&gt;Please understand: while</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_Waterdance_ explores a wide variety of aspects of the life of the spinally injured artfully. From the petty torments of faulty fluorescent lights flashing overhead to sexuality, masculinity and depression, the experience of disability is laid open.&lt;br /&gt;&lt;br /&gt;The diversity of the central characters themselves underscores the complexity of the material examined - Joel, the writer, Raymond, the black man with a murky past, and</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-canine ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tcanine\n",
      "tokenizer:\tCanineTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is amazing-looking movie with the whole thing done in just six or seven colors. When it came out over 15 years ago, it st</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sorry my fellow Nevada City neighbors, but this one is bad.&lt;br /&gt;&lt;br /&gt;Brian must have had too much botox because he had very</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== YituTech/conv-bert-base ===\n",
      "\n",
      "architecture:\tconvbert\n",
      "tokenizer:\tConvBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>' stanley and iris'show the triumph of the human spirit. for stanley, it's the struggle to become literate and realize his potential. for iris, it's to find the courage to love again after becoming a widow. the beauty of the movie is the dance that robert deniro and jane fonda do together, starting and stopping, before each has the skills and courage to completely trust each other and move on. in that sense it very nicely gives us a good view of how life often is, thus being credible. unlike some other reviewers i found the characters each rendered to be consistent for the whole picture.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dear friends and family, &lt; br / &gt; &lt; br / &gt; i guess if one teen wants to become biblical with another teen, then that's their eternal damnation - just remember kids, \" birth control \" doesn't mean \" oral sex \", i don't care what the honor student says. on the other hand, even if the senator's aid quotes himself as a \" bit of a romantic guy \", he's still only hitting on a high school girl. if she was my sister, i'd eat this guys kneecaps. &lt; br / &gt; &lt; br / &gt; other than that</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-deberta ===\n",
      "\n",
      "architecture:\tdeberta\n",
      "tokenizer:\tDebertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I can tell you just how bad this movie is. I was in the movie and I haven't seen it yet, but I cringe at the thought of anyone actually paying to see me drunk. Especially considering what we did that year. The thing is that they probably over edited it. Especially the scene where my roommate was snorting coke of the tits of a Mexican prostitute (they probably should have follow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The first time you see The Second Renaissance it may look boring. Look at it at least twice and definitely watch part 2. It will change your view of the matrix. Are the human people the ones who started the war? Is AI a bad thing?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-deberta-v2 ===\n",
      "\n",
      "architecture:\tdeberta_v2\n",
      "tokenizer:\tDebertaV2Tokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Some of the background details of this story are based, very, very loosely, on real events of the era in which this was placed. The story combines some of the details of the famous Leopold and Loeb case along with a bit of Aimee Semple McPherson.&lt;br /&gt;&lt;br /&gt;The story begins with two mothers (Shelley Winters and Debbie Reynolds) being hounded as they leave a courtroom. The crowd seems most intent on doing them bodily harm as their sons were just convicted of a heinous thrill crime. One person in the crowd apparently slashes Winters' hand as they make their way to a waiting car.&lt;br /&gt;&lt;br /&gt;Soon</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Friz Freleng's 'Rumours' is an excellent Private Snafu cartoon that warns against spreading panic-inducing rumours during wartime. Produced, as were all the Snafu shorts, to be shown to military audiences as entertaining instructional films, 'Rumours' is extremely imaginative and crams tons of ideas into its very brief lifespan. When Snafu starts a rumour about a bombing, it escalates into an eventual rumour that America has lost the war. This is illustrated brilliantly by way of a long, rubbery piece of baloney and several strange, fictional creatures who come back to haunt Snafu with ever more terrible news about his</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-distilbert ===\n",
      "\n",
      "architecture:\tdistilbert\n",
      "tokenizer:\tDistilBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>following the disasterous revolution, this film was pretty much the final nail in the coffin of goldcrest and thus the british film industry. the film</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>they changed the title of this atrocity to an unexpected love. the only thing worse is the film itself. the script contains dialogue that would be laugh</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-electra ===\n",
      "\n",
      "architecture:\telectra\n",
      "tokenizer:\tElectraTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sandwiched in between san francisco and captains courageous two of spencer tracy's greatest parts is this very curious film about war and the effects it has on some people. they gave him a gun stars spencer tracy and franchot tone in the only film they ever made together and gladys george as the woman who loves them both. &lt; br / &gt; &lt; br / &gt; tracy and tone are a couple of world war i draftees, tone is</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in the trivia section for pet sematary, it mentions that george romero ( director of two stephen king stories, creepshow and the dark half ) was set to direct and then pulled out. one wonders what he would've brought to the film, as the director mary lambert, while not really a bad director, doesn't really bring that much imagination to this adaptation of king's novel, of which he wrote the screenplay. there are of course some very effective, g</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/fnet-base ===\n",
      "\n",
      "architecture:\tfnet\n",
      "tokenizer:\tFNetTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As the jacket proclaims, this film is \"Gorgeously shot and masterfully edited,\" and, yes, it is mesmerizingly beautiful. The timelessness that we perceive in stoic rock and in the unceasing ebb and flow of water frames the ephemeral works from Goldsworthy's hands so that in their very ephemeralness they point to eternity.&lt;br /&gt;&lt;br /&gt;And so the beauty of his compositions haunt us with just a touch of melancholy woven in--or in the words of Matthew Ar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The line is funnier in England, where, away from Vixen!'s native America, the word \"fanny\" has a whole new meaning. Sadly, it's the only laugh you'll get in this terrible sex comedy that is neither sexy nor funny.&lt;br /&gt;&lt;br /&gt;Oddly unalluring with painted-on eyebrows, Erica Gavin (Acting ability: zero) is a nymphomaniac who lusts after her own brother, but rejects his black friend while making derogatory remarks about watermel</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-flaubert ===\n",
      "\n",
      "architecture:\tflaubert\n",
      "tokenizer:\tFlaubertTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am a HUGE Adam Sandler fan, and one day I was looking at the Cast &amp; Crew selection on one of his DVD' s and saw'Going Overboard'and decided to go out and rent it. So I went out with a few buddies of mine and rented it. We put it on and we were shocked to see an Adam Sandler that didn' t hit puberty yet, he looks as if he was 12 when this movie came out. I couldn' t even watch 30 minutes of this crap, I did</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The word'classic'is thrown around too loosely nowadays, but this movie well deserves the appelation. The combination of Neil Simon, Walter Matthau ( possibly the world' s best living comic actor ), and the late lamented George Burns make for a comic masterpiece. It is interesting to contemplate what the movie would have been like had not death prevented Jack Benny from playing George Burns'part, as had been planned. As it is, the reunion scene</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-funnel ===\n",
      "\n",
      "architecture:\tfunnel\n",
      "tokenizer:\tFunnelTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what a great gem this is. it's an unusual story that is fun to watch. yes, it has singing, but it is very nicely crafted into the story and is very melodic to</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what can i say about this movie? i have seen it quite a few times since the first time when i was around 6. i have seen the english version and it is done very w</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-gpt2 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tgpt2\n",
      "tokenizer:\tGPT2TokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What's that there in the skies? Is it a plane? Is it Superman?? Errr, no It's a TURTLE!?! See, that's what becomes of the Cold War! Nothing but bad news and other issues! The Americans shoot down a Russian combat plane somewhere over Artic territory and the subsequent explosion defrosts &amp; lit</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The minute the forward started, I knew we were in for trouble! The premise is laughable at best. The story line was even worse, if that is possible.&lt;br /&gt;&lt;br /&gt;The acting was stiff and the actors gave off a sense of inexperience. You expect more from the likes of Slater, Reid and Dorff. Lines were delivered as if from a ro</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-gptj ===\n",
      "\n",
      "=== hf-internal-testing/tiny-random-gpt_neo ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tgpt_neo\n",
      "tokenizer:\tGPT2TokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yep.. this is definatly up there with some of the worst of the MSTifyed movies, but I have definately seen worse. Think Gremlins rated R. Well anyway, I met Rick Sloane at some sci-fi convention, that amazingly, he was lecturing at! It was one of those really low budget conventions, where everything goes, an everyone b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>While this movie won't go down in the annals of great cinema, it is a fun way to spend an hour and a half with the family. The film is finally being released in video where it should have debuted in the first place.&lt;br /&gt;&lt;br /&gt;The film is about an eclectic group of friends who gather for dinners which they have named, \"The Hungry Bac</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-hubert ===\n",
      "\n",
      "=== kssteven/ibert-roberta-base ===\n",
      "\n",
      "architecture:\tibert\n",
      "tokenizer:\tRobertaTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A true anomaly in the French cinema,this despairing work has no equivalent in the contemporary production.One would rather have to look on the side of Louis Malle's \"le feu follet\" (1963)(the fire within) to find something not completely unlike Harel's effort.Wry and cynical,having lost all his illusions,the hero,a computer scientist,has got no more reason to live.Absolutely none.Estranged from the human race,he seems to live his life as some kind of entomologist,studying his colleagues.One of them catches his attention:Tisserand</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"The Last Big Thing\" is a wonderful satirical film that sardonically whips pop culture to the point of humorous self-desctruction. The characters are so interesting and fun to laugh at/sympathize with. Which brings me to an introduction to the characters I liked best...&lt;br /&gt;&lt;br /&gt;Simon Geist is a man in his late 30s/early 40s who creates a pop-culture driven editorial magazine called \"The Next Big Thing\". Thing is, this magazine doesnt really exist, and it is only an excuse for Simon to get close to actors by interviewing them, only to bitch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-led ===\n",
      "\n",
      "architecture:\tled\n",
      "tokenizer:\tLEDTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It seems that several of the people who have reviewed this movie only watched it in the first place because it was filmed near where they live. How's that for a ringing endorsement? If this movie was filmed near where I lived I wouldn't be mentioning it in my review. It is horrid! Several reviews state that this film is a spoof or tongue-in-cheek horr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK, I have watched the original French version. But I can't imagine this being better with subtitles.&lt;br /&gt;&lt;br /&gt;All I have to say that this is the most boring movie I have seen in a long time. There are almost no redeeming qualities to this film. That's why I can't understand all the positive reviews. It might be realistic in a s</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-layoutlm ===\n",
      "\n",
      "architecture:\tlayoutlm\n",
      "tokenizer:\tLayoutLMTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what do you get if you cross the matrix with the truman show? &lt; br / &gt; &lt; br / &gt; i'm sure you've all seen the matrix by now. the creators of the matrix say that it is'anime inspired '. just from watching the trailer to this classic, you can see where they took the plot from. &lt; br / &gt; &lt; br / &gt; the film is sort of set in 1980s japan, and it really shows. the costumes, music and words (</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ming the merciless does a little bardwork and a movie most foul!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== microsoft/layoutlmv2-base-uncased ===\n",
      "\n",
      "architecture:\tlayoutlmv2\n",
      "tokenizer:\tLayoutLMv2TokenizerFast\n",
      "\n",
      "Could not do one pass in your dataloader, there is something wrong in it\n",
      "=== hf-internal-testing/tiny-random-longformer ===\n",
      "\n",
      "architecture:\tlongformer\n",
      "tokenizer:\tLongformerTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes, as the other reviewers have already stated, this may not be vintage L&amp;H but it's far from being their worst work as at 20th Century Stupid...I mean Fox. This film certainly has all of the basic ingredients for things to go wrong for the boys. But it's their serious approach and determination that makes them funny. They don't play</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oh yes, Sakura Killers is a goofy, horrible ninja movie, make no mistake. But it's also an incredibly enjoyable one. This is largely thanks to the awesome presence of one Chuck Connors, who is billed as starring in the movie but really only shines in a few scenes. I suppose he's supposed to be sort of an Obi W</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-mbart ===\n",
      "\n",
      "architecture:\tmbart\n",
      "tokenizer:\tMBartTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Where to begin, there's so much wrong and horrible about this movie I am not sure where to start. Okay, the two stooges who wrote this crap</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Die Sieger\" was highly recommended to be one of the few good action movies made in Germany. I watched it last night and I must admit, t</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-mpnet ===\n",
      "\n",
      "architecture:\tmpnet\n",
      "tokenizer:\tMPNetTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>on the surface, this movie would appear to deal with the psychological process called individuation, that is how to become a true self by embracing the</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r balki tries to tell you a story that had been earlier told by ram gopal verma in nishabd in a sensuous way. this time it is mixed with mature humors. &lt; br / &gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== anton-l/megatron-11b ===\n",
      "\n",
      "=== hf-internal-testing/tiny-random-mobilebert ===\n",
      "\n",
      "architecture:\tmobilebert\n",
      "tokenizer:\tMobileBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\" black angel \" is minor whodunit, with june vincent as a woman trying to save her husband from the electric chair after he is found guilty of killing an ol</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>an interesting change from the first one. there was more mystery to this movie then the first. even when it ends your asking yourself what happened who w</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== openai-gpt ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\topenai\n",
      "tokenizer:\tOpenAIGPTTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the power to dream is a wonderful thing. there's a saying, \" not all dreamers achieve, but all achievers dream. \" by exploring our imagination we shape our own futures. or build empires. perhaps overcome our fears, limitations and obstacles. gain wisdom and benefit mankind. or ( put simply ) just find our way to true love and happiness. freud might express such things in symbols. the language of fantasy. &lt; br / &gt; &lt; br / &gt; tristan ventures out of a rather twee english village called wall. he goes through a break in the wall. a portal. in search of something that will prove his love</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>on an overnight flight from los angeles to miami, lisa reisert ( rachel mcadams ) meets a charming man who turns out to be a hired killer who demands her help killing a businessman or else her own father will die. &lt; br / &gt; &lt; br / &gt; red eye is a terrific thriller that keeps the audience on the edge of their seats. the premise is similar to cellular and phone booth but red eye is better than both of those films. almost everything about red eye is above average including the suspense, the acting and the direction. most of the film does take place on a plane but that doesn't slow down</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-reformer ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\treformer\n",
      "tokenizer:\tReformerTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"If I wanted to dribble, I'd call a nurse.\" &lt;br /&gt;&lt;br /&gt;\"Haven't you had enough?\"...\"More than enough.\"&lt;br /&gt;&lt;br /&gt;\"You got me a c</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Being the first feature film with Robert De Niro (although not released for years later), this is worth the watch. De Niro's role isn't h</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/rembert ===\n",
      "\n",
      "architecture:\trembert\n",
      "tokenizer:\tRemBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I actually like the original, and this film has its ups and downs. Here's just a few:&lt;br /&gt;&lt;br /&gt;Ups: Most of the original voice cast returned.&lt;br /&gt;&lt;br /&gt;Downs: I didn't like the voice of Timon's Ma. I know she did a voice in The Simpsons, but that show is just plain stupid. &lt;br /&gt;&lt;br /&gt;Ups: We get to see Simba as a \"teenager.\"&lt;br /&gt;&lt;br /&gt;Downs: They wasted it with a slug-slurping contest between Timon and Simba. &lt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oliver Gruner is totally unknown to me. My friend showed me this film because he had seen Gruner in, what he called a pretty good sci-fi film, Nemesis. So as we watched this, we found ourselves fastforwarding through the BS drama parts just to get to the unbelievable action sequences. Gruner loves to kick and kick and kick. And kick! haha&lt;br /&gt;&lt;br /&gt;Gruner character is a graduate student who is forced to stay in a ghetto close to the one that he grew up in. He finds himself watching after the boy who lives with him because he really wants to</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-roformer ===\n",
      "\n",
      "architecture:\troformer\n",
      "tokenizer:\tRoFormerTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>poorly - made \" blaxploitation \" crime - drama aimed squarely at the black urban market of the early 1970s. pam grier stars in the title role, that of a nurse who becomes a one - woman vigilante after drug - dealing thugs make coffy's little sister a junkie. violent nonsense plods along doggedly, with canned energy and excitement ; only grier's flaring temp</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>here we go with other slasher movie, good looking people and acting from everyone was really good! &lt; br / &gt; &lt; br / &gt; few kids playing pranks phones calls and there parents are killed by the killer in front of the kids! 20 years after they are still friends, they go to huge house, have fun, drugs and sex ( no nudity ) for least half a hour of the movie! again they start making pranks phone call all over again! and</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/reformer-enwik8 ===\n",
      "\n",
      "=== hf-internal-testing/tiny-random-roberta ===\n",
      "\n",
      "architecture:\troberta\n",
      "tokenizer:\tRobertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Twilight Zone has achieved a certain mythology about it--much like Star Trek. That's because there are many devoted lovers of the show that no matter what think every episode was a winner. They are the ones who score each individual show a 10 and cannot objectively evaluate the show. Because of this, a while back I reviewed all the original St</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love this movie. It's wacky, funny, violent, surreal, played out in a madman's head, and definitely not your usual comedy. &lt;br /&gt;&lt;br /&gt;If you don't find the film amusing then I guess it's just not for your tastes, so this is a tough one to write a review for.&lt;br /&gt;&lt;br /&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-squeezebert ===\n",
      "\n",
      "architecture:\tsqueezebert\n",
      "tokenizer:\tSqueezeBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>storyline : max von sydow's voice - over narration hypnotizes the protagonist ( and audience ) back to 1945 where our protagonist the young american id</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>after seeing jeremy brett as sherlock holmes, no actor should ever display such conceit as to imagine that he could ever come close to mr. brett's portr</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-transfo-xl ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\ttransfo_xl\n",
      "tokenizer:\tTransfoXLTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As you probably already know, Jess Franco is one prolific guy. made hundreds upon hundreds of films, many of which are crap. However, he managed to sneak in an occasionally quality work amongst all the assembly line exploitation. \"\" isn't his best work (thats either \"The Diabolical Dr. Z\" or \"Lesbos\"), but it has many of his trademarks that make it a must for anyone interested in diving into his large catalog. He combines the erotic (alternating between showing full-frontal nudity and leaving somethings left to the imagination) and the surreal seamlessly. This is a very dreamlike film, full of great atmosphere.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I didn't have many expectation going into the film, but I thought it was fantastic. Pierce Brosnan is outstanding in a very different role. He has dumped the slick suits for a ridiculous look and pays off showing that he is an excellent actor. Pierce and Greg Kinnear play off each other great, and make for one of the better buddy pairings in a long time. The humor is dark, the performances by Brosnan, Kinnear, and Hope Davis are great, mix that with a touching element to the story about friendship, and you have a great film. This is probably one of the better buddy comedies in a long time. This is a</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-xlm ===\n",
      "\n",
      "architecture:\txlm\n",
      "tokenizer:\tXLMTokenizer\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this movie is very important because suggested me this consideration : sometimes you can wish to be sick... sometimes you can wish to have a syndrome... sometimes, for example, you can wish have goldfield syndrome... that way you 'd not remember this boring movie... and above all you 'd not remember adam \" superfluos \" sandler... sometimes, simply, you can wish... have rented another movie... &lt; br / &gt; &lt; br / &gt; my vote? 3 out of 10. my suggestion? if you are neither a fan of boring romantic comedies or adam sandler (... it's a joke don 't exist adam sandler's</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is one of the better classic edgar wallace movies from the german series - it features all basics for a highly enjoyable wallace crime flic movie way back from the 60ies : although his majesty, mr. kinski, is missing you still have young joachim'blacky'fuchsberger, starring once again as the typical clever american'womanizer ', you have young eddi arendt in his best ( and just as well typical ) role ever - the cool, sophisticated british butler - and you have ( not so young anymore ) lowitz as the melancholic yet very'dry'ironic ( and thus</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-xlm-roberta ===\n",
      "\n",
      "architecture:\txlm_roberta\n",
      "tokenizer:\tXLMRobertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is yet another bad movie that you should probably avoid watching. The plot could be a lot \"thicker\" than it actually is and would be better made as a blockbuster type movie.br /&gt;br /&gt;The acting leaves something to be desired, though you can not quite place your finger on what it is.br /&gt;br /&gt;This is</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A fine western, following the fate of those who possess the prize winning gun, a Winchester '73. It has a great cast who give superb cliche characterisations with help from the usual effective story telling direction from Mann.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-random-xlnet ===\n",
      "\n",
      "architecture:\txlnet\n",
      "tokenizer:\tXLNetTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The first 30min of the flick was choppy and hard to know just what was going on (unless you read the book - which I had not).&lt;br /&gt;&lt;br /&gt;If you can stick with the first half, the second half is sweet - predictable, y</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"The Invisible Mouse\" is a delightful and different Tom &amp; Jerry's cartoon. It features the usual cat/mouse chases and battles, but in a different way this time. Jerry accidentally falls in a bottle of invisible in</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "model_cls = AutoModelForSequenceClassification\n",
    "bsz = 2\n",
    "seq_sz = 128\n",
    "\n",
    "test_results = []\n",
    "for model_name in pretrained_model_names:\n",
    "    error = None\n",
    "\n",
    "    print(f\"=== {model_name} ===\\n\")\n",
    "\n",
    "    tok_class = RobertaTokenizer if (\"/ibert\" in model_name) else None\n",
    "\n",
    "    try:\n",
    "\n",
    "        hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(model_name, model_cls=model_cls, tokenizer_cls=tok_class)\n",
    "\n",
    "        print(f\"architecture:\\t{hf_arch}\\ntokenizer:\\t{type(hf_tokenizer).__name__}\\n\")\n",
    "\n",
    "        # not all architectures include a native pad_token (e.g., gpt2, ctrl, etc...), so we add one here\n",
    "        if hf_tokenizer.pad_token is None:\n",
    "            hf_tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "            hf_config.pad_token_id = hf_tokenizer.get_vocab()[\"<pad>\"]\n",
    "            hf_model.resize_token_embeddings(len(hf_tokenizer))\n",
    "\n",
    "        blocks = (HF_TextBlock(hf_arch, hf_config, hf_tokenizer, hf_model, padding=\"max_length\", max_length=seq_sz), CategoryBlock)\n",
    "\n",
    "        dblock = DataBlock(blocks=blocks, get_x=ColReader(\"text\"), get_y=ColReader(\"label\"), splitter=ColSplitter())\n",
    "        dls = dblock.dataloaders(imdb_df, bs=bsz)\n",
    "        b = dls.one_batch()\n",
    "\n",
    "        print(\"*** TESTING DataLoaders ***\\n\")\n",
    "        test_eq(len(b), 2)\n",
    "        test_eq(len(b[0][\"input_ids\"]), bsz)\n",
    "        test_eq(b[0][\"input_ids\"].shape, torch.Size([bsz, seq_sz]))\n",
    "        test_eq(len(b[1]), bsz)\n",
    "\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, model_name, \"PASSED\", \"\"))\n",
    "        dls.show_batch(dataloaders=dls, max_n=2, trunc_at=1000)\n",
    "\n",
    "    except Exception as err:\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, model_name, \"FAILED\", err))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arch</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>model_name</th>\n",
       "      <th>result</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert</td>\n",
       "      <td>AlbertTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-albert</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bart</td>\n",
       "      <td>BartTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-random-bart</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bert</td>\n",
       "      <td>BertTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-bert</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>big_bird</td>\n",
       "      <td>BigBirdTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-random-big_bird</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bigbird_pegasus</td>\n",
       "      <td>PegasusTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-random-bigbird_pegasus</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ctrl</td>\n",
       "      <td>CTRLTokenizer</td>\n",
       "      <td>hf-internal-testing/tiny-random-ctrl</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>camembert</td>\n",
       "      <td>CamembertTokenizerFast</td>\n",
       "      <td>camembert-base</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>canine</td>\n",
       "      <td>CanineTokenizer</td>\n",
       "      <td>hf-internal-testing/tiny-random-canine</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>convbert</td>\n",
       "      <td>ConvBertTokenizerFast</td>\n",
       "      <td>YituTech/conv-bert-base</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>deberta</td>\n",
       "      <td>DebertaTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-deberta</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>deberta_v2</td>\n",
       "      <td>DebertaV2Tokenizer</td>\n",
       "      <td>hf-internal-testing/tiny-random-deberta-v2</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>distilbert</td>\n",
       "      <td>DistilBertTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-random-distilbert</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>electra</td>\n",
       "      <td>ElectraTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-electra</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>fnet</td>\n",
       "      <td>FNetTokenizerFast</td>\n",
       "      <td>google/fnet-base</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>flaubert</td>\n",
       "      <td>FlaubertTokenizer</td>\n",
       "      <td>hf-internal-testing/tiny-random-flaubert</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>funnel</td>\n",
       "      <td>FunnelTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-random-funnel</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>GPT2TokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-random-gpt2</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>GPT2TokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-random-gptj</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>Error(s) in loading state_dict for GPTJForSequenceClassification:\\n\\tsize mismatch for score.weight: copying a param with shape torch.Size([2, 512]) from checkpoint, the shape in current model is torch.Size([2, 32]).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>gpt_neo</td>\n",
       "      <td>GPT2TokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-random-gpt_neo</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gpt_neo</td>\n",
       "      <td>GPT2TokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-random-hubert</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>Can't load tokenizer for 'hf-internal-testing/tiny-random-hubert'. Make sure that:\\n\\n- 'hf-internal-testing/tiny-random-hubert' is a correct model identifier listed on 'https://huggingface.co/models'\\n  (make sure 'hf-internal-testing/tiny-random-hubert' is not a path to a local directory with something else, in that case)\\n\\n- or 'hf-internal-testing/tiny-random-hubert' is the correct path to a directory containing relevant tokenizer files\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ibert</td>\n",
       "      <td>RobertaTokenizer</td>\n",
       "      <td>kssteven/ibert-roberta-base</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>led</td>\n",
       "      <td>LEDTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-random-led</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>layoutlm</td>\n",
       "      <td>LayoutLMTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-layoutlm</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>layoutlmv2</td>\n",
       "      <td>LayoutLMv2TokenizerFast</td>\n",
       "      <td>microsoft/layoutlmv2-base-uncased</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>You must provide corresponding bounding boxes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>longformer</td>\n",
       "      <td>LongformerTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-random-longformer</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>mbart</td>\n",
       "      <td>MBartTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-random-mbart</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>mpnet</td>\n",
       "      <td>MPNetTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-random-mpnet</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>mpnet</td>\n",
       "      <td>MPNetTokenizerFast</td>\n",
       "      <td>anton-l/megatron-11b</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>'megatron'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mobilebert</td>\n",
       "      <td>MobileBertTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-random-mobilebert</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>openai</td>\n",
       "      <td>OpenAIGPTTokenizerFast</td>\n",
       "      <td>openai-gpt</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>reformer</td>\n",
       "      <td>ReformerTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-random-reformer</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>rembert</td>\n",
       "      <td>RemBertTokenizerFast</td>\n",
       "      <td>google/rembert</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>roformer</td>\n",
       "      <td>RoFormerTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-random-roformer</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>roformer</td>\n",
       "      <td>RoFormerTokenizerFast</td>\n",
       "      <td>google/reformer-enwik8</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>not a string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>roberta</td>\n",
       "      <td>RobertaTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-random-roberta</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>squeezebert</td>\n",
       "      <td>SqueezeBertTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-random-squeezebert</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>transfo_xl</td>\n",
       "      <td>TransfoXLTokenizer</td>\n",
       "      <td>hf-internal-testing/tiny-random-transfo-xl</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>xlm</td>\n",
       "      <td>XLMTokenizer</td>\n",
       "      <td>hf-internal-testing/tiny-random-xlm</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>xlm_roberta</td>\n",
       "      <td>XLMRobertaTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-xlm-roberta</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>xlnet</td>\n",
       "      <td>XLNetTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-random-xlnet</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide_input\n",
    "test_results_df = pd.DataFrame(test_results, columns=[\"arch\", \"tokenizer\", \"model_name\", \"result\", \"error\"])\n",
    "display_df(test_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `blurr.data.core` module contains the fundamental bits for all data preprocessing tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_data-core.ipynb.\n",
      "Converted 01_modeling-core.ipynb.\n",
      "Converted 02_data-language-modeling.ipynb.\n",
      "Converted 02_modeling-language-modeling.ipynb.\n",
      "Converted 03_data-token-classification.ipynb.\n",
      "Converted 03_modeling-token-classification.ipynb.\n",
      "Converted 04_data-question-answering.ipynb.\n",
      "Converted 04_modeling-question-answering.ipynb.\n",
      "Converted 10_data-seq2seq-core.ipynb.\n",
      "Converted 10_modeling-seq2seq-core.ipynb.\n",
      "Converted 11_data-seq2seq-summarization.ipynb.\n",
      "Converted 11_modeling-seq2seq-summarization.ipynb.\n",
      "Converted 12_data-seq2seq-translation.ipynb.\n",
      "Converted 12_modeling-seq2seq-translation.ipynb.\n",
      "Converted 99a_examples-high-level-api.ipynb.\n",
      "Converted 99b_examples-glue.ipynb.\n",
      "Converted 99c_examples-glue-plain-pytorch.ipynb.\n",
      "Converted 99d_examples-multilabel.ipynb.\n",
      "Converted 99e_examples-causal-lm-gpt2.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

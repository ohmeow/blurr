{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb, sys, inspect\n",
    "from enum import Enum\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from transformers import *\n",
    "from fastai2.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.set_device(1)\n",
    "print(f'Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FOR_QUESTION_ANSWERING_MAPPING\n",
    "MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING\n",
    "MODEL_MAPPING\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
    "ALL_MODELS = sum((tuple(conf.pretrained_config_archive_map.keys()) for conf in MODEL_CONFIG_CLASSES), (),)\n",
    "MODEL_CONFIG_CLASSES, MODEL_TYPES, #ALL_MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts string representation to class\n",
    "def str_to_class(classname):\n",
    "    return getattr(sys.modules[__name__], classname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Inspection & Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "\n",
    "[1] There are \"three standard classes required to use each model: **configuration, models and tokenizer**.\"\n",
    "\n",
    "[2] All three standard classes can be initialized [via] `from_pretrained()`.  This method will download (as needed), cache, and load the pre-trained instace from the library or via the filesystem. \n",
    "\n",
    "**Models**: All derive from `nn.Module` (e.g., `BertModel`)\n",
    "\n",
    "**Configuration**: Stores configuration required to **build a model** (e.g., `BertConfig`). \"*If you are using a pretrained model* without any modification, *creating the model will automatically take care of instantiating the configuration* (which is part of the model).\"\n",
    "\n",
    "**Tokenizer**: Stores the vocab for each model and provides methods to encode/decode strings and provide the various embeddings required to be fed into a model.\n",
    "\n",
    "**`from_pretrained()`**: To instantiate any of the above classes using a friendly name included in the library (`bert-base-uncased`) or from a path.\n",
    "\n",
    "**`save_pretrained()`**: To save any of the classes locally so it can be re-loaded using `from_pretrained()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_classes = inspect.getmembers(sys.modules[__name__], \n",
    "                                         lambda member: inspect.isclass(member)\n",
    "                                                         and member.__module__.startswith('transformers.'))\n",
    "\n",
    "transformer_classes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(transformer_classes, columns=['class_name', 'class_location'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['module'] = df.class_location.apply(lambda v: v.__module__); df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(labels=['class_location'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_parts_df = df.module.str.split(\".\", n = -1, expand = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(module_parts_df.columns)):\n",
    "    df[f'module_part_{i}'] = module_parts_df[i]\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_part_1_df = df.module_part_1.str.split(\"_\", n = 1, expand = True) \n",
    "module_part_1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['functional_area', 'arch']] = module_part_1_df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look custom, task-based implementations of models (indicated by `<model>For<task>`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type_df = df[(df.functional_area == 'modeling')].class_name.str.split('For', n=1, expand=True)\n",
    "model_type_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type_df[1] = np.where(model_type_df[1].notnull(), 'For' + model_type_df[1].astype(str), model_type_df[1])\n",
    "df['model_task'] = model_type_df[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look custom, task-based implementations of models (indicated by `<model>With<task>`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type_df = df[(df.functional_area == 'modeling')].class_name.str.split('With', n=1, expand=True)\n",
    "model_type_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type_df[1] = np.where(model_type_df[1].notnull(), \n",
    "                            'With' + model_type_df[1].astype(str), \n",
    "                            df[(df.functional_area == 'modeling')].model_task)\n",
    "\n",
    "df['model_task'] = model_type_df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(df.model_task.unique()))\n",
    "print(list(df.functional_area.unique()))\n",
    "print(list(df.module_part_2.unique()))\n",
    "print(list(df.module_part_3.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at what we're going to remove (use to verify we're just getting rid of stuff we want too)\n",
    "# df[~df['hf_class_type'].isin(['modeling', 'configuration', 'tokenization'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['functional_area'].isin(['modeling', 'configuration', 'tokenization'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get included architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_architectures(): \n",
    "    return df[(df.arch.notna()) & (df.arch != None)].arch.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_architectures())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORMER_ARCHITECTURES = Enum('TRANSFORMER_ARCHITECTURES', get_architectures())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(L(TRANSFORMER_ARCHITECTURES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get an architecture's config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(arch): \n",
    "    return df[(df.functional_area == 'configuration') & (df.arch == arch)].class_name.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_config('bert'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get an architecture's tokenizers\n",
    "\n",
    "There may be multiple so this returns a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizers(arch): \n",
    "    return df[(df.functional_area == 'tokenization') & (df.arch == arch)].class_name.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_tokenizers('electra'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get included custom model tasks\n",
    "\n",
    "Get the type of tasks for which there is a custom model for (*optional: by architecture*). There are a number of customized models built for specific tasks like token classification, question/answering, LM, etc...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tasks(arch=None): \n",
    "    query = ['model_task.notna()']\n",
    "    if (arch): query.append(f'arch == \"{arch}\"')\n",
    "        \n",
    "    return df.query(' & '.join(query)).model_task.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_tasks())\n",
    "print(get_tasks('bart'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORMER_TASKS_ALL = Enum('TRANSFORMER_TASKS_ALL', get_tasks())\n",
    "TRANSFORMER_TASKS_AUTO = Enum('TRANSFORMER_TASKS_AUTO', get_tasks('auto'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--- all tasks ---')\n",
    "print(L(TRANSFORMER_TASKS_ALL))\n",
    "print('\\n--- auto only ---')\n",
    "print(L(TRANSFORMER_TASKS_AUTO))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get included models\n",
    "\n",
    "The transformer models available for use (*optional: by architecture | task*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models(arch=None, task=None): \n",
    "    query = ['functional_area == \"modeling\"']\n",
    "    if (arch): query.append(f'arch == \"{arch}\"')\n",
    "    if (task): query.append(f'model_task == \"{task}\"')\n",
    "        \n",
    "    return df.query(' & '.join(query)).class_name.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(L(get_models()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_models(arch='bert'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_models(task='ForTokenClassification'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_models(arch='bert', task='ForTokenClassification'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORMER_MODELS = Enum('TRANSFORMER_MODELS', get_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(L(TRANSFORMER_MODELS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get tokenizers, config, and model for a given model name / enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes_for_model(model_name_or_enum):\n",
    "    model_name = model_name_or_enum if isinstance(model_name_or_enum, str) else model_name_or_enum.name\n",
    "    \n",
    "    meta = df[df.class_name == model_name]\n",
    "    tokenizers = get_tokenizers(meta.arch.values[0])\n",
    "    config = get_config(meta.arch.values[0])\n",
    "    \n",
    "    return ([str_to_class(tok) for tok in tokenizers], str_to_class(config), str_to_class(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers, config, model = get_classes_for_model('RobertaForSequenceClassification')\n",
    "\n",
    "print(tokenizers[0])\n",
    "print(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers, config, model = get_classes_for_model(TRANSFORMER_MODELS.DistilBertModel)\n",
    "\n",
    "print(tokenizers[0])\n",
    "print(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_architecture(model_name_or_enum):\n",
    "    model_name = model_name_or_enum if isinstance(model_name_or_enum, str) else model_name_or_enum.name\n",
    "    return df[df.class_name == model_name].arch.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_model_architecture('RobertaForSequenceClassification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Pre-Trained (configs, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auto_hf_objects(pretrained_model_name_or_path,\n",
    "                        task=TRANSFORMER_TASKS_AUTO.ForSequenceClassification, \n",
    "                        config=None):\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
    "    config = AutoConfig.from_pretrained(pretrained_model_name_or_path) if (config is None) else config\n",
    "    \n",
    "    model = str_to_class(f'AutoModel{task.name}').from_pretrained(pretrained_model_name_or_path, \n",
    "                                                                  config=config)\n",
    "    arch = get_model_architecture(type(model).__name__)\n",
    "    \n",
    "    return (arch, tokenizer, config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch, tokenizer, config, model = get_auto_hf_objects(\"bert-base-cased-finetuned-mrpc\", \n",
    "                                                     task=TRANSFORMER_TASKS_AUTO.WithLMHead)\n",
    "\n",
    "print(arch)\n",
    "print(type(tokenizer))\n",
    "print(type(config))\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch, tokenizer, config, model = get_auto_hf_objects(\"fmikaelian/flaubert-base-uncased-squad\", \n",
    "                                                              task=TRANSFORMER_TASKS_AUTO.ForQuestionAnswering)\n",
    "\n",
    "print(arch)\n",
    "print(type(tokenizer))\n",
    "print(type(config))\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformer_objects(pretrained_model_name_or_path, \n",
    "                            tokenizer_cls=BertTokenizer,\n",
    "                            model_cls=TRANSFORMER_MODELS.BertModel,\n",
    "                            config_cls=BertConfig):\n",
    "    \n",
    "    tokenizer = tokenizer_cls.from_pretrained(pretrained_model_name_or_path)\n",
    "    \n",
    "    if (config_cls is None):\n",
    "        model = str_to_class(model_cls.name).from_pretrained(pretrained_model_name_or_path)\n",
    "        config = None\n",
    "    else:\n",
    "        config = config_cls.from_pretrained(pretrained_model_name_or_path)\n",
    "        model = str_to_class(model_cls.name).from_pretrained(pretrained_model_name_or_path, config=config)\n",
    "    \n",
    "    arch = get_model_architecture(type(model).__name__)\n",
    "        \n",
    "    return (arch, tokenizer, config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch, tokenizer, config, model = get_transformer_objects(\"bert-base-cased-finetuned-mrpc\",\n",
    "                                                         tokenizer_cls=BertTokenizer, \n",
    "                                                         config_cls=None,\n",
    "                                                         model_cls=TRANSFORMER_MODELS.BertForNextSentencePrediction)\n",
    "print(arch)\n",
    "print(type(tokenizer))\n",
    "print(type(config))\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terms:\n",
    "\n",
    "**Input IDs**: \\\n",
    "\"The input ids are often the only required parameters to be passed to the model as input. They are *token indices, numerical representations of tokens* building the sequences that will be used as input by the model.\"\n",
    "\n",
    "`tokenizer.tokenize(sequence)` => Splits the sequnce into tokens based on vocab\n",
    "\n",
    "`tokenizer.encode(sequence)` => Converts tokens to their numerical IDs (add `add_special_tokens=False` to exclude special tokens)\n",
    "\n",
    "`tokenizer.encode_plus(sequence)` => Returnes a dictionary of \"input_ids\", \"token_type_ids\", and \"attention_mask\"\n",
    "\n",
    "**Attention Mask**: \\\n",
    "\"This argument indicates to the model which tokens should be attended to, and which should not ... a binary tensor indicating the position of the padded indices so that the model does not attend to them. For the BertTokenizer, 1 indicate a value that should be attended to while 0 indicate a padded value.\" (optional)\n",
    "\n",
    "`tokenizer.encode(sequence, max_length=20, pad_to_max_length=True)`\n",
    "\n",
    "**Token Type IDs**: \\\n",
    "\"Some models’ purpose is to do sequence classification or question answering. These require two different sequences to be encoded in the same input IDs. They are usually separated by special tokens, such as the classifier and separator tokens.... The Token Type IDs are a binary mask identifying the different sequences (segments) in the model.\"\n",
    "\n",
    "`tokenizer.encode(sequence_a, sequence_b)`\n",
    "\n",
    "\"The first sequence, the “context” used for the question, has all its tokens represented by 0, whereas the question has all its tokens represented by 1. Some models, like `XLNetModel` use an additional token represented by a 2.\"\n",
    "\n",
    "**Position IDs**: \\\n",
    "\"The position IDs are used by the model to identify which token is at which position. Contrary to RNNs that have the position of each token embedded within them, transformers are unaware of the position of each token.... If no position IDs are passed to the model, they are automatically created as absolute positional embeddings.\" (optional)\n",
    "\n",
    "\"Absolute positional embeddings are selected in the range `[0, config.max_position_embeddings - 1]`. Some models use other types of positional embeddings, such as sinusoidal position embeddings or relative position embeddings.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "print(tokenizer.tokenize(\"Hi! You like the Bert Tokenizer?\"))\n",
    "print(tokenizer.encode(\"Hi! You like the Bert Tokenizer?\"))\n",
    "print(tokenizer.encode(\"Hi! You like the Bert Tokenizer?\", add_special_tokens=False))\n",
    "print(tokenizer.encode_plus(\"Hi! You like the Bert Tokenizer?\"))\n",
    "print(tokenizer.encode_plus(\"Hi!\", \"You like the Bert Tokenizer?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.encode(\"Hi! You like the Bert Tokenizer?\", add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALBERT\n",
    "tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "tok_results =  tokenizer.encode_plus(\"Hi!\", \"You like the Bert Tokenizer?\")\n",
    "print(tok_results)\n",
    "print(tokenizer.decode(tok_results['input_ids']))\n",
    "print(tokenizer.pad_token_id, tokenizer.pad_token_type_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BART\n",
    "tokenizer = BartTokenizer.from_pretrained(\"bart-large-cnn\")\n",
    "tok_results =  tokenizer.encode_plus(\"Hi!\", \"You like the Bert Tokenizer?\")\n",
    "print(tok_results)\n",
    "print(tokenizer.decode(tok_results['input_ids']))\n",
    "print(tokenizer.pad_token_id, tokenizer.pad_token_type_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tok_results =  tokenizer.encode_plus(\"Hi!\", \"You like the Bert Tokenizer?\")\n",
    "print(tok_results)\n",
    "print(tokenizer.decode(tok_results['input_ids']))\n",
    "print(tokenizer.pad_token_id, tokenizer.pad_token_type_id)\n",
    "print(tokenizer.prepare_for_model([101, 8790, 106, 102, 1192, 1176, 1103, 15035, 1706, 6378, 17260, 136, 102],None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTRL\n",
    "tokenizer = CTRLTokenizer.from_pretrained(\"ctrl\")\n",
    "tok_results =  tokenizer.encode_plus(\"Hi!\", \"You like the Bert Tokenizer?\")\n",
    "print(tok_results)\n",
    "print(tokenizer.decode(tok_results['input_ids']))\n",
    "print(tokenizer.pad_token_id, tokenizer.pad_token_type_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAMBERT\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
    "tok_results =  tokenizer.encode_plus(\"Hi!\", \"You like the Bert Tokenizer?\")\n",
    "print(tok_results)\n",
    "print(tokenizer.decode(tok_results['input_ids']))\n",
    "print(tokenizer.pad_token_id, tokenizer.pad_token_type_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELECTRA\n",
    "tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')\n",
    "tok_results =  tokenizer.encode_plus(\"Hi!\", \"You like the Bert Tokenizer?\")\n",
    "print(tok_results)\n",
    "print(tokenizer.decode(tok_results['input_ids']))\n",
    "print(tokenizer.pad_token_id, tokenizer.pad_token_type_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tok_results =  tokenizer.encode_plus(\"Hi!\", \"You like the Bert Tokenizer?\")\n",
    "print(tok_results)\n",
    "print(tokenizer.decode(tok_results['input_ids']))\n",
    "print(tokenizer.pad_token_id, tokenizer.pad_token_type_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
    "tok_results =  tokenizer.encode_plus(\"Hi!\", \"You like the Bert Tokenizer?\")\n",
    "print(tok_results)\n",
    "print(tokenizer.decode(tok_results['input_ids']))\n",
    "print(tokenizer.pad_token_id, tokenizer.pad_token_type_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "tok_results =  tokenizer.encode_plus(\"Hi!\", \"You like the Bert Tokenizer?\")\n",
    "print(tok_results)\n",
    "print(tokenizer.decode(tok_results['input_ids']))\n",
    "print(tokenizer.pad_token_id, tokenizer.pad_token_type_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T5\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "tok_results =  tokenizer.encode_plus(\"Hi!\", \"You like the Bert Tokenizer?\")\n",
    "print(tok_results)\n",
    "print(tokenizer.decode(tok_results['input_ids']))\n",
    "print(tokenizer.pad_token_id, tokenizer.pad_token_type_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TransfoXLTokenizer\n",
    "tokenizer = TransfoXLTokenizer.from_pretrained('transfo-xl-wt103')\n",
    "tok_results =  tokenizer.encode_plus(\"Hi!\", \"You like the Bert Tokenizer?\", add_space_before_punct_symbol=True)\n",
    "print(tok_results)\n",
    "print(tokenizer.decode(tok_results['input_ids']))\n",
    "print(tokenizer.pad_token_id, tokenizer.pad_token_type_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XLMRobertaTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XLM\n",
    "tokenizer = XLMTokenizer.from_pretrained(\"xlm-mlm-en-2048\")\n",
    "tok_results =  tokenizer.encode_plus(\"Hi!\", None)\n",
    "print(tok_results)\n",
    "print(tokenizer.decode(tok_results['input_ids']))\n",
    "print(tokenizer.pad_token_id, tokenizer.pad_token_type_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XLNet\n",
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "tok_results =  tokenizer.encode_plus(\"Hi! what do you thing of this thing we are doing\")\n",
    "\n",
    "print(tok_results)\n",
    "print(tokenizer.pad_token_id, tokenizer.pad_token_type_id)\n",
    "\n",
    "tokenizer.batch_encode_plus(['Hi! what do you thing of this thing we are doing'], \n",
    "                            max_length=10, stride=5,\n",
    "                            pad_to_max_length=True,\n",
    "                               return_overflowing_tokens=True,\n",
    "                               return_special_tokens_masks=True,\n",
    "                            return_input_lengths=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_ids = tokenizer.encode(\"Hi!\", \"You like the Bert Tokenizer?\")\n",
    "print(encoded_ids)\n",
    "toks = tokenizer.convert_ids_to_tokens(encoded_ids)\n",
    "print(toks)\n",
    "sep_idxs = [idx for idx, tok in enumerate(toks) if tok == tokenizer.sep_token]\n",
    "print(len(sep_idxs), sep_idxs)\n",
    "toks_modified = toks if len(sep_idxs) == 1 else [toks[:sep_idxs[0]+1], toks[sep_idxs[0]+1:]]\n",
    "print(toks_modified)\n",
    "tokenizer.get_special_tokens_mask(*toks_modified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode('.', add_special_tokens=False)\n",
    "tokenizer.get_vocab()['.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_a =tokenizer.tokenize(\"Hi!\")\n",
    "tok_b =tokenizer.tokenize(\"You like the Bert Tokenizer?\")\n",
    "tok_a, tok_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"Hi!\"))\n",
    "b = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"You like the Bert Tokenizer?\"))\n",
    "\n",
    "print(tokenizer.build_inputs_with_special_tokens(a,b))\n",
    "print(tokenizer.create_token_type_ids_from_sequences(a,b))\n",
    "\n",
    "# ddd = tokenizer.build_inputs_with_special_tokens(a,b)\n",
    "# [0 if idx == tokenizer.pad_token_id else 1 for idx in ddd]\n",
    "\n",
    "tokenizer.pad_token_id, tokenizer.pad_token_type_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = tokenizer.prepare_for_model(a,b, max_length=25, pad_to_max_length=True, return_tensors='pt')\n",
    "e = tokenizer.prepare_for_model(a,b, max_length=25, pad_to_max_length=True, return_tensors='pt')\n",
    "f = tokenizer.prepare_for_model(a,b, max_length=25, pad_to_max_length=True, return_tensors='pt')\n",
    "\n",
    "x = [d['input_ids'], e['input_ids'], f['input_ids']]\n",
    "d['input_ids'].shape, torch.cat(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"See the models docstrings for the detail of the inputs\" ... `outputs = model(tokens_tensor, token_type_ids=segments_tensors)`\n",
    "\n",
    "\"Transformers models always output tuples. See the models docstrings for the detail of all the outputs. In our case, the first element is the hidden state of the last layer of the Bert model\" ... `encoded_layers = outputs[0]`\n",
    "\n",
    "`GPT-2`, `GPT`, `XLNet`, `Transfo-XL`, `CTRL` (and some others) \"make use of a `past` or `mems` attribute which can be used to prevent re-computing the key/value pairs when using sequential decoding. It is useful when generating sequences as a big part of the attention mechanism benefits from previous computations.\"\n",
    "\n",
    "\"If you want to fine-tune a model on a specific task, you can leverage one of the `run_$TASK.py` script in the examples directory.\n",
    "\n",
    "**AutoModel**:\n",
    "\"These examples leverage auto-models, which are classes that will instantiate a model according to a given checkpoint, automatically selecting the correct model architecture. Please check the `AutoModel` documentation for more information\"\n",
    "- AutoConfig\n",
    "- AutoTokenizer\n",
    "- AutoModel\n",
    "- AutoModelForPreTraining\n",
    "- AutoModelWithLMHead\n",
    "- AutoModelForQuestionAnswering\n",
    "- AutoModelForSequenceClassification\n",
    "- AutoModelForTokenClassification\n",
    "\n",
    "**Inference**:\n",
    "\n",
    "Option 1: Use `Pipelines`\n",
    "\n",
    "Option 2: Use the model directly with the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question-Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
    "input_ids = tokenizer.encode(question, text)\n",
    "token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]\n",
    "start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([101,\n",
       "  2040,\n",
       "  2001,\n",
       "  3958,\n",
       "  27227,\n",
       "  1029,\n",
       "  102,\n",
       "  3958,\n",
       "  27227,\n",
       "  2001,\n",
       "  1037,\n",
       "  3835,\n",
       "  13997,\n",
       "  102],\n",
       " torch.Size([1, 14]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids, start_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a nice puppet'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "answer = ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1])\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLMTokenizer.from_pretrained('xlm-mlm-en-2048')\n",
    "model = XLMForQuestionAnsweringSimple.from_pretrained('xlm-mlm-en-2048')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
    "input_ids = tokenizer.encode(question, text, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(torch.tensor([input_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "answer = ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1])\n",
    "answer, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(tokenizer.encode(\"Who was Jim Henson?\", \"Jim Henson was a nice puppet\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "start_positions = torch.tensor([1])\n",
    "end_positions = torch.tensor([3])\n",
    "outputs = model(input_ids)\n",
    "# loss = outputs[0]\n",
    "torch.argmax(outputs[1]), len(input_ids[0]), input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigating nn hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_groups = hft_splitter(temp_arch, tmp_model)\n",
    "# print(len(layer_groups))\n",
    "\n",
    "# for g in layer_groups:\n",
    "#     print(len(g))\n",
    "\n",
    "# layer_groups[3][3].shape\n",
    "\n",
    "# tmp_model\n",
    "\n",
    "# for g in layer_groups:\n",
    "#     print(len(g))\n",
    "\n",
    "# x = list(hft_model.named_children())[0]\n",
    "\n",
    "# len(list(x[1].named_children()))\n",
    "# for m in x[1].named_children():\n",
    "#     print(m[0])\n",
    "\n",
    "# for m in tmp_model.named_children():\n",
    "#     print(m[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

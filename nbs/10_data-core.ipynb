{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core: Data\n",
    "\n",
    "> The `data.core` module contains the core bits required to use fast.ai's low-level and/or mid-level APIs to define `Datasets` and build `DataLoaders` suitable for training transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp data.core\n",
    "# |default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "from __future__ import annotations\n",
    "\n",
    "import gc, importlib, sys, traceback\n",
    "\n",
    "from accelerate.logging import get_logger\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "from fastai.callback.all import *\n",
    "from fastai.imports import *\n",
    "from fastai.learner import *\n",
    "from fastai.losses import CrossEntropyLossFlat\n",
    "from fastai.data.block import TransformBlock\n",
    "from fastai.data.transforms import TfmdDL\n",
    "from fastai.text.data import SortedDL\n",
    "from fastai.torch_core import *\n",
    "from fastai.torch_imports import *\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    PretrainedConfig,\n",
    "    PreTrainedTokenizerBase,\n",
    "    PreTrainedModel,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "from transformers import logging as hf_logging\n",
    "from transformers.data.data_collator import DataCollatorWithPadding\n",
    "\n",
    "from blurr.utils import clean_memory, get_hf_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "import pdb\n",
    "from datasets import Dataset\n",
    "from fastai.data.transforms import DataLoader, DataLoaders, Datasets, ItemTransform\n",
    "from fastai.losses import BaseLoss, BCEWithLogitsLossFlat\n",
    "from datasets import concatenate_datasets, load_dataset, Value\n",
    "from fastai.data.block import CategoryBlock, ColReader, ColSplitter, DataBlock, FuncSplitter, MultiCategoryBlock\n",
    "from fastcore.test import *\n",
    "import nbdev\n",
    "\n",
    "from blurr.utils import print_versions, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |export\n",
    "# silence all the HF warnings and load environment variables\n",
    "warnings.simplefilter(\"ignore\")\n",
    "hf_logging.set_verbosity_error()\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #0: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "# |hide\n",
    "# |notest\n",
    "torch.cuda.set_device(0)\n",
    "print(f\"Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we're running with at the time this documentation was generated:\n",
      "torch: 1.13.1\n",
      "fastai: 2.7.11\n",
      "transformers: 4.26.1\n"
     ]
    }
   ],
   "source": [
    "# | echo: false\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(\"What we're running with at the time this documentation was generated:\")\n",
    "print_versions(\"torch fastai transformers\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll use a subset of `imdb` to demonstrate how to configure your BLURR for sequence classification tasks. **BLURR** is designed to work with Hugging Face `Dataset` and/or pandas `DataFrame` objects"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/wgilliam/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bfb359371cb4da89f19b0f20c7129c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 200\n",
      "1000 200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Having read some of the other comments here I was expecting something truly awful but was pleasantly surprised. REALITY CHECK: The original series wasn't that good. I think some people remember it with more affection than it deserved but apart from the car chases and Daisy Duke's legs the scripts were weak and poorly acted. The Duke boys were too intelligent and posh for backwood hicks, the shrunken Boss Hog was too cretinous to be evil and Rosco was just hyper throughout every screen moment. It's amazing the series actually lasted as long as it did because it ran out of story lines during...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dani(Reese Witherspoon) has always been very close with her older sister Maureen(Emily Warfield) until they both start falling in love with their neighbor Court(Jason London). But it is not after a terrible tragedy strikes that the two sisters realize that nothing can keep them apart and that their love for each other will never fade away.&lt;br /&gt;&lt;br /&gt;This was truly a heartbreaking story about first love. Probably the most painful story about young love that I have ever seen. All the acting is amazing and Reese Witherspoon gives a great performance in her first movie. I would give The Man i...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BASEketball is awesome! It's hilarious and so damned funny that you will wet your pants laughing. I have seen it so many times I have stopped counting. But everytime it gets funnier.&lt;br /&gt;&lt;br /&gt;Trust me on this one...BASEketball is a surefire hit and I loved it and will continue to love it. I hope one day there will be a special edition DVD brought out!!!&lt;br /&gt;&lt;br /&gt;Ten Thumbs Up!!!</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I enjoyed a lot watching this movie. It has a great direction, by the already know Bigas Luna, born in Spain. And it is precisely in Spain that the movie takes place, in Cataluña, to be more precise.&lt;br /&gt;&lt;br /&gt;Luna explores once more the theme of an obcession, in this case the obcession of a young boy for the women's milk. There are some psychological concepts in this story such as the rejection complex that the elder son feels with the birth of his brother. In the movie this is what leads to the obcession of the young boy who suddenly sees all his mother's milk go to the recently born so...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This was a very good film. I didn't go into it with very high expectations and was pleasantly surprised by the acting, the script, and the scenery. Miranda Richardson was fantastic and so was Joan Plowright. They stole the show. But the other actors played their parts wonderfully also. Very enjoyable film.</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0  Having read some of the other comments here I was expecting something truly awful but was pleasantly surprised. REALITY CHECK: The original series wasn't that good. I think some people remember it with more affection than it deserved but apart from the car chases and Daisy Duke's legs the scripts were weak and poorly acted. The Duke boys were too intelligent and posh for backwood hicks, the shrunken Boss Hog was too cretinous to be evil and Rosco was just hyper throughout every screen moment. It's amazing the series actually lasted as long as it did because it ran out of story lines during...   \n",
       "1  Dani(Reese Witherspoon) has always been very close with her older sister Maureen(Emily Warfield) until they both start falling in love with their neighbor Court(Jason London). But it is not after a terrible tragedy strikes that the two sisters realize that nothing can keep them apart and that their love for each other will never fade away.<br /><br />This was truly a heartbreaking story about first love. Probably the most painful story about young love that I have ever seen. All the acting is amazing and Reese Witherspoon gives a great performance in her first movie. I would give The Man i...   \n",
       "2                                                                                                                                                                                                                        BASEketball is awesome! It's hilarious and so damned funny that you will wet your pants laughing. I have seen it so many times I have stopped counting. But everytime it gets funnier.<br /><br />Trust me on this one...BASEketball is a surefire hit and I loved it and will continue to love it. I hope one day there will be a special edition DVD brought out!!!<br /><br />Ten Thumbs Up!!!   \n",
       "3  I enjoyed a lot watching this movie. It has a great direction, by the already know Bigas Luna, born in Spain. And it is precisely in Spain that the movie takes place, in Cataluña, to be more precise.<br /><br />Luna explores once more the theme of an obcession, in this case the obcession of a young boy for the women's milk. There are some psychological concepts in this story such as the rejection complex that the elder son feels with the birth of his brother. In the movie this is what leads to the obcession of the young boy who suddenly sees all his mother's milk go to the recently born so...   \n",
       "4                                                                                                                                                                                                                                                                                                      This was a very good film. I didn't go into it with very high expectations and was pleasantly surprised by the acting, the script, and the scenery. Miranda Richardson was fantastic and so was Joan Plowright. They stole the show. But the other actors played their parts wonderfully also. Very enjoyable film.   \n",
       "\n",
       "   label  is_valid  \n",
       "0      1     False  \n",
       "1      1     False  \n",
       "2      1     False  \n",
       "3      1     False  \n",
       "4      1     False  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dsd = load_dataset(\"imdb\", split=[\"train\", \"test\"])\n",
    "\n",
    "# build HF `Dataset` objects\n",
    "train_ds = imdb_dsd[0].add_column(\"is_valid\", [False] * len(imdb_dsd[0])).shuffle().select(range(1000))\n",
    "valid_ds = imdb_dsd[1].add_column(\"is_valid\", [True] * len(imdb_dsd[1])).shuffle().select(range(200))\n",
    "imdb_ds = concatenate_datasets([train_ds, valid_ds])\n",
    "\n",
    "# build a `DataFrame` representation as well\n",
    "imdb_df = pd.DataFrame(imdb_ds)\n",
    "\n",
    "print(len(train_ds), len(valid_ds))\n",
    "print(len(imdb_df[imdb_df[\"is_valid\"] == False]), len(imdb_df[imdb_df[\"is_valid\"] == True]))\n",
    "imdb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = imdb_dsd[0].features[\"label\"].names\n",
    "label_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset civil_comments (/home/wgilliam/.cache/huggingface/datasets/civil_comments/default/0.9.0/e7a3aacd2ab7d135fa958e7209d10b1fa03807d44c486e3c34897aa08ea8ffab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f92f5cbac55247df895d46eb10bb81f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c81bd3bb4d95495da15f416d96ebd174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d697fb53bea04cd7b7b9aceacb0e0526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1dcd472b64248b9abf5874e555287c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0e4d7ab9cd4eb7ba852454da773996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 200\n",
      "1000 200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Like out of a Seinfeld episode......</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If you’re proud to be American and Alaskan, then clean up your damn mess, it’s embarrassing to you, your neighbors, the city and state.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I wonder what happened here. This is not a steep or dangerous trail.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not that I want to give the communist party of China any ideas, but when will it learn that silencing sources of dissent can only be accomplished by removing the senses, disabling speech, the ability to write, to see, to be mobile, to think.  mao all ready tried to convert populations in China to human forms of machinery when he worked and starved so many to death with the policy of the great leap forward. A policy that wanted to change people's names to numbers.  What did China fail to learn from the stalinist policies such as The Holodomor and the soviet political pogroms of the 30's.  W...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>And the completed forms provide the basis for members of the public to independently assess any potential conflicts. Opportunity for some community people to make more than 25.00 an hour and be able to afford to live here. Start a non-profit. To independently assess any potential conflicts. Have a constructive voice in ypur community. Prevention.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Like out of a Seinfeld episode......   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  If you’re proud to be American and Alaskan, then clean up your damn mess, it’s embarrassing to you, your neighbors, the city and state.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     I wonder what happened here. This is not a steep or dangerous trail.   \n",
       "3  Not that I want to give the communist party of China any ideas, but when will it learn that silencing sources of dissent can only be accomplished by removing the senses, disabling speech, the ability to write, to see, to be mobile, to think.  mao all ready tried to convert populations in China to human forms of machinery when he worked and starved so many to death with the policy of the great leap forward. A policy that wanted to change people's names to numbers.  What did China fail to learn from the stalinist policies such as The Holodomor and the soviet political pogroms of the 30's.  W...   \n",
       "4                                                                                                                                                                                                                                                             And the completed forms provide the basis for members of the public to independently assess any potential conflicts. Opportunity for some community people to make more than 25.00 an hour and be able to afford to live here. Start a non-profit. To independently assess any potential conflicts. Have a constructive voice in ypur community. Prevention.   \n",
       "\n",
       "   toxicity  severe_toxicity  obscene  threat  insult  identity_attack  \\\n",
       "0         0                0        0       0       0                0   \n",
       "1         1                0        1       0       0                0   \n",
       "2         0                0        0       0       0                0   \n",
       "3         0                0        0       0       0                0   \n",
       "4         0                0        0       0       0                0   \n",
       "\n",
       "   sexual_explicit  is_valid  \n",
       "0                0     False  \n",
       "1                0     False  \n",
       "2                0     False  \n",
       "3                0     False  \n",
       "4                0     False  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "civil_dsd = load_dataset(\"civil_comments\", split=[\"train\", \"validation\"])\n",
    "\n",
    "# round the floats\n",
    "civil_label_names = [\"toxicity\", \"severe_toxicity\", \"obscene\", \"threat\", \"insult\", \"identity_attack\", \"sexual_explicit\"]\n",
    "\n",
    "\n",
    "def round_targs(example):\n",
    "    for lbl in civil_label_names:\n",
    "        example[lbl] = np.round(example[lbl])\n",
    "    return example\n",
    "\n",
    "\n",
    "# convert floats to ints\n",
    "def fix_dtypes(ds):\n",
    "    new_features = ds.features.copy()\n",
    "    for lbl in civil_label_names:\n",
    "        new_features[lbl] = Value(\"int32\")\n",
    "    return ds.cast(new_features)\n",
    "\n",
    "\n",
    "# build HF `Dataset` objects\n",
    "civil_train_ds = civil_dsd[0].add_column(\"is_valid\", [False] * len(civil_dsd[0])).shuffle().select(range(1000))\n",
    "civil_train_ds = civil_train_ds.map(round_targs)\n",
    "civil_train_ds = fix_dtypes(civil_train_ds)\n",
    "\n",
    "civil_valid_ds = civil_dsd[1].add_column(\"is_valid\", [True] * len(civil_dsd[1])).shuffle().select(range(200))\n",
    "civil_valid_ds = civil_valid_ds.map(round_targs)\n",
    "civil_valid_ds = fix_dtypes(civil_valid_ds)\n",
    "\n",
    "civil_ds = concatenate_datasets([civil_train_ds, civil_valid_ds])\n",
    "\n",
    "# build a `DataFrame` representation as well\n",
    "civil_df = pd.DataFrame(civil_ds)\n",
    "\n",
    "print(len(civil_train_ds), len(civil_valid_ds))\n",
    "print(len(civil_df[civil_df[\"is_valid\"] == False]), len(civil_df[civil_df[\"is_valid\"] == True]))\n",
    "civil_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task specific functions\n",
    "\n",
    "The below functions provide a basic way to fetch your Hugging Face objects and pretokenize your inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def get_task_hf_objects(pretrained_model_name: str, label_names: list[str] = [\"neg\", \"pos\"], verbose: bool = False):\n",
    "    \"\"\"A helper function for getting the Hugging Face objects that works out of the box for most sequence classification tasks\"\"\"\n",
    "    model_cls = AutoModelForSequenceClassification\n",
    "    n_label_names = len(label_names)\n",
    "\n",
    "    hf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(\n",
    "        pretrained_model_name, model_cls=model_cls, config_kwargs={\"num_labels\": n_label_names}\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)\n",
    "\n",
    "        print(\"=== config ===\")\n",
    "        print(f\"# of labels:\\t{hf_config.num_labels}\")\n",
    "        print(\"\")\n",
    "        print(\"=== tokenizer ===\")\n",
    "        print(f\"Vocab size:\\t\\t{hf_tokenizer.vocab_size}\")\n",
    "        print(f\"Max # of tokens:\\t{hf_tokenizer.model_max_length}\")\n",
    "        print(f\"Attributes expected by model in forward pass:\\t{hf_tokenizer.model_input_names}\")\n",
    "\n",
    "    return hf_arch, hf_config, hf_tokenizer, hf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/data/token_classification.py#L57){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_task_hf_objects\n",
       "\n",
       ">      get_task_hf_objects (pretrained_model_name:str,\n",
       ">                           label_names:list[str]=['neg', 'pos'],\n",
       ">                           verbose:bool=False)\n",
       "\n",
       "A helper function for getting the Hugging Face objects that works out of the box for most sequence classification tasks"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/data/token_classification.py#L57){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_task_hf_objects\n",
       "\n",
       ">      get_task_hf_objects (pretrained_model_name:str,\n",
       ">                           label_names:list[str]=['neg', 'pos'],\n",
       ">                           verbose:bool=False)\n",
       "\n",
       "A helper function for getting the Hugging Face objects that works out of the box for most sequence classification tasks"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdev.show_doc(get_task_hf_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\n",
    "    \"microsoft/deberta-v3-small\", [\"great\", \"good\", \"bad\", \"horrific\"], verbose=False\n",
    ")\n",
    "\n",
    "test_eq(hf_arch, \"deberta_v2\")\n",
    "test_eq(hf_config.num_labels, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def multiclass_tokenize_func(\n",
    "    examples,\n",
    "    hf_tokenizer: PreTrainedTokenizerBase,\n",
    "    text_attr: str = \"text\",\n",
    "    text_pair_attr: str = None,\n",
    "    max_length: int = None,\n",
    "    padding: bool | str = True,\n",
    "    truncation: bool | str = True,\n",
    "    tok_kwargs: dict = {},\n",
    "):\n",
    "    \"\"\"A tokenization function that works out of the box for most multiclassification tasks\"\"\"\n",
    "    txts = [examples[text_attr]] if text_pair_attr is None else [examples[text_attr], examples[text_pair_attr]]\n",
    "    return hf_tokenizer(*txts, max_length=max_length, padding=padding, truncation=truncation, **tok_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### multiclass_tokenize_func\n",
       "\n",
       ">      multiclass_tokenize_func (examples, hf_tokenizer:transformers.tokenizatio\n",
       ">                                n_utils_base.PreTrainedTokenizerBase,\n",
       ">                                text_attr:str='text', text_pair_attr:str=None,\n",
       ">                                max_length:int=None, padding:bool|str=True,\n",
       ">                                truncation:bool|str=True, tok_kwargs:dict={})\n",
       "\n",
       "A tokenization function that works out of the box for most multiclassification tasks"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### multiclass_tokenize_func\n",
       "\n",
       ">      multiclass_tokenize_func (examples, hf_tokenizer:transformers.tokenizatio\n",
       ">                                n_utils_base.PreTrainedTokenizerBase,\n",
       ">                                text_attr:str='text', text_pair_attr:str=None,\n",
       ">                                max_length:int=None, padding:bool|str=True,\n",
       ">                                truncation:bool|str=True, tok_kwargs:dict={})\n",
       "\n",
       "A tokenization function that works out of the box for most multiclassification tasks"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdev.show_doc(multiclass_tokenize_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb86e59310ff46b79b8870c7ee61c612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_dict = {\"id\": [0, 1, 2], \"text\": [\"This is great!\", \"This is so horrible\", \"It was, uh, kinda meh.\"], \"label\": [1, 0, 0]}\n",
    "test_ds = Dataset.from_dict(my_dict)\n",
    "\n",
    "tokenize_func = partial(multiclass_tokenize_func, hf_tokenizer=hf_tokenizer)\n",
    "proc_test_ds = test_ds.map(tokenize_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_example = proc_test_ds[0]\n",
    "\n",
    "test_eq(isinstance(proc_example, dict), True)\n",
    "test_eq(\"input_ids\" in proc_example.keys(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def multilabel_tokenize_func(\n",
    "    examples,\n",
    "    hf_tokenizer: PreTrainedTokenizerBase,\n",
    "    label_attrs: list[str],\n",
    "    text_attr: str = \"text\",\n",
    "    text_pair_attr: str = None,\n",
    "    max_length: int = None,\n",
    "    padding: bool | str = True,\n",
    "    truncation: bool | str = True,\n",
    "    tok_kwargs: dict = {},\n",
    "):\n",
    "    \"\"\"A tokenization function that works out of the box for most multilabel classification tasks\"\"\"\n",
    "    txts = [examples[text_attr]] if text_pair_attr is None else [examples[text_attr], examples[text_pair_attr]]\n",
    "    inputs = hf_tokenizer(*txts, max_length=max_length, padding=padding, truncation=truncation, **tok_kwargs)\n",
    "\n",
    "    label_names = torch.stack([tensor(examples[lbl]) for lbl in label_attrs], dim=-1)\n",
    "    inputs[\"label\"] = label_names\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### multilabel_tokenize_func\n",
       "\n",
       ">      multilabel_tokenize_func (examples, hf_tokenizer:transformers.tokenizatio\n",
       ">                                n_utils_base.PreTrainedTokenizerBase,\n",
       ">                                label_attrs:list[str], text_attr:str='text',\n",
       ">                                text_pair_attr:str=None, max_length:int=None,\n",
       ">                                padding:bool|str=True,\n",
       ">                                truncation:bool|str=True, tok_kwargs:dict={})\n",
       "\n",
       "A tokenization function that works out of the box for most multilabel classification tasks"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### multilabel_tokenize_func\n",
       "\n",
       ">      multilabel_tokenize_func (examples, hf_tokenizer:transformers.tokenizatio\n",
       ">                                n_utils_base.PreTrainedTokenizerBase,\n",
       ">                                label_attrs:list[str], text_attr:str='text',\n",
       ">                                text_pair_attr:str=None, max_length:int=None,\n",
       ">                                padding:bool|str=True,\n",
       ">                                truncation:bool|str=True, tok_kwargs:dict={})\n",
       "\n",
       "A tokenization function that works out of the box for most multilabel classification tasks"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdev.show_doc(multilabel_tokenize_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79598d58b58746eebcfffaa1632bb116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_dict = {\n",
    "    \"id\": [0, 1, 2],\n",
    "    \"text\": [\"This is great!\", \"This is so horrible\", \"It was, uh, kinda meh.\"],\n",
    "    \"label1\": [1, 0, 0],\n",
    "    \"label2\": [0, 1, 1],\n",
    "}\n",
    "\n",
    "test_ds = Dataset.from_dict(my_dict)\n",
    "\n",
    "tokenize_func = partial(multilabel_tokenize_func, hf_tokenizer=hf_tokenizer, label_attrs=[\"label1\", \"label2\"])\n",
    "proc_test_ds = test_ds.map(tokenize_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_example = proc_test_ds[0]\n",
    "\n",
    "test_eq(isinstance(proc_example, dict), True)\n",
    "test_eq(\"input_ids\" in proc_example.keys(), True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TextCollatorWithPadding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@dataclass\n",
    "class TextCollatorWithPadding:\n",
    "    def __init__(\n",
    "        self,\n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (e.b., bert, bart, etc..)\n",
    "        hf_arch: str = None,\n",
    "        # A specific configuration instance you want to use\n",
    "        hf_config: PretrainedConfig = None,\n",
    "        # A Hugging Face model\n",
    "        hf_model: PreTrainedModel = None,\n",
    "        # The number of inputs expected by your model\n",
    "        n_inp: int = 1,\n",
    "        # Defaults to use Hugging Face's DataCollatorWithPadding(tokenizer=hf_tokenizer)\n",
    "        data_collator_cls: type = DataCollatorWithPadding,\n",
    "        # kwyargs specific for the instantiation of the `data_collator`\n",
    "        data_collator_kwargs: dict = {},\n",
    "    ):\n",
    "        \"\"\"A data collation function that can be used across blurr's base, low-level, and mid-level APIs\"\"\"\n",
    "        store_attr()\n",
    "        self.hf_tokenizer = data_collator_kwargs.pop(\"tokenizer\", self.hf_tokenizer)\n",
    "        self.data_collator = data_collator_cls(tokenizer=self.hf_tokenizer, **data_collator_kwargs)\n",
    "\n",
    "    def __call__(self, features):\n",
    "        features = L(features)\n",
    "        inputs, labels, targs = [], [], []\n",
    "\n",
    "        # features contain dictionaries\n",
    "        if isinstance(features[0], dict):\n",
    "            feature_keys = list(features[0].keys())\n",
    "            inputs = [self._build_inputs_d(features, feature_keys)]\n",
    "\n",
    "            input_labels = self._build_input_labels(inputs[0], features, feature_keys)\n",
    "            if input_labels is not None:\n",
    "                labels, targs = [input_labels], [input_labels.clone()]\n",
    "        # features contains tuples, each of which can contain multiple inputs and/or targets\n",
    "        elif isinstance(features[0], tuple):\n",
    "            for f_idx in range(self.n_inp):\n",
    "                feature_keys = list(features[0][f_idx].keys())\n",
    "                inputs.append(self._build_inputs_d(features.itemgot(f_idx), feature_keys))\n",
    "\n",
    "                input_labels = self._build_input_labels(inputs[0], features.itemgot(f_idx), feature_keys)\n",
    "                labels.append(input_labels if input_labels is not None else [])\n",
    "\n",
    "            targs = [self._proc_targets(inputs[0], list(features.itemgot(f_idx))) for f_idx in range(self.n_inp, len(features[0]))]\n",
    "\n",
    "        return self._build_batch(inputs, labels, targs)\n",
    "\n",
    "    # ----- utility methods -----\n",
    "\n",
    "    # to build the inputs dictionary\n",
    "    def _build_inputs_d(self, features, feature_keys):\n",
    "        return {fwd_arg: list(features.attrgot(fwd_arg)) for fwd_arg in self.hf_tokenizer.model_input_names if fwd_arg in feature_keys}\n",
    "\n",
    "    # to build the input \"labels\"\n",
    "    def _build_input_labels(self, inputs_d, features, feature_keys):\n",
    "        if \"label\" in feature_keys:\n",
    "            labels = list(features.attrgot(\"label\"))\n",
    "            return self._proc_targets(inputs_d, labels)\n",
    "        return None\n",
    "\n",
    "    # used to give the labels/targets the right shape\n",
    "    def _proc_targets(self, inputs_d, targs):\n",
    "        if is_listy(targs[0]):\n",
    "            targs = torch.stack([tensor(lbls) for lbls in targs])\n",
    "        elif isinstance(targs[0], torch.Tensor) and len(targs[0].size()) > 0:\n",
    "            targs = torch.stack(targs)\n",
    "        else:\n",
    "            targs = torch.tensor(targs)\n",
    "\n",
    "        return targs\n",
    "\n",
    "    # will properly assemble are batch given a list of inputs, labels, and targets\n",
    "    def _build_batch(self, inputs, labels, targs):\n",
    "        batch = []\n",
    "\n",
    "        for input, input_labels in zip(inputs, labels):\n",
    "            input_d = dict(self.data_collator(input))\n",
    "            if len(input_labels) > 0:\n",
    "                input_d[\"labels\"] = input_labels\n",
    "\n",
    "            batch.append(input_d)\n",
    "\n",
    "        for targ in targs:\n",
    "            batch.append(targ)\n",
    "\n",
    "        return tuplify(batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base API: Examples\n",
    "\n",
    "This section demonstrates how you can use standard `Dataset` objects (PyTorch and Hugging Face) to build PyTorch `DataLoader`s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t2\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", label_names, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: `Dataset`s (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "263df3aa05ae48c98192cc40a66db3bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd995f0c00044da3be1b22733bc07f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "tokenize_func = partial(multiclass_tokenize_func, hf_tokenizer=hf_tokenizer)\n",
    "proc_train_ds = train_ds.map(tokenize_func, batched=True)\n",
    "proc_valid_ds = valid_ds.map(tokenize_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our PyTorch Dataset class\n",
    "class HFTextClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset, hf_tokenizer):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.hf_tokenizer = hf_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.hf_dataset[idx]\n",
    "        return item\n",
    "\n",
    "\n",
    "# build our PyTorch training and validation Datasets\n",
    "pt_proc_train_ds = HFTextClassificationDataset(proc_train_ds, hf_tokenizer=hf_tokenizer)\n",
    "pt_proc_valid_ds = HFTextClassificationDataset(proc_valid_ds, hf_tokenizer=hf_tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoader`s (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build your fastai `DataLoaders` from Pytorch `DataLoader` objects\n",
    "batch_size = 4\n",
    "data_collator = TextCollatorWithPadding(hf_tokenizer)\n",
    "train_dl = torch.utils.data.DataLoader(pt_proc_train_ds, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "valid_dl = torch.utils.data.DataLoader(pt_proc_valid_ds, batch_size=batch_size * 2, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "dls = DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of batches in train|validation dataloaders: 250 25\n",
      "# of items in each batch: 2\n",
      "\n",
      "Decoded input_ids: [CLS] Berlin-born in 1942 Margarethe von Trott ... \n",
      "Targets: tensor([1, 1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "print(\"# of batches in train|validation dataloaders:\", len(train_dl), len(valid_dl))\n",
    "\n",
    "b = next(iter(train_dl))\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"\")\n",
    "print(f\"Decoded input_ids: {hf_tokenizer.decode(b[0]['input_ids'][0][:10])} ... \")\n",
    "print(\"Targets:\", b[1])\n",
    "\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOPE: Won't work with PyTorch DataLoaders\n",
    "# AttributeError: 'DataLoader' object has no attribute 'show_batch'\n",
    "# dls.show_batch(dataloaders=dls, max_n=2, trunc_at=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilabel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t7\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", civil_label_names, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: `Dataset`s (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a77a9538c2ce48df885d0dec6d067cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9bc27e65b7f41f1911b09f2d6aeab78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "tokenize_func = partial(multilabel_tokenize_func, hf_tokenizer=hf_tokenizer, label_attrs=civil_label_names)\n",
    "proc_civil_train_ds = civil_train_ds.map(tokenize_func, batched=True)\n",
    "proc_civil_valid_ds = civil_valid_ds.map(tokenize_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our PyTorch Dataset class\n",
    "class HFTextMultilabelClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset, hf_tokenizer, label_names):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.hf_tokenizer = hf_tokenizer\n",
    "        self.label_names = label_names\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.hf_dataset[idx]\n",
    "        # item[\"label\"] = [item[lbl] for lbl in self.label_names]\n",
    "        return item\n",
    "\n",
    "\n",
    "# build our PyTorch training and validation Datasets\n",
    "pt_proc_civil_train_ds = HFTextMultilabelClassificationDataset(\n",
    "    proc_civil_train_ds, hf_tokenizer=hf_tokenizer, label_names=civil_label_names\n",
    ")\n",
    "pt_proc_civil_valid_ds = HFTextMultilabelClassificationDataset(\n",
    "    proc_civil_valid_ds, hf_tokenizer=hf_tokenizer, label_names=civil_label_names\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoader`s (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build your fastai `DataLoaders` from Pytorch `DataLoader` objects\n",
    "batch_size = 4\n",
    "data_collator = TextCollatorWithPadding(hf_tokenizer)\n",
    "train_dl = torch.utils.data.DataLoader(pt_proc_civil_train_ds, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "valid_dl = torch.utils.data.DataLoader(pt_proc_civil_valid_ds, batch_size=batch_size * 2, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "dls = DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of batches in train|validation dataloaders: 250 25\n",
      "# of items in each batch: 2\n",
      "\n",
      "Decoded input_ids: [CLS] Because conservatives all across Canada really really want to ... \n",
      "Targets: tensor([[0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(\"# of batches in train|validation dataloaders:\", len(train_dl), len(valid_dl))\n",
    "\n",
    "b = next(iter(train_dl))\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"\")\n",
    "print(f\"Decoded input_ids: {hf_tokenizer.decode(b[0]['input_ids'][0][:10])} ... \")\n",
    "print(\"Targets:\", b[1])\n",
    "\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t2\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", label_names, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: `Datasets` (huggingface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/wgilliam/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-4d70714e74eab81b.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01bdefe758a64836a7936dcd286194fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "tokenize_func = partial(multiclass_tokenize_func, hf_tokenizer=hf_tokenizer)\n",
    "proc_train_ds = train_ds.map(tokenize_func, batched=True)\n",
    "proc_valid_ds = valid_ds.map(tokenize_func, batched=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoader`s (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build your fastai `DataLoaders` from Pytorch `DataLoader` objects\n",
    "batch_size = 4\n",
    "data_collator = TextCollatorWithPadding(hf_tokenizer)\n",
    "train_dl = torch.utils.data.DataLoader(proc_train_ds, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "valid_dl = torch.utils.data.DataLoader(proc_valid_ds, batch_size=batch_size * 2, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "dls = DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of batches in train|validation dataloaders: 250 25\n",
      "# of items in each batch: 2\n",
      "\n",
      "Decoded input_ids: [CLS] While this is a faithful adaptation, it is ... \n",
      "Targets: tensor([1, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(\"# of batches in train|validation dataloaders:\", len(train_dl), len(valid_dl))\n",
    "\n",
    "b = next(iter(train_dl))\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"\")\n",
    "print(f\"Decoded input_ids: {hf_tokenizer.decode(b[0]['input_ids'][0][:10])} ... \")\n",
    "print(\"Targets:\", b[1])\n",
    "\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t7\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", civil_label_names, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: `Datasets` (huggingface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/wgilliam/.cache/huggingface/datasets/civil_comments/default/0.9.0/e7a3aacd2ab7d135fa958e7209d10b1fa03807d44c486e3c34897aa08ea8ffab/cache-0e3bd91ca9581868.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac58fd3aab545e2b642624ad0eb4b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "tokenize_func = partial(multilabel_tokenize_func, hf_tokenizer=hf_tokenizer, label_attrs=civil_label_names)\n",
    "proc_civil_train_ds = civil_train_ds.map(tokenize_func, batched=True)\n",
    "proc_civil_valid_ds = civil_valid_ds.map(tokenize_func, batched=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoader`s (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build your fastai `DataLoaders` from Pytorch `DataLoader` objects\n",
    "batch_size = 4\n",
    "data_collator = TextCollatorWithPadding(hf_tokenizer)\n",
    "train_dl = torch.utils.data.DataLoader(proc_civil_train_ds, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "valid_dl = torch.utils.data.DataLoader(proc_civil_valid_ds, batch_size=batch_size * 2, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "dls = DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of batches in train|validation dataloaders: 250 25\n",
      "# of items in each batch: 2\n",
      "\n",
      "Decoded input_ids: [CLS] Walter: What do you think to the links ... \n",
      "Targets: tensor([[0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(\"# of batches in train|validation dataloaders:\", len(train_dl), len(valid_dl))\n",
    "\n",
    "b = next(iter(train_dl))\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"\")\n",
    "print(f\"Decoded input_ids: {hf_tokenizer.decode(b[0]['input_ids'][0][:10])} ... \")\n",
    "print(\"Targets:\", b[1])\n",
    "\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-Level API\n",
    "\n",
    "This section demonstrates how you can migrate from using PyTorch/Hugging Face to fast.ai `Datasets` and `DataLoaders` to recapture much of the fast.ai specific features unavailable when using basic PyTorch. This includes:\n",
    "\n",
    "- `DataLoaders.one_batch()`\n",
    "- `DataLoaders.show_batch()`\n",
    "- `Leaner.export()`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TextInput` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class TextInput(TensorBase):\n",
    "    \"\"\"The base represenation of your inputs; used by the various fastai `show` methods\"\"\"\n",
    "\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `TextInput` object is returned from the decodes method of `BatchDecodeTransform` as a means to customize `@typedispatch`ed functions like `DataLoaders.show_batch` and `Learner.show_results`. The value will the your \"input_ids\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `BatchDecodeTransform` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class BatchDecodeTransform(Transform):\n",
    "    \"\"\"A class used to cast your inputs as `input_return_type` for fastai `show` methods\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # A Hugging Face tokenizer (not required if passing in an instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (not required if passing in an instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_arch: str = None,\n",
    "        # A Hugging Face configuration object (not required if passing in an instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_config: PretrainedConfig = None,\n",
    "        # A Hugging Face model (not required if passing in an instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_model: PreTrainedModel = None,\n",
    "        # Used by typedispatched show methods\n",
    "        input_return_type: type = TextInput,\n",
    "        # The token ID that should be ignored when calculating the loss\n",
    "        ignore_token_id: int = CrossEntropyLossFlat().ignore_index,\n",
    "        # Any other keyword arguments\n",
    "        **kwargs,\n",
    "    ):\n",
    "        store_attr()\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def decodes(self, items: dict):\n",
    "        \"\"\"Returns the proper object and data for show related fastai methods\"\"\"\n",
    "        return self.input_return_type(items[\"input_ids\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of fastai 2.1.5, before batch transforms no longer have a `decodes` method ... and so, I've introduced a standard batch transform here, `BatchDecodeTransform`, (one that occurs \"after\" the batch has been created) that will do the decoding for us."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility classes and methods \n",
    "\n",
    "These methods are use internally for getting blurr transforms associated to your `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def get_blurr_tfm(\n",
    "    # A list of transforms (e.g., dls.after_batch, dls.before_batch, etc...)\n",
    "    tfms_list: Pipeline,\n",
    "    # The transform to find\n",
    "    tfm_class: Transform = BatchDecodeTransform,\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a fastai DataLoaders batch transforms, this method can be used to get at a transform\n",
    "    instance used in your Blurr DataBlock\n",
    "    \"\"\"\n",
    "    return next(filter(lambda el: issubclass(type(el), tfm_class), tfms_list), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/data/core.py#L197){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_blurr_tfm\n",
       "\n",
       ">      get_blurr_tfm (tfms_list:fastcore.transform.Pipeline,\n",
       ">                     tfm_class:fastcore.transform.Transform=<class\n",
       ">                     '__main__.BatchDecodeTransform'>)\n",
       "\n",
       "Given a fastai DataLoaders batch transforms, this method can be used to get at a transform\n",
       "instance used in your Blurr DataBlock\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| tfms_list | Pipeline |  | A list of transforms (e.g., dls.after_batch, dls.before_batch, etc...) |\n",
       "| tfm_class | Transform | BatchDecodeTransform | The transform to find |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/data/core.py#L197){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_blurr_tfm\n",
       "\n",
       ">      get_blurr_tfm (tfms_list:fastcore.transform.Pipeline,\n",
       ">                     tfm_class:fastcore.transform.Transform=<class\n",
       ">                     '__main__.BatchDecodeTransform'>)\n",
       "\n",
       "Given a fastai DataLoaders batch transforms, this method can be used to get at a transform\n",
       "instance used in your Blurr DataBlock\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| tfms_list | Pipeline |  | A list of transforms (e.g., dls.after_batch, dls.before_batch, etc...) |\n",
       "| tfm_class | Transform | BatchDecodeTransform | The transform to find |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdev.show_doc(get_blurr_tfm, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def first_blurr_tfm(\n",
    "    # Your fast.ai `DataLoaders\n",
    "    dls: DataLoaders,\n",
    "    # The Blurr transforms to look for in order\n",
    "    tfms: list[Transform] = [BatchDecodeTransform],\n",
    "):\n",
    "    \"\"\"\n",
    "    This convenience method will find the first Blurr transform required for methods such as\n",
    "    `show_batch` and `show_results`. The returned transform should have everything you need to properly\n",
    "    decode and 'show' your Hugging Face inputs/targets\n",
    "    \"\"\"\n",
    "    for tfm in tfms:\n",
    "        found_tfm = get_blurr_tfm(dls.before_batch, tfm_class=tfm)\n",
    "        if found_tfm:\n",
    "            return found_tfm\n",
    "\n",
    "        found_tfm = get_blurr_tfm(dls.after_batch, tfm_class=tfm)\n",
    "        if found_tfm:\n",
    "            return found_tfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/data/core.py#L211){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### first_blurr_tfm\n",
       "\n",
       ">      first_blurr_tfm (dls:fastai.data.core.DataLoaders,\n",
       ">                       tfms:list[fastcore.transform.Transform]=[<class\n",
       ">                       '__main__.BatchDecodeTransform'>])\n",
       "\n",
       "This convenience method will find the first Blurr transform required for methods such as\n",
       "`show_batch` and `show_results`. The returned transform should have everything you need to properly\n",
       "decode and 'show' your Hugging Face inputs/targets\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dls | DataLoaders |  | Your fast.ai `DataLoaders |\n",
       "| tfms | list[Transform] | [<class '__main__.BatchDecodeTransform'>] | The Blurr transforms to look for in order |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/data/core.py#L211){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### first_blurr_tfm\n",
       "\n",
       ">      first_blurr_tfm (dls:fastai.data.core.DataLoaders,\n",
       ">                       tfms:list[fastcore.transform.Transform]=[<class\n",
       ">                       '__main__.BatchDecodeTransform'>])\n",
       "\n",
       "This convenience method will find the first Blurr transform required for methods such as\n",
       "`show_batch` and `show_results`. The returned transform should have everything you need to properly\n",
       "decode and 'show' your Hugging Face inputs/targets\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dls | DataLoaders |  | Your fast.ai `DataLoaders |\n",
       "| tfms | list[Transform] | [<class '__main__.BatchDecodeTransform'>] | The Blurr transforms to look for in order |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdev.show_doc(first_blurr_tfm, title_level=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `show_batch` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@typedispatch\n",
    "def show_batch(\n",
    "    # This typedispatched `show_batch` will be called for `TextInput` typed inputs\n",
    "    x: TextInput,\n",
    "    # Your targets\n",
    "    y,\n",
    "    # Your raw inputs/targets\n",
    "    samples,\n",
    "    # Your `DataLoaders`. This is required so as to get at the Hugging Face objects for\n",
    "    # decoding them into something understandable\n",
    "    dataloaders,\n",
    "    # Your `show_batch` context\n",
    "    ctxs=None,\n",
    "    # The maximum number of items to show\n",
    "    max_n=6,\n",
    "    # Any truncation your want applied to your decoded inputs\n",
    "    trunc_at=None,\n",
    "    # Any other keyword arguments you want applied to `show_batch`\n",
    "    **kwargs,\n",
    "):\n",
    "    # grab our tokenizer\n",
    "    tfm = first_blurr_tfm(dataloaders)\n",
    "    hf_tokenizer = tfm.hf_tokenizer\n",
    "\n",
    "    # if we've included our label_names list, we'll use it to look up the value of our target(s)\n",
    "    trg_label_names = tfm.kwargs[\"label_names\"] if (\"label_names\" in tfm.kwargs) else None\n",
    "    if trg_label_names is None and dataloaders.vocab is not None:\n",
    "        trg_label_names = dataloaders.vocab\n",
    "\n",
    "    res = L()\n",
    "    n_inp = dataloaders.n_inp\n",
    "\n",
    "    n_samples = min(max_n, dataloaders.bs)\n",
    "    for idx in range(n_samples):\n",
    "        input_ids = x[idx]\n",
    "        rets = [hf_tokenizer.decode(input_ids, skip_special_tokens=True)[:trunc_at]]\n",
    "\n",
    "        sample = samples[idx] if samples is not None else None\n",
    "        for item_idx, item in enumerate(sample[n_inp:]):\n",
    "            label = y[item_idx] if y is not None else item\n",
    "\n",
    "            if torch.is_tensor(label):\n",
    "                label = list(label.numpy()) if len(label.size()) > 0 else label.item()\n",
    "\n",
    "            if is_listy(label):\n",
    "                trg = [trg_label_names[int(idx)] for idx, val in enumerate(label) if (val == 1)] if trg_label_names else label\n",
    "            else:\n",
    "                trg = trg_label_names[int(item)] if trg_label_names else item\n",
    "\n",
    "            rets.append(trg)\n",
    "        res.append(tuplify(rets))\n",
    "\n",
    "    cols = [\"text\"] + [\"target\" if (i == 0) else f\"target_{i}\" for i in range(len(res[0]) - n_inp)]\n",
    "    display_df(pd.DataFrame(res, columns=cols)[:max_n])\n",
    "    return ctxs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sorted_dl_func` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def sorted_dl_func(\n",
    "    example,\n",
    "    # A Hugging Face tokenizer\n",
    "    hf_tokenizer: PreTrainedTokenizerBase,\n",
    "    # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. \\\n",
    "    # Set this to 'True' if your inputs are pre-tokenized (not numericalized)\n",
    "    is_split_into_words: bool = False,\n",
    "    # Any other keyword arguments you want to include during tokenization\n",
    "    tok_kwargs: dict = {},\n",
    "):\n",
    "    \"\"\"This method is used by the `SortedDL` to ensure your dataset is sorted *after* tokenization\"\"\"\n",
    "    txt = None\n",
    "    if isinstance(example[0], dict):\n",
    "        if \"input_ids\" in example[0]:\n",
    "            # if inputs are pretokenized\n",
    "            return len(example[0][\"input_ids\"])\n",
    "        else:\n",
    "            txt = example[0][\"text\"]\n",
    "    else:\n",
    "        txt = example[0]\n",
    "\n",
    "    return len(txt) if is_split_into_words else len(hf_tokenizer.tokenize(txt, **tok_kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/data/core.py#L291){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### sorted_dl_func\n",
       "\n",
       ">      sorted_dl_func (example, hf_tokenizer:transformers.tokenization_utils_bas\n",
       ">                      e.PreTrainedTokenizerBase,\n",
       ">                      is_split_into_words:bool=False, tok_kwargs:dict={})\n",
       "\n",
       "This method is used by the `SortedDL` to ensure your dataset is sorted *after* tokenization\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| example |  |  |  |\n",
       "| hf_tokenizer | PreTrainedTokenizerBase |  | A Hugging Face tokenizer |\n",
       "| is_split_into_words | bool | False | The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. \\<br>Set this to 'True' if your inputs are pre-tokenized (not numericalized) |\n",
       "| tok_kwargs | dict | {} | Any other keyword arguments you want to include during tokenization |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/ohmeow/blurr/blob/dev-3.0.0 #master/blurr/data/core.py#L291){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### sorted_dl_func\n",
       "\n",
       ">      sorted_dl_func (example, hf_tokenizer:transformers.tokenization_utils_bas\n",
       ">                      e.PreTrainedTokenizerBase,\n",
       ">                      is_split_into_words:bool=False, tok_kwargs:dict={})\n",
       "\n",
       "This method is used by the `SortedDL` to ensure your dataset is sorted *after* tokenization\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| example |  |  |  |\n",
       "| hf_tokenizer | PreTrainedTokenizerBase |  | A Hugging Face tokenizer |\n",
       "| is_split_into_words | bool | False | The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. \\<br>Set this to 'True' if your inputs are pre-tokenized (not numericalized) |\n",
       "| tok_kwargs | dict | {} | Any other keyword arguments you want to include during tokenization |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdev.show_doc(sorted_dl_func, title_level=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-Level API: Examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t2\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", label_names, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: `Datasets` (fastai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c186120850455489f423a2fe662b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "tokenize_func = partial(multiclass_tokenize_func, hf_tokenizer=hf_tokenizer)\n",
    "proc_imdb_ds = imdb_ds.map(tokenize_func, batched=True)\n",
    "\n",
    "# turn Arrow into DataFrame (`ColSplitter` only works with `DataFrame`s)\n",
    "train_df = pd.DataFrame(proc_imdb_ds)\n",
    "train_df.head()\n",
    "\n",
    "# define dataset splitter\n",
    "splitter = ColSplitter(\"is_valid\")\n",
    "splits = splitter(imdb_df)\n",
    "\n",
    "\n",
    "# define how we want to build our inputs and targets\n",
    "def _build_inputs(example):\n",
    "    return {fwd_arg_name: example[fwd_arg_name] for fwd_arg_name in hf_tokenizer.model_input_names if fwd_arg_name in list(example.keys())}\n",
    "\n",
    "\n",
    "def _build_targets(example):\n",
    "    return example[\"label\"]\n",
    "\n",
    "\n",
    "# create our fastai `Datasets` object\n",
    "dsets = Datasets(items=train_df, splits=splits, tfms=[[_build_inputs], _build_targets], n_inp=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items in train|validation datasets:  1000 200\n",
      "Items in each example: 2\n",
      "Example inputs: ['input_ids', 'token_type_ids', 'attention_mask']\n",
      "Example target(s): 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Items in train|validation datasets: \", len(dsets.train), len(dsets.valid))\n",
    "\n",
    "example = dsets.valid[0]\n",
    "# example\n",
    "\n",
    "print(f\"Items in each example: {len(example)}\")\n",
    "print(f\"Example inputs: {list(example[0].keys())}\")\n",
    "print(f\"Example target(s): {example[1]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: `DataLoaders` (fastai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = TextCollatorWithPadding(hf_tokenizer)\n",
    "sort_func = partial(sorted_dl_func, hf_tokenizer=hf_tokenizer)\n",
    "batch_decode_tfm = BatchDecodeTransform(hf_tokenizer, hf_arch, hf_config, hf_model, label_names=label_names)\n",
    "\n",
    "dls = dsets.dataloaders(\n",
    "    batch_size=4,\n",
    "    create_batch=data_collator,\n",
    "    after_batch=batch_decode_tfm,\n",
    "    dl_type=partial(SortedDL, sort_func=sort_func),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of batches in train|validation dataloaders: 250 25\n",
      "# of items in each batch: 2\n",
      "\n",
      "Decoded input_ids: [CLS] For once I think that Richard Rhyner is ... \n",
      "Targets: tensor([[0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(\"# of batches in train|validation dataloaders:\", len(train_dl), len(valid_dl))\n",
    "\n",
    "b = next(iter(train_dl))\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"\")\n",
    "print(f\"Decoded input_ids: {hf_tokenizer.decode(b[0]['input_ids'][0][:10])} ... \")\n",
    "print(\"Targets:\", b[1])\n",
    "\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Having read some of the other comments here I was expecting something truly awful but was pleasantly surprised. REALITY CHECK: The original series wasn't that good. I think some people remember it with more affection than it deserved but apart from the car chases and Daisy Duke's legs the scripts were weak and poorly acted. The Duke boys were too intelligent and posh for backwood hicks, the shrunken Boss Hog was too cretinous to be evil and Rosco was just hyper throughout every screen moment. It's amazing the series actually lasted as long as it did because it ran out of story lines during the first series.&lt;br /&gt;&lt;br /&gt;Back to the movie. If you watch this film in it's own right, not as a direct comparison to however you remember the TV series, then it's not bad at all. The real star is of c</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The selection of Sylvester Stallone to perform the protagonist by Renny Harlin is commendable since Stallone is that sort of tough and craggy person who had earlier rendered the requisite audaciously versatile aura to the characters of Rocky Balbao and Rambo. But to compare Die Hard series with Cliffhanger is a far-fetched notion.&lt;br /&gt;&lt;br /&gt;The excellently crafted opening scene introduces the audience to the thrill, suspense and intrigue which is going to engulf them in the ensuing bloody and perilous encounter with the outlaws. The heist and the high altitude transfer of hard cash in suit cases from one plane to the other is something not filmed before.&lt;br /&gt;&lt;br /&gt;The biting cold of the snow capped Alps and the unfolding deceit and treachery among the antagonist forces makes one shiver w</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilabel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t7\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", civil_label_names, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: `Datasets` (fastai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d0fc90e3b647eebee12914c040921b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "tokenize_func = partial(multilabel_tokenize_func, hf_tokenizer=hf_tokenizer, label_attrs=civil_label_names)\n",
    "proc_civil_ds = civil_ds.map(tokenize_func, batched=True)\n",
    "\n",
    "# turn Arrow into DataFrame (`ColSplitter` only works with `DataFrame`s)\n",
    "train_df = pd.DataFrame(proc_civil_ds)\n",
    "train_df.head()\n",
    "\n",
    "# define dataset splitter\n",
    "splitter = ColSplitter(\"is_valid\")\n",
    "splits = splitter(civil_df)\n",
    "\n",
    "\n",
    "# define how we want to build our inputs and targets\n",
    "def _build_inputs(example):\n",
    "    return {fwd_arg_name: example[fwd_arg_name] for fwd_arg_name in hf_tokenizer.model_input_names if fwd_arg_name in list(example.keys())}\n",
    "\n",
    "\n",
    "def _build_targets(example):\n",
    "    return example[\"label\"]\n",
    "\n",
    "\n",
    "# create our fastai `Datasets` object\n",
    "dsets = Datasets(items=train_df, splits=splits, tfms=[[_build_inputs], _build_targets], n_inp=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items in train|validation datasets:  1000 200\n",
      "Items in each example: 2\n",
      "Example inputs: ['input_ids', 'token_type_ids', 'attention_mask']\n",
      "Example target(s): [0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Items in train|validation datasets: \", len(dsets.train), len(dsets.valid))\n",
    "\n",
    "example = dsets.valid[0]\n",
    "# example\n",
    "\n",
    "print(f\"Items in each example: {len(example)}\")\n",
    "print(f\"Example inputs: {list(example[0].keys())}\")\n",
    "print(f\"Example target(s): {example[1]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: `DataLoaders` (fastai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = TextCollatorWithPadding(hf_tokenizer)\n",
    "sort_func = partial(sorted_dl_func, hf_tokenizer=hf_tokenizer)\n",
    "batch_decode_tfm = BatchDecodeTransform(hf_tokenizer, hf_arch, hf_config, hf_model, label_names=civil_label_names)\n",
    "\n",
    "dls = dsets.dataloaders(\n",
    "    batch_size=4,\n",
    "    create_batch=data_collator,\n",
    "    after_batch=batch_decode_tfm,\n",
    "    dl_type=partial(SortedDL, sort_func=sort_func),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of batches in train|validation dataloaders: 250 25\n",
      "# of items in each batch: 2\n",
      "\n",
      "Decoded input_ids: [CLS] There is no data to support your contention that ... \n",
      "Targets: tensor([[0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(\"# of batches in train|validation dataloaders:\", len(train_dl), len(valid_dl))\n",
    "\n",
    "b = next(iter(train_dl))\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"\")\n",
    "print(f\"Decoded input_ids: {hf_tokenizer.decode(b[0]['input_ids'][0][:10])} ... \")\n",
    "print(\"Targets:\", b[1])\n",
    "\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Like out of a Seinfeld episode......</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The only way Lou makes a deal is if it improves the club over the long term helping to capture the Shanahan vision of achieving perrenial Cup contender status - in other words becoming the New England Patriots of the NHL!</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid-Level API\n",
    "\n",
    "BLURR's mid-level API provides a way to build your `DataLoaders` using fast.ai's mid-level `DataBlock` API utilizing one of these three tokenization strategies:\n",
    "\n",
    "1. Using pre-tokenized data (the traditional approach)\n",
    "\n",
    "2. batch-time tokenization (the default approach in previous versions of blurr)\n",
    "\n",
    "2. item-time tokenization (e.g., to apply tokenization on individual items as they are pulled from their respective `Dataset`)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `BatchTokenizeTransform` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class BatchTokenizeTransform(Transform):\n",
    "    \"\"\"\n",
    "    Handles everything you need to assemble a mini-batch of inputs and targets, as well as\n",
    "    decode the dictionary produced as a byproduct of the tokenization process in the `encodes` method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (e.b., bert, bart, etc..)\n",
    "        hf_arch: str,\n",
    "        # A specific configuration instance you want to use\n",
    "        hf_config: PretrainedConfig,\n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # A Hugging Face model\n",
    "        hf_model: PreTrainedModel,\n",
    "        # To control whether the \"include_labels\" are included in your inputs. If they are, the loss will be calculated in \\\n",
    "        # the model's forward function and you can simply use `PreCalculatedLoss` as your `Learner`'s loss function to use it\n",
    "        include_labels: bool = True,\n",
    "        # The token ID that should be ignored when calculating the loss\n",
    "        ignore_token_id: int = CrossEntropyLossFlat().ignore_index,\n",
    "        # To control the length of the padding/truncation. It can be an integer or None, \\\n",
    "        # in which case it will default to the maximum length the model can accept. \\\n",
    "        # If the model has no specific maximum input length, truncation/padding to max_length is deactivated. \\\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        max_length: int = None,\n",
    "        # To control the `padding` applied to your `hf_tokenizer` during tokenization. \\\n",
    "        # If None, will default to 'False' or 'do_not_pad'. \\\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        padding: bool | str = True,\n",
    "        # To control `truncation` applied to your `hf_tokenizer` during tokenization. \\\n",
    "        # If None, will default to 'False' or 'do_not_truncate'. \\\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        truncation: bool | str = True,\n",
    "        # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. \\\n",
    "        # Set this to 'True' if your inputs are pre-tokenized (not numericalized) \\\n",
    "        is_split_into_words: bool = False,\n",
    "        # Any other keyword arguments you want included when using your `hf_tokenizer` to tokenize your inputs\n",
    "        tok_kwargs: dict = {},\n",
    "        # Keyword arguments to apply to `BatchTokenizeTransform`\n",
    "        **kwargs,\n",
    "    ):\n",
    "        store_attr()\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def encodes(self, samples, return_batch_encoding=False):\n",
    "        \"\"\"\n",
    "        This method peforms on-the-fly, batch-time tokenization of your data. In other words, your raw inputs\n",
    "        are tokenized as needed for each mini-batch of data rather than requiring pre-tokenization of your full\n",
    "        dataset ahead of time.\n",
    "        \"\"\"\n",
    "        samples = L(samples)\n",
    "\n",
    "        # grab inputs\n",
    "        is_dict = isinstance(samples[0][0], dict)\n",
    "        test_inp = samples[0][0][\"text\"] if is_dict else samples[0][0]\n",
    "\n",
    "        if is_listy(test_inp) and not self.is_split_into_words:\n",
    "            if is_dict:\n",
    "                inps = [(item[\"text\"][0], item[\"text\"][1]) for item in samples.itemgot(0).items]\n",
    "            else:\n",
    "                inps = list(zip(samples.itemgot(0, 0), samples.itemgot(0, 1)))\n",
    "        else:\n",
    "            inps = [item[\"text\"] for item in samples.itemgot(0).items] if is_dict else samples.itemgot(0).items\n",
    "\n",
    "        inputs = self.hf_tokenizer(\n",
    "            inps,\n",
    "            max_length=self.max_length,\n",
    "            padding=self.padding,\n",
    "            truncation=self.truncation,\n",
    "            is_split_into_words=self.is_split_into_words,\n",
    "            return_tensors=\"pt\",\n",
    "            **self.tok_kwargs,\n",
    "        )\n",
    "\n",
    "        d_keys = inputs.keys()\n",
    "\n",
    "        # update the samples with tokenized inputs (e.g. input_ids, attention_mask, etc...), as well as extra information\n",
    "        # if the inputs is a dictionary.\n",
    "        # (< 2.0.0): updated_samples = [(*[{k: inputs[k][idx] for k in d_keys}], *sample[1:]) for idx, sample in enumerate(samples)]\n",
    "        updated_samples = []\n",
    "        for idx, sample in enumerate(samples):\n",
    "            inps = {k: inputs[k][idx] for k in d_keys}\n",
    "            if is_dict:\n",
    "                inps = {\n",
    "                    **inps,\n",
    "                    **{k: v for k, v in sample[0].items() if k not in [\"text\"]},\n",
    "                }\n",
    "\n",
    "            trgs = sample[1:]\n",
    "            if self.include_labels and len(trgs) > 0:\n",
    "                inps[\"label\"] = trgs[0]\n",
    "\n",
    "            updated_samples.append((*[inps], *trgs))\n",
    "\n",
    "        if return_batch_encoding:\n",
    "            return updated_samples, inputs\n",
    "\n",
    "        return updated_samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by this [article](https://docs.fast.ai/tutorial.transformers.html), `BatchTokenizeTransform` inputs can come in as raw **text**, **a list of words** (e.g., tasks like Named Entity Recognition (NER), where you want to predict the label of each token), or as a **dictionary** that includes extra information you want to use during post-processing.\n",
    "\n",
    "Part of the inspiration for this derives from the mechanics of Hugging Face tokenizers, in particular it can return a collated mini-batch of data given a list of sequences. As such, the collating required for our inputs can be done during tokenization ***before*** our batch transforms run in a `before_batch_tfms` transform (where we get a list of examples)! This allows users of BLURR to have everything done dynamically at batch-time without prior preprocessing with at least four potential benefits:\n",
    "\n",
    "1. No need to pretokenize your text\n",
    "2. Less code\n",
    "3. Faster mini-batch creation\n",
    "4. Less RAM utilization and time spent tokenizing beforehand (this really helps with very large datasets)\n",
    "5. Batch level flexibility"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ItemTokenizeTransform` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class ItemTokenizeTransform(ItemTransform):\n",
    "    split_idx = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # A Hugging Face configuration object\n",
    "        hf_config: PretrainedConfig = None,\n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase = None,\n",
    "        # Any keyword arguments you want your Hugging Face tokenizer to use during tokenization\n",
    "        tok_kwargs: dict = {},\n",
    "        # Any keyword arguments you want applied to `ItemTokenizeTransform`\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        store_attr()\n",
    "\n",
    "        if tok_kwargs.get(\"truncation\", None) is None:\n",
    "            tok_kwargs[\"truncation\"] = True\n",
    "        if tok_kwargs.get(\"max_length\", None) is None:\n",
    "            tok_kwargs[\"max_length\"] = True\n",
    "\n",
    "    def encodes(self, txt, **kwargs):\n",
    "        inputs = self.hf_tokenizer(txt, **self.tok_kwargs)\n",
    "        return dict(inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas the `BatchTokenizeTransform` allows you to tokenize text at batch-time, `ItemTokenizeTransform` allows you to tokenize text at item-time (e.g., when you get an item out of the dataset). This may be very useful if applying some form of data augmentation to your inputs.\n",
    "\n",
    "In order for this transform to run when getting an item, it needs to be specified as one of your `DataBlock`s `type_tfms`.\n",
    "\n",
    "Potential benefits:\n",
    "\n",
    "1. No need to pretokenize your text\n",
    "2. The ability to apply data augmentation each time an item is pulled from your dataset\n",
    "3. Less RAM utilization and time spent tokenizing beforehand (this really helps with very large datasets)\n",
    "4. Item level flexibility"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TextBlock` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class TextBlock(TransformBlock):\n",
    "    \"\"\"The core `TransformBlock` to prepare your inputs for training in Blurr with fastai's `DataBlock` API\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (not required if passing in an \\\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_arch: str = None,\n",
    "        # A Hugging Face configuration object (not required if passing in an \\\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_config: PretrainedConfig = None,\n",
    "        # A Hugging Face tokenizer (not required if passing in an \\\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_tokenizer: PreTrainedTokenizerBase = None,\n",
    "        # A Hugging Face model (not required if passing in an \\\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_model: PreTrainedModel = None,\n",
    "        # Any transforms to apply when getting an item from a dataset (useufl for item-time tokenization)\n",
    "        type_tfms: list[ItemTokenizeTransform] = None,\n",
    "        # The \"before_batch\" transform you want to use if tokenizing your raw data on the fly (optional)\n",
    "        tokenize_tfm: Transform = None,\n",
    "        # The batch_tfm you want to decode your inputs into a type that can be used in the fastai show methods, \\\n",
    "        # (defaults to BatchDecodeTransform)\n",
    "        batch_decode_tfm: BatchDecodeTransform = None,\n",
    "        # An instance of `TextCollatorWithPadding` to use when not performing batch-time tokenization, \\\n",
    "        # (defaults to `TextCollatorWithPadding` when using pretokenized or item-time tokenization)\n",
    "        data_collator: TextCollatorWithPadding = None,\n",
    "        # To control whether the \"include_labels\" are included in your inputs. If they are, the loss will be calculated in \\\n",
    "        # the model's forward function and you can simply use `PreCalculatedLoss` as your `Learner`'s loss function to use it\n",
    "        include_labels: bool = True,\n",
    "        # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. \\\n",
    "        # Set this to `True` if your inputs are pre-tokenized (not numericalized)\n",
    "        is_split_into_words: bool = False,\n",
    "        # The return type your decoded inputs should be cast too (used by methods such as `show_batch`)\n",
    "        input_return_type: type = TextInput,\n",
    "        # The type of `DataLoader` you want created (defaults to `SortedDL`)\n",
    "        dl_type: DataLoader = None,\n",
    "        # Any keyword arguments you want applied to your `batch_decode_tfm` (will be set as a fastai `batch_tfms`)\n",
    "        batch_decode_kwargs: dict = {},\n",
    "        # Any keyword arguments you want your Hugging Face tokenizer to use during tokenization\n",
    "        tok_kwargs: dict = {},\n",
    "        # Any keyword arguments you want to have applied with generating text\n",
    "        text_gen_kwargs: dict = {},\n",
    "        # Any keyword arguments you want applied to `TextBlock`\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if (not all([hf_arch, hf_config, hf_tokenizer, hf_model])) and tokenize_tfm is None:\n",
    "            raise ValueError(\"You must supply an hf_arch, hf_config, hf_tokenizer, hf_model -or- a tokenize_tfm\")\n",
    "\n",
    "        # if we are using a transform to tokenize our inputs, grab the HF objects from it\n",
    "        if tokenize_tfm is not None:\n",
    "            hf_arch = getattr(tokenize_tfm, \"hf_arch\", hf_arch)\n",
    "            hf_config = getattr(tokenize_tfm, \"hf_config\", hf_config)\n",
    "            hf_tokenizer = getattr(tokenize_tfm, \"hf_tokenizer\", hf_tokenizer)\n",
    "            hf_model = getattr(tokenize_tfm, \"hf_model\", hf_model)\n",
    "            is_split_into_words = getattr(tokenize_tfm, \"is_split_into_words\", is_split_into_words)\n",
    "            include_labels = getattr(tokenize_tfm, \"include_labels\", include_labels)\n",
    "\n",
    "        # configure our batch decode transform (used by show_batch/results methods)\n",
    "        if batch_decode_tfm is None:\n",
    "            batch_decode_tfm = BatchDecodeTransform(\n",
    "                hf_arch=hf_arch,\n",
    "                hf_config=hf_config,\n",
    "                hf_tokenizer=hf_tokenizer,\n",
    "                hf_model=hf_model,\n",
    "                input_return_type=input_return_type,\n",
    "                **batch_decode_kwargs.copy(),\n",
    "            )\n",
    "\n",
    "        # default to SortedDL using our custom sort function if no `dl_type` is specified\n",
    "        if dl_type is None:\n",
    "            dl_sort_func = partial(\n",
    "                sorted_dl_func, hf_tokenizer=hf_tokenizer, is_split_into_words=is_split_into_words, tok_kwargs=tok_kwargs.copy()\n",
    "            )\n",
    "            dl_type = partial(SortedDL, sort_func=dl_sort_func)\n",
    "\n",
    "        # build our custom `TransformBlock`\n",
    "        if tokenize_tfm is None:\n",
    "            if data_collator is None:\n",
    "                data_collator = TextCollatorWithPadding(hf_tokenizer)\n",
    "            dl_kwargs = {\"create_batch\": data_collator}\n",
    "        else:\n",
    "            dl_kwargs = {\"before_batch\": tokenize_tfm}\n",
    "\n",
    "        return super().__init__(dl_type=dl_type, dls_kwargs=dl_kwargs, type_tfms=type_tfms, batch_tfms=batch_decode_tfm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid-Level API: Examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretokenized"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t2\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", label_names, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Step 2: `DataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define DataBlock splitter\n",
    "def _split_func(example):\n",
    "    return example[\"is_valid\"] == True\n",
    "\n",
    "\n",
    "# define how we want to build our targets\n",
    "# note: we don't need to define how to build our inputs because we're using an HF `Dataset` in this example\n",
    "def get_y(example):\n",
    "    return example[\"label\"]\n",
    "\n",
    "\n",
    "# define the DataBlock\n",
    "txt_block = TextBlock(\n",
    "    hf_arch=hf_arch, hf_config=hf_config, hf_tokenizer=hf_tokenizer, hf_model=hf_model, batch_decode_kwargs={\"label_names\": label_names}\n",
    ")\n",
    "\n",
    "blocks = (txt_block, CategoryBlock)\n",
    "dblock = DataBlock(blocks=blocks, get_y=get_y, splitter=FuncSplitter(_split_func))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe71e0c6be54f30a21b8e961d25bffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "tokenize_func = partial(multiclass_tokenize_func, hf_tokenizer=hf_tokenizer)\n",
    "proc_imdb_ds = imdb_ds.map(tokenize_func, batched=True)\n",
    "\n",
    "# build your `DataLoaders`\n",
    "dls = dblock.dataloaders(proc_imdb_ds, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of items in each batch: 2\n",
      "# of inputs in each batch: 4\n",
      "# of targets in each batch: 4\n",
      "Shape of input_ids (bsz, seq): torch.Size([4, 2988])\n"
     ]
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"# of inputs in each batch:\", len(b[0][\"input_ids\"]))\n",
    "print(\"# of targets in each batch:\", len(b[1]))\n",
    "print(\"Shape of input_ids (bsz, seq):\", b[0][\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the actual types represented by our batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Having read some of the other comments here I was expecting something truly awful but was pleasantly surprised. REALITY CHECK: The original series wasn't that good. I think some people remember it with more affection than it deserved but apart from the car chases and Daisy Duke's legs the scripts were weak and poorly acted. The Duke boys were too intelligent and posh for backwood hicks, the shrunken Boss Hog was too cretinous to be evil and Rosco was just hyper throughout every screen moment. It</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This was the best movie I've ever seen about Bulimia. It hit the exact spot of what Bulimia is really about and how it's not always about being skinny and dieting. It showed how people with Bulimia tend to think about things such as their outlook on life, friends and themselves. The best line and the part that really shows what the problem with Bulimia is, is when Beth says,\"It's not about you!\" That line really showed a lot about the character and others with the same problem. It showed that pe</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t7\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", civil_label_names, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Step 2: `DataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define DataBlock splitter\n",
    "def _split_func(example):\n",
    "    return example[\"is_valid\"] == True\n",
    "\n",
    "\n",
    "# define how we want to build our targets\n",
    "# note: we don't need to define how to build our inputs because we're using an HF `Dataset` in this example\n",
    "def get_y(example):\n",
    "    return example[\"label\"]\n",
    "\n",
    "\n",
    "# define the DataBlock\n",
    "blocks = (\n",
    "    TextBlock(hf_arch=hf_arch, hf_config=hf_config, hf_tokenizer=hf_tokenizer, hf_model=hf_model),\n",
    "    MultiCategoryBlock(encoded=True, vocab=civil_label_names),\n",
    ")\n",
    "dblock = DataBlock(blocks=blocks, get_y=get_y, splitter=FuncSplitter(_split_func))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/wgilliam/.cache/huggingface/datasets/civil_comments/default/0.9.0/e7a3aacd2ab7d135fa958e7209d10b1fa03807d44c486e3c34897aa08ea8ffab/cache-d90b12b8527cb575.arrow\n"
     ]
    }
   ],
   "source": [
    "# tokenize the dataset\n",
    "tokenize_func = partial(multilabel_tokenize_func, hf_tokenizer=hf_tokenizer, label_attrs=civil_label_names)\n",
    "proc_civil_ds = civil_ds.map(tokenize_func, batched=True)\n",
    "\n",
    "dls = dblock.dataloaders(proc_civil_ds, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of items in each batch: 2\n",
      "# of inputs in each batch: 4\n",
      "# of targets in each batch: 4\n",
      "Shape of input_ids (bsz, seq): torch.Size([4, 255])\n"
     ]
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"# of inputs in each batch:\", len(b[0][\"input_ids\"]))\n",
    "print(\"# of targets in each batch:\", len(b[1]))\n",
    "print(\"Shape of input_ids (bsz, seq):\", b[0][\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the actual types represented by our batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Like out of a Seinfeld episode......</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What a mess.</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch-Time Tokenization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t2\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", label_names, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Step 2: `DataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_tfm = BatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model)\n",
    "\n",
    "# note: we pass the label_names here because the labels in the dataset are already encoded as 0 or 1\n",
    "blocks = (\n",
    "    TextBlock(tokenize_tfm=tokenize_tfm, batch_decode_kwargs={\"label_names\": label_names}),\n",
    "    CategoryBlock,\n",
    ")\n",
    "dblock = DataBlock(\n",
    "    blocks=blocks,\n",
    "    get_x=ColReader(\"text\"),\n",
    "    get_y=ColReader(\"label\"),\n",
    "    splitter=ColSplitter(),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(imdb_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of items in each batch: 2\n",
      "# of inputs in each batch: 4\n",
      "# of targets in each batch: 4\n",
      "Shape of input_ids (bsz, seq): torch.Size([4, 2988])\n"
     ]
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"# of inputs in each batch:\", len(b[0][\"input_ids\"]))\n",
    "print(\"# of targets in each batch:\", len(b[1]))\n",
    "print(\"Shape of input_ids (bsz, seq):\", b[0][\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the actual types represented by our batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Match 1: Tag Team Table Match Bubba Ray and Spike Dudley vs Eddie Guerrero and Chris Benoit Bubba Ray and Spike Dudley started things off with a Tag Team Table Match against Eddie Guerrero and Chris Benoit. According to the rules of the match, both opponents have to go through tables in order to get the win. Benoit and Guerrero heated up early on by taking turns hammering first Spike and then Bubba Ray. A German suplex by Benoit to Bubba took the wind out of the Dudley brother. Spike tried to he</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Where to begin? How about with the erroneous synopsis: &lt;br /&gt;&lt;br /&gt;\"X-Men Origins: Wolverine tells the story of Wolverine's epically violent and romantic past, his complex relationship with Victor Creed, and the ominous Weapon X program.\"&lt;br /&gt;&lt;br /&gt;His epically violent past turns out to exceptionally non-violent.&lt;br /&gt;&lt;br /&gt;His relationship with Creed is so glossed over it's difficult to understand how they have any connection at all. We are thrown from one point in the opening scene that shows</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t7\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", civil_label_names, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Step 2: `DataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_tfm = BatchTokenizeTransform(hf_arch, hf_config, hf_tokenizer, hf_model)\n",
    "\n",
    "blocks = (TextBlock(tokenize_tfm=tokenize_tfm), MultiCategoryBlock(encoded=True, vocab=civil_label_names))\n",
    "dblock = DataBlock(blocks=blocks, get_x=ColReader(\"text\"), get_y=ColReader(civil_label_names), splitter=ColSplitter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(civil_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of items in each batch: 2\n",
      "# of inputs in each batch: 4\n",
      "# of targets in each batch: 4\n",
      "Shape of input_ids (bsz, seq): torch.Size([4, 255])\n"
     ]
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"# of inputs in each batch:\", len(b[0][\"input_ids\"]))\n",
    "print(\"# of targets in each batch:\", len(b[1]))\n",
    "print(\"Shape of input_ids (bsz, seq):\", b[0][\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the actual types represented by our batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Much ado about nothing. The Trumpenproletariat are apoplectic because the Democrats tried to find negative information about Trump? So what? Nothing illegal or unethical here. The truly interesting thing about this dossier is that it was originally contracted by one of Trump's primary opponents, a \"Never Trumper\", (probably a Jeb Bush super-pac) before the Dems got involved: \"When Fusion approached Elias, it had already been doing research work on Trump for a client during the GOP primary. The i</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I can't find on my tablet your full reply to my comment on ENDA, and employment discrimination against LGBT people in the US. I received an e-mail with only a partial reply. You mentioned Ashers bakery, in Belfast Northern Ireland, and claimed that a petition in support of its decision to refuse to fulfil an order for a cake made by a gay man showed around 70% of signatories in favour. Shall I stop here? Or have you already spotted your gaffe? Yes, that's right, T. My comment concerned EMPLOYMEN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item-Time Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t2\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", label_names, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Step 2: `DataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm = ItemTokenizeTransform(hf_config=hf_config, hf_tokenizer=hf_tokenizer)\n",
    "tfm.split_idx = 0\n",
    "\n",
    "tfm2 = ItemTokenizeTransform(hf_config=hf_config, hf_tokenizer=hf_tokenizer)\n",
    "tfm2.split_idx = 1\n",
    "\n",
    "blocks = (\n",
    "    TextBlock(\n",
    "        hf_arch=hf_arch,\n",
    "        hf_config=hf_config,\n",
    "        hf_tokenizer=hf_tokenizer,\n",
    "        hf_model=hf_model,\n",
    "        type_tfms=[tfm, tfm2],\n",
    "        batch_decode_kwargs={\"label_names\": label_names},\n",
    "    ),\n",
    "    CategoryBlock,\n",
    ")\n",
    "\n",
    "dblock = DataBlock(\n",
    "    blocks=blocks,\n",
    "    get_x=ColReader(\"text\"),\n",
    "    get_y=ColReader(\"label\"),\n",
    "    splitter=ColSplitter(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(imdb_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of items in each batch: 2\n",
      "# of inputs in each batch: 4\n",
      "# of targets in each batch: 4\n",
      "Shape of input_ids (bsz, seq): torch.Size([4, 2988])\n"
     ]
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"# of inputs in each batch:\", len(b[0][\"input_ids\"]))\n",
    "print(\"# of targets in each batch:\", len(b[1]))\n",
    "print(\"Shape of input_ids (bsz, seq):\", b[0][\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the actual types represented by our batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Match 1: Tag Team Table Match Bubba Ray and Spike Dudley vs Eddie Guerrero and Chris Benoit Bubba Ray and Spike Dudley started things off with a Tag Team Table Match against Eddie Guerrero and Chris Benoit. According to the rules of the match, both opponents have to go through tables in order to get the win. Benoit and Guerrero heated up early on by taking turns hammering first Spike and then Bubba Ray. A German suplex by Benoit to Bubba took the wind out of the Dudley brother. Spike tried to he</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When the remake of When A Stranger Calls was out, obviously I was interested in watching the original. Then when I read about the original (which I recall had my sisters totally freaked out back in the day) I saw that the real money is on Black Christmas, which apparently beat everyone to the \"the caller is in the house\" punch. So I Netflix that, and it sits at the top of my list for months due to its \"very long wait.\" All this time I am getting more and more eager to see it! Then one day, out o</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: HF objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== config ===\n",
      "# of labels:\t7\n",
      "\n",
      "=== tokenizer ===\n",
      "Vocab size:\t\t128000\n",
      "Max # of tokens:\t1000000000000000019884624838656\n",
      "Attributes expected by model in forward pass:\t['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# | output: false\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_task_hf_objects(\"microsoft/deberta-v3-small\", civil_label_names, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Step 2: `DataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm = ItemTokenizeTransform(hf_config=hf_config, hf_tokenizer=hf_tokenizer)\n",
    "tfm.split_idx = 0\n",
    "\n",
    "tfm2 = ItemTokenizeTransform(hf_config=hf_config, hf_tokenizer=hf_tokenizer)\n",
    "tfm2.split_idx = 1\n",
    "\n",
    "blocks = (\n",
    "    TextBlock(hf_arch=hf_arch, hf_config=hf_config, hf_tokenizer=hf_tokenizer, hf_model=hf_model, type_tfms=[tfm, tfm2]),\n",
    "    MultiCategoryBlock(encoded=True, vocab=civil_label_names),\n",
    ")\n",
    "\n",
    "dblock = DataBlock(\n",
    "    blocks=blocks,\n",
    "    get_x=ColReader(\"text\"),\n",
    "    get_y=ColReader(civil_label_names),\n",
    "    splitter=ColSplitter(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(civil_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of items in each batch: 2\n",
      "# of inputs in each batch: 4\n",
      "# of targets in each batch: 4\n",
      "Shape of input_ids (bsz, seq): torch.Size([4, 255])\n"
     ]
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "print(\"# of items in each batch:\", len(b))\n",
    "print(\"# of inputs in each batch:\", len(b[0][\"input_ids\"]))\n",
    "print(\"# of targets in each batch:\", len(b[1]))\n",
    "print(\"Shape of input_ids (bsz, seq):\", b[0][\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the actual types represented by our batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Much ado about nothing. The Trumpenproletariat are apoplectic because the Democrats tried to find negative information about Trump? So what? Nothing illegal or unethical here. The truly interesting thing about this dossier is that it was originally contracted by one of Trump's primary opponents, a \"Never Trumper\", (probably a Jeb Bush super-pac) before the Dems got involved: \"When Fusion approached Elias, it had already been doing research work on Trump for a client during the GOP primary. The i</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I agree with you that prescription opioids – which are effective for short term pain relief – have little support for long term use. The problem is that most physicians are confusing inflammatory injury pain (nociception) with exaggerated sensitivity in the nervous system (peripheral neuropathy). The Institute for the Study and Treatment of Pain – Definition of Pain http://www.istop.org/education.html There is already an opioid alternative by using physical therapies: intramuscular stimulation b</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2, trunc_at=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |echo:false\n",
    "try:\n",
    "    del dls, hf_model\n",
    "except:\n",
    "    pass\n",
    "finally:\n",
    "    clean_memory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "The tests below to ensure the core DataBlock code above works for **all** pretrained sequence classification models available in Hugging Face.  These tests are excluded from the CI workflow because of how long they would take to run and the amount of data that would be required to download.\n",
    "\n",
    "**Note**: Feel free to modify the code below to test whatever pretrained classification models you are working with ... and if any of your pretrained sequence classification models fail, please submit a github issue *(or a PR if you'd like to fix it yourself)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data\n",
    "\n",
    "> This module contains the bits required to use the fastai DataBlock API and/or mid-level data processing pipelines to organize your data in a way modelable by huggingface transformer implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import ast\n",
    "from functools import reduce\n",
    "\n",
    "from blurr.utils import *\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "from fastai2.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import pdb\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "#cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f'Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base tokenization, batch transform, and DataBlock methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_BaseInput(list): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `HF_BaseInput` object is used to encapsulate all the inputs required by whatever huggingface model we are using. We use it as a container for the `input_ids`, `token_type_ids`, and `attention_mask` tensors required by most models, and also as a mean to customize @typedispatched functions like `DataLoaders.show_batch` and `Learner.show_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_Tokenizer():\n",
    "    \"\"\"huggingface friendly tokenization function.\"\"\"\n",
    "    def __init__(self, hf_arch, hf_tokenizer, mode='str', list_split_func=str.split, **kwargs):\n",
    "        store_attr(self, 'hf_arch, hf_tokenizer, mode, list_split_func')\n",
    "\n",
    "    def __call__(self, items): \n",
    "        for txt in items: yield self._tokenize(txt)\n",
    "\n",
    "    def _tokenize(self, txt):   \n",
    "        if (self.mode == 'str'): \n",
    "            return self.hf_tokenizer.tokenize(txt)\n",
    "        \n",
    "        if (self.mode == 'list'):\n",
    "            try: tokens = ast.literal_eval(txt)\n",
    "            except: \n",
    "                tokens = self.list_split_func(txt)\n",
    "            finally:\n",
    "                return [sub_toks for entity in tokens for sub_toks in self.hf_tokenizer.tokenize(entity)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`HF_Tokenizer` complies with the requirements of a basic tokenization function in fastai.  See [here](http://dev.fast.ai/text.core#Tokenizing).\n",
    "\n",
    "We've updated the `_tokenize` method to operate on a string or a list (the later being very handy for tasks like token classification whereby the examples consist of a list of tokens and a list of labels for each)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def build_hf_input(task, tokenizer, a_tok_ids, b_tok_ids=None, targets=None,\n",
    "                   max_length=512, pad_to_max_length=True, truncation_strategy='longest_first'):\n",
    "\n",
    "    res = tokenizer.prepare_for_model(a_tok_ids, b_tok_ids, \n",
    "                                       max_length=max_length, pad_to_max_length=pad_to_max_length,\n",
    "                                       truncation_strategy=truncation_strategy, return_tensors='pt')\n",
    "    \n",
    "    input_ids = res['input_ids'][0]\n",
    "    token_type_ids = res['token_type_ids'][0] if ('token_type_ids' in res) else torch.tensor([-9999]) \n",
    "    attention_mask = res['attention_mask'][0] if ('attention_mask' in res) else torch.tensor([-9999]) \n",
    "    \n",
    "    return HF_BaseInput([input_ids, token_type_ids, attention_mask]), targets\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`build_hf_input` uses fastai's @typedispatched decorator to provide for complete flexibility in terms of how your numericalized tokens are assembled, and also what you return via `HF_BaseInput` and as your targets.  You can override this implementation as needed by assigning a type to the `task` argument (and optionally the `tokenizer` argument as well).\n",
    "\n",
    "What you return here is what will be fed into your huggingface model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_BatchTransform(Transform):\n",
    "    \"\"\"Handles everything you need to assemble a mini-batch of inputs and targets\"\"\"\n",
    "    def __init__(self, hf_arch, hf_tokenizer, max_seq_len=512, truncation_strategy='longest_first', task=None):\n",
    "        \n",
    "        self.hf_arch = hf_arch\n",
    "        self.hf_tokenizer = hf_tokenizer\n",
    "        store_attr(self, 'max_seq_len, truncation_strategy, task')\n",
    "        \n",
    "    def encodes(self, samples):\n",
    "        \n",
    "        encoded_samples = []\n",
    "        for idx, sample in enumerate(samples):\n",
    "            \n",
    "            if (isinstance(sample[0], tuple)):\n",
    "                a_tok_ids = sample[0][0].tolist()\n",
    "                b_tok_ids = sample[0][1].tolist()\n",
    "            else:\n",
    "                a_tok_ids = sample[0].tolist()\n",
    "                b_tok_ids = None\n",
    "\n",
    "            hf_base_input, targets = build_hf_input(self.task, self.hf_tokenizer, \n",
    "                                                    a_tok_ids, b_tok_ids, sample[1:],\n",
    "                                                    self.max_seq_len, True, self.truncation_strategy)\n",
    "            \n",
    "            encoded_samples.append((hf_base_input, *targets))\n",
    "            \n",
    "        return encoded_samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_TextBlock(TransformBlock):\n",
    "    \n",
    "    @delegates(Numericalize.__init__)\n",
    "    def __init__(self, tok_tfms, hf_arch, hf_tokenizer, hf_batch_tfm=None, vocab=None, task=None,\n",
    "                 max_seq_len=512, **kwargs):\n",
    "\n",
    "        if hf_batch_tfm is None:\n",
    "            hf_batch_tfm = HF_BatchTransform(hf_arch, hf_tokenizer, max_seq_len=max_seq_len,\n",
    "                                             truncation_strategy='longest_first', task=task)\n",
    "            \n",
    "        return super().__init__(type_tfms=[*tok_tfms, Numericalize(vocab, **kwargs)],\n",
    "                                dl_type=SortedDL, \n",
    "                                dls_kwargs={ 'before_batch': hf_batch_tfm })\n",
    "\n",
    "    @classmethod\n",
    "    @delegates(Tokenizer.from_df, keep=True)\n",
    "    def from_df(cls, text_cols_lists, hf_arch, hf_tokenizer, hf_batch_tfm=None, vocab=None, task=None, \n",
    "                tok_func_mode='str', res_col_names=None, max_seq_len=512, **kwargs):\n",
    "        \"\"\"Creates a HF_TextBlock via a pandas DataFrame\"\"\"\n",
    "        \n",
    "        # grab hf tokenizer class to do the actual tokenization (via tok_func) and its vocab\n",
    "        tokenizer_cls = partial(HF_Tokenizer, hf_arch=hf_arch, hf_tokenizer=hf_tokenizer, mode=tok_func_mode)\n",
    "        if (vocab is None): vocab = list(hf_tokenizer.get_vocab())\n",
    "\n",
    "        # build the column name(s) returned after tokenization\n",
    "        if (res_col_names is None): res_col_names = [ f'text{i}' for i in range(len(text_cols_lists)) ] \n",
    "    \n",
    "        tok_tfms = [ Tokenizer.from_df(text_cols, \n",
    "                                       res_col_name=res_col_name, \n",
    "                                       tok_func=tokenizer_cls,\n",
    "                                       rules=[], **kwargs) \n",
    "                    for text_cols, res_col_name in zip(text_cols_lists, res_col_names) ]\n",
    "  \n",
    "        return cls(tok_tfms, hf_arch=hf_arch, hf_tokenizer=hf_tokenizer, hf_batch_tfm=hf_batch_tfm, \n",
    "                   vocab=vocab, task=task, max_seq_len=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"HF_TextBlock.from_df\" class=\"doc_header\"><code>HF_TextBlock.from_df</code><a href=\"__main__.py#L16\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>HF_TextBlock.from_df</code>(**`text_cols_lists`**, **`hf_arch`**, **`hf_tokenizer`**, **`hf_batch_tfm`**=*`None`*, **`vocab`**=*`None`*, **`task`**=*`None`*, **`tok_func_mode`**=*`'str'`*, **`res_col_names`**=*`None`*, **`max_seq_len`**=*`512`*, **`tok_func`**=*`'SpacyTokenizer'`*, **`rules`**=*`None`*, **`sep`**=*`' '`*, **`n_workers`**=*`16`*, **`mark_fields`**=*`None`*, **`res_col_name`**=*`'text'`*, **\\*\\*`kwargs`**)\n",
       "\n",
       "Creates a HF_TextBlock via a pandas DataFrame"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(HF_TextBlock.from_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, we've only implemented building this block from a pandas DataFrame.  It handles single and multiple text inputs so that it can be used out-of-the-box against any model in the huggingface arsenal (e.g. sequence classification, question-answer, summarization, token classification, etc...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence classification (e.g., models that require a single text input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "\n",
    "model_path = Path('models')\n",
    "imdb_df = pd.read_csv(path/'texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.&lt;br /&gt;&lt;br /&gt;But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.&lt;br /&gt;&lt;br /&gt;Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.&lt;br /&gt;&lt;br /&gt;Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie \"Duty, Honor, Country\" are not just mere words blathered from the lips of a high-brassed offic...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  \\\n",
       "0  negative   \n",
       "1  positive   \n",
       "2  negative   \n",
       "3  positive   \n",
       "4  negative   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0                                                                                                                                                                                                    Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!   \n",
       "1  This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.<br /><br />But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...   \n",
       "2  Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.<br /><br />Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...   \n",
       "3  Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.<br /><br />Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie \"Duty, Honor, Country\" are not just mere words blathered from the lips of a high-brassed offic...   \n",
       "4  This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...   \n",
       "\n",
       "   is_valid  \n",
       "0     False  \n",
       "1     False  \n",
       "2     False  \n",
       "3     False  \n",
       "4     False  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a bunch of ways we can get at the four huggingface elements we need (e.g., architecture name, tokenizer, config, and model).  We can just create them directly, or we can use one of the helper methods available via `BLURR_MODEL_HELPER`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = HF_TASKS_AUTO.ForSequenceClassification\n",
    "\n",
    "pretrained_model_name = \"roberta-base\" # \"distilbert-base-uncased\" \"bert-base-uncased\"\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "\n",
    "hf_arch, hf_tokenizer, hf_config, hf_model = BLURR_MODEL_HELPER.get_auto_hf_objects(pretrained_model_name, \n",
    "                                                                                    task=task, \n",
    "                                                                                    config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have those elements, you can create your `DataBlock` as simple as the below. Note that you can use multiple columns in your DataFrame to make up the *single* text element required by `HF_TextBlock` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single input\n",
    "blocks = (\n",
    "    HF_TextBlock.from_df(text_cols_lists=[['text']], hf_arch=hf_arch, hf_tokenizer=hf_tokenizer),\n",
    "    CategoryBlock\n",
    ")\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, \n",
    "                   get_x=lambda x: x.text0,\n",
    "                   get_y=ColReader('label'), \n",
    "                   splitter=ColSplitter(col='is_valid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dblock.summary(imdb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls = dblock.dataloaders(imdb_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 4)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch(); len(b), len(b[0]), len(b[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 512]),\n",
       " torch.Size([4, 1]),\n",
       " torch.Size([4, 512]),\n",
       " torch.Size([4]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0][0].shape, b[0][1].shape, b[0][2].shape, b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_batch(x:HF_BaseInput, y, samples, hf_tokenizer, skip_special_tokens=True, ctxs=None, max_n=6, **kwargs):        \n",
    "    if ctxs is None: ctxs = get_empty_df(min(len(samples), max_n))\n",
    "    \n",
    "    samples = L((TitledStr(hf_tokenizer.decode(inp, skip_special_tokens=skip_special_tokens).replace(hf_tokenizer.pad_token, '')),*s[1:]) \n",
    "                for inp, s in zip(x[0], samples))\n",
    "    \n",
    "    ctxs = show_batch[object](x, y, samples, max_n=max_n, ctxs=ctxs, **kwargs)\n",
    "\n",
    "    display_df(pd.DataFrame(ctxs))\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Raising Victor Vargas: A Review&lt;br /&gt;&lt;br /&gt;You know, Raising Victor Vargas is like sticking your hands into a big, steaming bowl of oatmeal. It's warm and gooey, but you're not sure if it feels right. Try as I might, no matter how warm and gooey Raising Victor Vargas became I was always aware that something didn't quite feel right. Victor Vargas suffers from a certain overconfidence on the director's part. Apparently, the director thought that the ethnic backdrop of a Latino family on the lower east side, and an idyllic storyline would make the film critic proof. He was right, but it didn't fool me. Raising Victor Vargas is the story about a seventeen-year old boy called, you guessed it, Victor Vargas (Victor Rasuk) who lives his teenage years chasing more skirt than the Rolling Stones could do in all the years they've toured. The movie starts off in `Ugly Fat' Donna's bedroom where Victor is sure to seduce her, but a cry from outside disrupts his plans when his best-friend Harold (Kevin Rivera) comes-a-looking for him. Caught in the attempt by Harold and his sister, Victor Vargas runs off for damage control. Yet even with the embarrassing implication that he's been boffing the homeliest girl in the neighborhood, nothing dissuades young Victor from going off on the hunt for more fresh meat. On a hot, New York City day they make way to the local public swimming pool where Victor's eyes catch a glimpse of the lovely young nymph Judy (Judy Marte), who's not just pretty, but a strong and independent too. The relationship that develops between Victor and Judy becomes the focus of the film. The story also focuses on Victor's family that is comprised of his grandmother or abuelita (Altagracia Guzman), his brother Nino (also played by real life brother to Victor, Silvestre Rasuk) and his sister Vicky (Krystal Rodriguez). The action follows Victor between scenes with Judy and scenes with his family. Victor tries to cope with being an oversexed pimp-daddy, his feelings for Judy and his grandmother's conservative Catholic upbringing.&lt;br /&gt;&lt;br /&gt;The problems that arise from Raising Victor Vargas are a few, but glaring errors. Throughout the film you get to know certain characters like Vicky, Nino, Grandma,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This film sat on my Tivo for weeks before I watched it. I dreaded a self-indulgent yuppie flick about relationships gone bad. I was wrong; this was an engrossing excursion into the screwed-up libidos of New Yorkers.&lt;br /&gt;&lt;br /&gt;The format is the same as Max Ophuls' \"La Ronde,\" based on a play by Arthur Schnitzler, who is given an \"inspired by\" credit. It starts from one person, a prostitute, standing on a street corner in Brooklyn. She is picked up by a home contractor, who has sex with her on the hood of a car, but can't come. He refuses to pay her. When he's off peeing, she answers his cell phone and takes a message. She runs away with his keys.&lt;br /&gt;&lt;br /&gt;Then the story switches to the contractor, who pays a professional call on a rich, bored New York woman, who plays with him until he is aroused, then she pulls away. She tells him how desperate and unhappy she is; he tells her how beautiful she is, and lucky. As he is leaving, she asks if he would have sex with her. She sits on top of him, bounces up and down. This time he comes, the he leaves.&lt;br /&gt;&lt;br /&gt;The woman and her husband throw a dinner party for their trendy friends. Hubby (Robert) is talking business, wife (Ellen) is bored, and switches the subject to sex, and how often men and women think about it. Husband switches conversation to desert. Later, after the guests leave, Ellen tries to entice Robert into sex. Robert wants none of it, and puts on a jazz record. Ellen turns on the radio; Robert turns up the music; Ellen turns on the TV; Robert turns on another TV. Cacophony ensues. Ellen goes up on the roof, Robert joins her. Ellen confesses that she needs to experience more men, men other than Robert. Robert says that he too needs to experience men.&lt;br /&gt;&lt;br /&gt;We next follow Robert as he visits an artist, Martin, played by Steve Buscemi. I wish Buscemi could have more roles like this, where he is a sexy, smart, totally desirable guy. Robert praises Martin's work, much more than it deserves, promises to get it into a show. Martin is excited, until it turns out that Robert is speaking out of his groin, it</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(hf_tokenizer=hf_tokenizer, max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answering (e.g., models that require two text inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've provided a simple subset of a pre-processed SQUADv2 dataset below just for demonstration purposes. There is a lot that can be done to make this much better and more fully functional.  The idea here is just to show you how things can work for tasks beyond sequence classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path('./')\n",
    "squad_df = pd.read_csv(path/'squad_sample.csv'); len(squad_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question_id</th>\n",
       "      <th>question_text</th>\n",
       "      <th>is_impossible</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New_York_City</td>\n",
       "      <td>The New York City Fire Department (FDNY), provides fire protection, technical rescue, primary response to biological, chemical, and radioactive hazards, and emergency medical services for the five boroughs of New York City. The New York City Fire Department is the largest municipal fire department in the United States and the second largest in the world after the Tokyo Fire Department. The FDNY employs approximately 11,080 uniformed firefighters and over 3,300 uniformed EMTs and paramedics. The FDNY's motto is New York's Bravest.</td>\n",
       "      <td>56d1076317492d1400aab78c</td>\n",
       "      <td>What does FDNY stand for?</td>\n",
       "      <td>False</td>\n",
       "      <td>New York City Fire Department</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cyprus</td>\n",
       "      <td>Following the death in 1473 of James II, the last Lusignan king, the Republic of Venice assumed control of the island, while the late king's Venetian widow, Queen Catherine Cornaro, reigned as figurehead. Venice formally annexed the Kingdom of Cyprus in 1489, following the abdication of Catherine. The Venetians fortified Nicosia by building the Venetian Walls, and used it as an important commercial hub. Throughout Venetian rule, the Ottoman Empire frequently raided Cyprus. In 1539 the Ottomans destroyed Limassol and so fearing the worst, the Venetians also fortified Famagusta and Kyrenia.</td>\n",
       "      <td>572e7f8003f98919007566df</td>\n",
       "      <td>In what year did the Ottomans destroy Limassol?</td>\n",
       "      <td>False</td>\n",
       "      <td>1539</td>\n",
       "      <td>481</td>\n",
       "      <td>485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           title  \\\n",
       "0  New_York_City   \n",
       "1         Cyprus   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               context  \\\n",
       "0                                                              The New York City Fire Department (FDNY), provides fire protection, technical rescue, primary response to biological, chemical, and radioactive hazards, and emergency medical services for the five boroughs of New York City. The New York City Fire Department is the largest municipal fire department in the United States and the second largest in the world after the Tokyo Fire Department. The FDNY employs approximately 11,080 uniformed firefighters and over 3,300 uniformed EMTs and paramedics. The FDNY's motto is New York's Bravest.   \n",
       "1  Following the death in 1473 of James II, the last Lusignan king, the Republic of Venice assumed control of the island, while the late king's Venetian widow, Queen Catherine Cornaro, reigned as figurehead. Venice formally annexed the Kingdom of Cyprus in 1489, following the abdication of Catherine. The Venetians fortified Nicosia by building the Venetian Walls, and used it as an important commercial hub. Throughout Venetian rule, the Ottoman Empire frequently raided Cyprus. In 1539 the Ottomans destroyed Limassol and so fearing the worst, the Venetians also fortified Famagusta and Kyrenia.   \n",
       "\n",
       "                question_id                                    question_text  \\\n",
       "0  56d1076317492d1400aab78c                        What does FDNY stand for?   \n",
       "1  572e7f8003f98919007566df  In what year did the Ottomans destroy Limassol?   \n",
       "\n",
       "   is_impossible                    answer_text  answer_start  answer_end  \n",
       "0          False  New York City Fire Department             4          33  \n",
       "1          False                           1539           481         485  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len= 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_df = squad_df[(squad_df.answer_end < max_seq_len) & (squad_df.is_impossible == False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = HF_TASKS_AUTO.ForQuestionAnswering\n",
    "\n",
    "pretrained_model_name = \"roberta-base\"\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "\n",
    "hf_arch, hf_tokenizer, hf_config, hf_model = BLURR_MODEL_HELPER.get_auto_hf_objects(pretrained_model_name, \n",
    "                                                                                    task=task, \n",
    "                                                                                    config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dict(enumerate(range(max_seq_len)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we utilize the @typedispatch decorator to completely change how we'll tokenize the data for the `ForQuestionAnsweringTask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def build_hf_input(task:ForQuestionAnsweringTask, tokenizer, \n",
    "                   a_tok_ids, b_tok_ids=None, targets=None,\n",
    "                   max_length=512, pad_to_max_length=True, truncation_strategy=None):\n",
    "    \n",
    "    if (truncation_strategy is None):\n",
    "        truncation_strategy = \"only_second\" if tokenizer.padding_side == \"right\" else \"only_first\"\n",
    "\n",
    "    res = tokenizer.prepare_for_model(a_tok_ids if tokenizer.padding_side == \"right\" else b_tok_ids, \n",
    "                                      b_tok_ids if tokenizer.padding_side == \"right\" else a_tok_ids,\n",
    "                                      max_length=max_length, \n",
    "                                      pad_to_max_length=pad_to_max_length,\n",
    "                                      truncation_strategy=truncation_strategy, \n",
    "                                      return_tensors='pt')\n",
    "    \n",
    "    input_ids = res['input_ids'][0]\n",
    "    token_type_ids = res['token_type_ids'][0] if ('token_type_ids' in res) else torch.tensor([-9999]) \n",
    "    attention_mask = res['attention_mask'][0] if ('attention_mask' in res) else torch.tensor([-9999]) \n",
    "    \n",
    "    return HF_BaseInput([input_ids, token_type_ids, attention_mask]), targets\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we demonstrate some more of the extensibility bits of the framework, by passing in our own instance of `HF_BatchTransform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional): override HF_BatchTransform defaults\n",
    "hf_batch_tfm = HF_BatchTransform(hf_arch, hf_tokenizer, task=ForQuestionAnsweringTask(),\n",
    "                                 max_seq_len=128, truncation_strategy=None)\n",
    "\n",
    "blocks = (\n",
    "    HF_TextBlock.from_df(text_cols_lists=[['question_text'],['context']],\n",
    "                         hf_arch=hf_arch, \n",
    "                         hf_tokenizer=hf_tokenizer, \n",
    "                         hf_batch_tfm=hf_batch_tfm),\n",
    "    CategoryBlock(vocab=vocab),\n",
    "    CategoryBlock(vocab=vocab)\n",
    ")\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, \n",
    "                   get_x=lambda x: (x.text0, x.text1),\n",
    "                   get_y=[ColReader('answer_start'), ColReader('answer_end')],\n",
    "                   splitter=RandomSplitter(),\n",
    "                   n_inp=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dblock.summary(squad_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls = dblock.dataloaders(squad_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 4, 4)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch(); len(b), len(b[0]), len(b[1]), len(b[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 128]),\n",
       " torch.Size([4, 1]),\n",
       " torch.Size([4, 128]),\n",
       " torch.Size([4]),\n",
       " torch.Size([4]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0][0].shape, b[0][1].shape, b[0][2].shape, b[1].shape, b[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>category_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;As a result of Resolution 1970, what body was Libya suspended from?&lt;/s&gt;&lt;/s&gt;In the conflict's early months it appeared that Gaddafi's government – with its greater firepower – would be victorious. Both sides disregarded the laws of war, committing human rights abuses, including arbitrary arrests, torture, extrajudicial executions and revenge attacks. On 26 February the United Nations Security Council passed Resolution 1970, suspending Libya from the UN Human Rights Council, implementing sanctions and calling for an International Criminal Court (ICC) investigation into the killing of unarmed civilians. In March, the Security Council declared a no fly zone to protect the civilian population from aerial bombardment&lt;/s&gt;</td>\n",
       "      <td>378</td>\n",
       "      <td>401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;When did Zhuge Liang live from?&lt;/s&gt;&lt;/s&gt;Following the fall of Wu and the turmoil of the Wu Hu uprising against the Jin dynasty (265–420), most of elite Chinese families had collaborated with the non-Chinese rulers and military conquerors in the north. Some may have lost social privilege, and took refugee in areas south to Yangtze River. Some of the Chinese refugees from north China might have resided in areas near Hangzhou. For example, the clan of Zhuge Liang (181–234), a chancellor of the state of Shu Han from Central Plain in north China during the Three Kingdoms period, gathered together at&lt;/s&gt;</td>\n",
       "      <td>426</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(hf_tokenizer=hf_tokenizer, skip_special_tokens=False, max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token classification (e.g., NER tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>token</th>\n",
       "      <th>tag1</th>\n",
       "      <th>tag2</th>\n",
       "      <th>ds_type</th>\n",
       "      <th>seq_id</th>\n",
       "      <th>n_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Schartau</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>O</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>sagte</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>dem</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>\"</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Tagesspiegel</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>O</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pos         token   tag1 tag2 ds_type  seq_id  n_tokens\n",
       "0    1      Schartau  B-PER    O   train       1         3\n",
       "1    2         sagte      O    O   train       1         1\n",
       "2    3           dem      O    O   train       1         1\n",
       "3    4             \"      O    O   train       1         1\n",
       "4    5  Tagesspiegel  B-ORG    O   train       1         3"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# germ_eval_df = pd.read_csv('./data/task-token-classification/germeval2014ner/germeval2014ner_cleaned.csv')\n",
    "germ_eval_df = pd.read_csv('./germeval2014_sample.csv')\n",
    "germ_eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>token</th>\n",
       "      <th>tag1</th>\n",
       "      <th>tag2</th>\n",
       "      <th>ds_type</th>\n",
       "      <th>seq_id</th>\n",
       "      <th>n_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [pos, token, tag1, tag2, ds_type, seq_id, n_tokens]\n",
       "Index: []"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "germ_eval_df.dropna(inplace=True)\n",
    "germ_eval_df[germ_eval_df.token.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-LOC', 'B-LOCderiv', 'B-LOCpart', 'B-ORG', 'B-ORGpart', 'B-OTH', 'B-PER', 'B-PERpart', 'I-LOC', 'I-LOCderiv', 'I-ORG', 'I-ORGpart', 'I-OTH', 'I-PER', 'O']\n"
     ]
    }
   ],
   "source": [
    "labels = sorted(germ_eval_df.tag1.unique())\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bert',\n",
       " transformers.tokenization_bert.BertTokenizer,\n",
       " transformers.configuration_bert.BertConfig,\n",
       " transformers.modeling_bert.BertForTokenClassification)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = HF_TASKS_AUTO.ForTokenClassification\n",
    "\n",
    "pretrained_model_name = \"bert-base-multilingual-cased\"\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "config.num_labels = len(labels)\n",
    "\n",
    "hf_arch, hf_tokenizer, hf_config, hf_model = BLURR_MODEL_HELPER.get_auto_hf_objects(pretrained_model_name, \n",
    "                                                                                    task=task, \n",
    "                                                                                    config=config)\n",
    "hf_arch, type(hf_tokenizer), type(hf_config), type(hf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_id</th>\n",
       "      <th>pos</th>\n",
       "      <th>token</th>\n",
       "      <th>tag1</th>\n",
       "      <th>tag2</th>\n",
       "      <th>ds_type</th>\n",
       "      <th>n_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]</td>\n",
       "      <td>[Schartau, sagte, dem, \", Tagesspiegel, \", vom, Freitag, ,, Fischer, sei, \", in, einer, Weise, aufgetreten, ,, die, alles, andere, als, überzeugend, war, \", .]</td>\n",
       "      <td>[B-PER, O, O, O, B-ORG, O, O, O, O, B-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train]</td>\n",
       "      <td>[3, 1, 1, 1, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]</td>\n",
       "      <td>[Firmengründer, Wolf, Peter, Bree, arbeitete, Anfang, der, siebziger, Jahre, als, Möbelvertreter, ,, als, er, einen, fliegenden, Händler, aus, dem, Libanon, traf, .]</td>\n",
       "      <td>[O, B-PER, I-PER, I-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train]</td>\n",
       "      <td>[3, 1, 1, 2, 1, 1, 1, 3, 1, 1, 4, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]</td>\n",
       "      <td>[Ob, sie, dabei, nach, dem, Runden, Tisch, am, 23., April, in, Berlin, durch, ein, pädagogisches, Konzept, unterstützt, wird, ,, ist, allerdings, zu, bezweifeln, .]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 3, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>[Bayern, München, ist, wieder, alleiniger, Top-, Favorit, auf, den, Gewinn, der, deutschen, Fußball-Meisterschaft, .]</td>\n",
       "      <td>[B-ORG, I-ORG, O, O, O, O, O, O, O, O, O, B-LOCderiv, O, O]</td>\n",
       "      <td>[B-LOC, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[train, train, train, train, train, train, train, train, train, train, train, train, train, train]</td>\n",
       "      <td>[1, 1, 1, 1, 2, 2, 3, 1, 1, 1, 1, 1, 3, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>[Dabei, hätte, der, tapfere, Schlussmann, allen, Grund, gehabt, ,, sich, viel, früher, aufzuregen, .]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[train, train, train, train, train, train, train, train, train, train, train, train, train, train]</td>\n",
       "      <td>[1, 1, 1, 2, 2, 1, 1, 3, 1, 1, 1, 1, 3, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seq_id  \\\n",
       "0       1   \n",
       "1       2   \n",
       "2       3   \n",
       "3       4   \n",
       "4       5   \n",
       "\n",
       "                                                                                           pos  \\\n",
       "0  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]   \n",
       "1              [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]   \n",
       "2      [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]   \n",
       "3                                              [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "4                                              [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "\n",
       "                                                                                                                                                                   token  \\\n",
       "0        [Schartau, sagte, dem, \", Tagesspiegel, \", vom, Freitag, ,, Fischer, sei, \", in, einer, Weise, aufgetreten, ,, die, alles, andere, als, überzeugend, war, \", .]   \n",
       "1  [Firmengründer, Wolf, Peter, Bree, arbeitete, Anfang, der, siebziger, Jahre, als, Möbelvertreter, ,, als, er, einen, fliegenden, Händler, aus, dem, Libanon, traf, .]   \n",
       "2   [Ob, sie, dabei, nach, dem, Runden, Tisch, am, 23., April, in, Berlin, durch, ein, pädagogisches, Konzept, unterstützt, wird, ,, ist, allerdings, zu, bezweifeln, .]   \n",
       "3                                                  [Bayern, München, ist, wieder, alleiniger, Top-, Favorit, auf, den, Gewinn, der, deutschen, Fußball-Meisterschaft, .]   \n",
       "4                                                                  [Dabei, hätte, der, tapfere, Schlussmann, allen, Grund, gehabt, ,, sich, viel, früher, aufzuregen, .]   \n",
       "\n",
       "                                                                                      tag1  \\\n",
       "0  [B-PER, O, O, O, B-ORG, O, O, O, O, B-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "1       [O, B-PER, I-PER, I-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O]   \n",
       "2             [O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "3                              [B-ORG, I-ORG, O, O, O, O, O, O, O, O, O, B-LOCderiv, O, O]   \n",
       "4                                               [O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "\n",
       "                                                                          tag2  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "1           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "2     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "3                           [B-LOC, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "4                                   [O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "\n",
       "                                                                                                                                                                           ds_type  \\\n",
       "0  [train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train]   \n",
       "1                       [train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train]   \n",
       "2         [train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train]   \n",
       "3                                                                               [train, train, train, train, train, train, train, train, train, train, train, train, train, train]   \n",
       "4                                                                               [train, train, train, train, train, train, train, train, train, train, train, train, train, train]   \n",
       "\n",
       "                                                                      n_tokens  \n",
       "0  [3, 1, 1, 1, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1]  \n",
       "1           [3, 1, 1, 2, 1, 1, 1, 3, 1, 1, 4, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1]  \n",
       "2     [1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 3, 1]  \n",
       "3                                   [1, 1, 1, 1, 2, 2, 3, 1, 1, 1, 1, 1, 3, 1]  \n",
       "4                                   [1, 1, 1, 2, 2, 1, 1, 3, 1, 1, 1, 1, 3, 1]  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "germ_eval_df = germ_eval_df.groupby(by='seq_id').agg(list).reset_index()\n",
    "germ_eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_TokenTensorCategory(TensorBase): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_TokenCategorize(Transform):\n",
    "    \"Reversible transform of a list of category string to `vocab` id\"\n",
    "    \n",
    "    def __init__(self, vocab=None, ignore_token=None, ignore_token_id=None):  \n",
    "        self.vocab = None if vocab is None else CategoryMap(vocab)\n",
    "        self.ignore_token = '[xIGNx]' if ignore_token is None else ignore_token\n",
    "        self.ignore_token_id = CrossEntropyLossFlat().ignore_index if ignore_token_id is None else ignore_token_id\n",
    "        \n",
    "        self.loss_func, self.order = CrossEntropyLossFlat(ignore_index=self.ignore_token_id), 1\n",
    "\n",
    "    def setups(self, dsets):\n",
    "        if self.vocab is None and dsets is not None: self.vocab = CategoryMap(dsets)\n",
    "        self.c = len(self.vocab)\n",
    "\n",
    "    def encodes(self, labels):\n",
    "        ids = [[self.vocab.o2i[lbl]] + [self.ignore_token_id]*(n_subtoks-1) for lbl, n_subtoks in labels] \n",
    "        return HF_TokenTensorCategory(reduce(operator.concat, ids))\n",
    "    \n",
    "    def decodes(self, encoded_labels): \n",
    "        return Category([(self.vocab[lbl_id]) for lbl_id in encoded_labels if lbl_id != self.ignore_token_id ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`HF_TokenCategorize` modifies the fastai `Categorize` transform in a couple of ways.  First, it allows your targets to consist of a `Category` ***per*** token, and second, it uses the idea of an `ignore_token` to mask subtokens that don't need a prediction.  For example, the target of special tokens (e.g., pad, cls, sep) are set to `ignore_token` as are subsequent sub-tokens of a given token should more than 1 sub-token make it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def HF_TokenCategoryBlock(vocab=None, ignore_token=None, ignore_token_id=None):\n",
    "    \"`TransformBlock` for single-label categorical targets\"\n",
    "    \n",
    "    return TransformBlock(type_tfms=HF_TokenCategorize(vocab=vocab, \n",
    "                                                       ignore_token=ignore_token,\n",
    "                                                       ignore_token_id=ignore_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"HF_TokenCategoryBlock\" class=\"doc_header\"><code>HF_TokenCategoryBlock</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>HF_TokenCategoryBlock</code>(**`vocab`**=*`None`*, **`ignore_token`**=*`None`*, **`ignore_token_id`**=*`None`*)\n",
       "\n",
       "`TransformBlock` for single-label categorical targets"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(HF_TokenCategoryBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def build_hf_input(task:ForTokenClassificationTask, tokenizer, a_tok_ids, b_tok_ids=None, targets=None,\n",
    "                   max_length=512, pad_to_max_length=True, truncation_strategy='longest_first'):\n",
    "\n",
    "    res = tokenizer.prepare_for_model(a_tok_ids, b_tok_ids, \n",
    "                                      max_length=max_length, \n",
    "                                      pad_to_max_length=pad_to_max_length,\n",
    "                                      truncation_strategy=truncation_strategy, \n",
    "                                      return_special_tokens_mask=True,\n",
    "                                      return_tensors='pt')\n",
    "\n",
    "    input_ids = res['input_ids'][0]\n",
    "    token_type_ids = res['token_type_ids'][0] if ('token_type_ids' in res) else torch.tensor([-9999]) \n",
    "    attention_mask = res['attention_mask'][0] if ('attention_mask' in res) else torch.tensor([-9999]) \n",
    "    \n",
    "    # we assume that first target = the categories we want to predict for each token\n",
    "    if (len(targets) > 0):\n",
    "        target_cls = type(targets[0])\n",
    "        idx_first_input_id = res['special_tokens_mask'].index(0)\n",
    "        targ_ids = target_cls([ el*-100 if (el == 1) else targets[0][idx-idx_first_input_id].item() \n",
    "                    for idx, el in enumerate(res['special_tokens_mask']) ])\n",
    "\n",
    "        # just in case there are other targets, we modify the first with the padded targ_ids\n",
    "        updated_targets = list(targets)\n",
    "        updated_targets[0] = targ_ids\n",
    "    else:\n",
    "        updated_targets= list(targets)\n",
    "    \n",
    "    return HF_BaseInput([input_ids, token_type_ids, attention_mask]), tuple(updated_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a custom `build_hf_input` because we need to align the target tokens with the input tokens (e.g., if there are 512 input tokens there need to be 512 targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single input\n",
    "blocks = (\n",
    "    HF_TextBlock.from_df(text_cols_lists=[['token']], \n",
    "                         hf_arch=hf_arch, \n",
    "                         hf_tokenizer=hf_tokenizer, \n",
    "                         tok_func_mode='list', \n",
    "                         task=ForTokenClassificationTask()),\n",
    "    HF_TokenCategoryBlock(vocab=labels)\n",
    ")\n",
    "\n",
    "def get_y(inp):\n",
    "    return [ (label, len(hf_tokenizer.tokenize(str(entity)))) for entity, label in zip(inp.token, inp.tag1) ]\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, \n",
    "                   get_x=lambda x: x.text0,\n",
    "                   get_y=get_y,\n",
    "                   splitter=RandomSplitter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note in the example above we had to define a `get_y` in order to return both the entity we want to predict a category for, as well as, how many subtokens are used by the `hf_tokenizer` to represent it.  This is necessary for the input/target alignment discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dblock.summary(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls = dblock.dataloaders(germ_eval_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([4, 512]), torch.Size([4, 512]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b), b[0][0].shape, b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scenes of a Sexual Nature ( GB 2006 ) - Regie : Ed Blum Shortbus ( USA 2006 ) - Regie : John Cameron Mitchell : Film über den gleichnamigen New Yorker Club, der verschiedensten Paaren eine Plattform zur Aufarbeitung ihrer Probleme bietet.</td>\n",
       "      <td>['B-OTH', 'I-OTH', 'I-OTH', 'I-OTH', 'I-OTH', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'B-OTH', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'B-LOCderiv', 'I-LOCderiv', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ausbildung In der Bundesrepublik Deutschland können die Befähigungszeugnisse „ Kapitän BK ( Kleine Hochseefischerei ) und „ Kapitän BG ( Große Hochseefischerei ) im Rahmen einer bundeseinheitlichen Regelung erworben werden.</td>\n",
       "      <td>['O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(hf_tokenizer=hf_tokenizer, max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_data.ipynb.\n",
      "Converted 02_modeling.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

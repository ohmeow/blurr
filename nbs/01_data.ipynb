{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data\n",
    "\n",
    "> This module contains the bits required to use the fastai DataBlock API and/or mid-level data processing pipelines to organize your data in a way modelable by huggingface transformer implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import ast\n",
    "from functools import reduce\n",
    "\n",
    "from blurr.utils import *\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "from fastai2.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import pdb\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "#cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f'Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base tokenization, batch transform, and DataBlock methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_BaseInput(list): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `HF_BaseInput` object is used to encapsulate all the inputs required by whatever huggingface model we are using. We use it as a container for the `input_ids`, `token_type_ids`, and `attention_mask` tensors required by most models, and also as a mean to customize @typedispatched functions like `DataLoaders.show_batch` and `Learner.show_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_Tokenizer():\n",
    "    \"\"\"huggingface friendly tokenization function.\"\"\"\n",
    "    def __init__(self, hf_arch, hf_tokenizer, mode='str', list_split_func=str.split, **kwargs):\n",
    "        store_attr(self, 'hf_arch, hf_tokenizer, mode, list_split_func')\n",
    "\n",
    "    def __call__(self, items): \n",
    "        for txt in items: yield self._tokenize(txt)\n",
    "\n",
    "    def _tokenize(self, txt):   \n",
    "        if (self.mode == 'str'): \n",
    "            return self.hf_tokenizer.tokenize(txt)\n",
    "        \n",
    "        if (self.mode == 'list'):\n",
    "            try: tokens = ast.literal_eval(txt)\n",
    "            except: tokens = self.list_split_func(txt)\n",
    "            finally:\n",
    "                return [sub_toks for entity in tokens for sub_toks in self.hf_tokenizer.tokenize(entity)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`HF_Tokenizer` complies with the requirements of a basic tokenization function in fastai.  See [here](http://dev.fast.ai/text.core#Tokenizing).\n",
    "\n",
    "We've updated the `_tokenize` method to operate on a string or a list (the later being very handy for tasks like token classification whereby the examples consist of a list of tokens and a list of labels for each)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def build_hf_input(task, tokenizer, a_tok_ids, b_tok_ids=None, targets=None,\n",
    "                   max_length=512, pad_to_max_length=True, truncation_strategy='longest_first'):\n",
    "\n",
    "    res = tokenizer.prepare_for_model(a_tok_ids, b_tok_ids, \n",
    "                                       max_length=max_length, pad_to_max_length=pad_to_max_length,\n",
    "                                       truncation_strategy=truncation_strategy, return_tensors='pt')\n",
    "    \n",
    "    input_ids = res['input_ids'][0]\n",
    "    token_type_ids = res['token_type_ids'][0] if ('token_type_ids' in res) else torch.tensor([-9999]) \n",
    "    attention_mask = res['attention_mask'][0] if ('attention_mask' in res) else torch.tensor([-9999]) \n",
    "    \n",
    "    return HF_BaseInput([input_ids, token_type_ids, attention_mask]), targets\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`build_hf_input` uses fastai's @typedispatched decorator to provide for complete flexibility in terms of how your numericalized tokens are assembled, and also what you return via `HF_BaseInput` and as your targets.  You can override this implementation as needed by assigning a type to the `task` argument (and optionally the `tokenizer` argument as well).\n",
    "\n",
    "What you return here is what will be fed into your huggingface model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_BatchTransform(Transform):\n",
    "    \"\"\"Handles everything you need to assemble a mini-batch of inputs and targets\"\"\"\n",
    "    def __init__(self, hf_arch, hf_tokenizer, max_seq_len=512, truncation_strategy='longest_first', task=None):\n",
    "        \n",
    "        self.hf_arch = hf_arch\n",
    "        self.hf_tokenizer = hf_tokenizer\n",
    "        store_attr(self, 'max_seq_len, truncation_strategy, task')\n",
    "        \n",
    "    def encodes(self, samples):\n",
    "        \n",
    "        encoded_samples = []\n",
    "        for idx, sample in enumerate(samples):\n",
    "            \n",
    "            if (isinstance(sample[0], tuple)):\n",
    "                a_tok_ids = sample[0][0].tolist()\n",
    "                b_tok_ids = sample[0][1].tolist()\n",
    "            else:\n",
    "                a_tok_ids = sample[0].tolist()\n",
    "                b_tok_ids = None\n",
    "\n",
    "            hf_base_input, targets = build_hf_input(self.task, self.hf_tokenizer, \n",
    "                                                    a_tok_ids, b_tok_ids, sample[1:],\n",
    "                                                    self.max_seq_len, True, self.truncation_strategy)\n",
    "            \n",
    "            encoded_samples.append((hf_base_input, *targets))\n",
    "            \n",
    "        return encoded_samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_TextBlock(TransformBlock):\n",
    "    \n",
    "    @delegates(Numericalize.__init__)\n",
    "    def __init__(self, tok_tfms, hf_arch, hf_tokenizer, hf_batch_tfm=None, vocab=None, task=None,\n",
    "                 max_seq_len=512, **kwargs):\n",
    "\n",
    "        if hf_batch_tfm is None:\n",
    "            hf_batch_tfm = HF_BatchTransform(hf_arch, hf_tokenizer, max_seq_len=max_seq_len,\n",
    "                                             truncation_strategy='longest_first', task=task)\n",
    "            \n",
    "        return super().__init__(type_tfms=[*tok_tfms, Numericalize(vocab, **kwargs)],\n",
    "                                dl_type=SortedDL, \n",
    "                                dls_kwargs={ 'before_batch': hf_batch_tfm })\n",
    "\n",
    "    @classmethod\n",
    "    @delegates(Tokenizer.from_df, keep=True)\n",
    "    def from_df(cls, text_cols_lists, hf_arch, hf_tokenizer, hf_batch_tfm=None, vocab=None, task=None, \n",
    "                tok_func_mode='str', res_col_names=None, max_seq_len=512, **kwargs):\n",
    "        \"\"\"Creates a HF_TextBlock via a pandas DataFrame\"\"\"\n",
    "        \n",
    "        # grab hf tokenizer class to do the actual tokenization (via tok_func) and its vocab\n",
    "        tokenizer_cls = partial(HF_Tokenizer, hf_arch=hf_arch, hf_tokenizer=hf_tokenizer, mode=tok_func_mode)\n",
    "        if (vocab is None): vocab = list(hf_tokenizer.get_vocab())\n",
    "\n",
    "        # build the column name(s) returned after tokenization\n",
    "        if (res_col_names is None): res_col_names = [ f'text{i}' for i in range(len(text_cols_lists)) ] \n",
    "    \n",
    "        tok_tfms = [ Tokenizer.from_df(text_cols, \n",
    "                                       res_col_name=res_col_name, \n",
    "                                       tok_func=tokenizer_cls,\n",
    "                                       rules=[], **kwargs) \n",
    "                    for text_cols, res_col_name in zip(text_cols_lists, res_col_names) ]\n",
    "  \n",
    "        return cls(tok_tfms, hf_arch=hf_arch, hf_tokenizer=hf_tokenizer, hf_batch_tfm=hf_batch_tfm, \n",
    "                   vocab=vocab, task=task, max_seq_len=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"HF_TextBlock.from_df\" class=\"doc_header\"><code>HF_TextBlock.from_df</code><a href=\"__main__.py#L16\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>HF_TextBlock.from_df</code>(**`text_cols_lists`**, **`hf_arch`**, **`hf_tokenizer`**, **`hf_batch_tfm`**=*`None`*, **`vocab`**=*`None`*, **`task`**=*`None`*, **`tok_func_mode`**=*`'str'`*, **`res_col_names`**=*`None`*, **`max_seq_len`**=*`512`*, **`tok_func`**=*`'SpacyTokenizer'`*, **`rules`**=*`None`*, **`sep`**=*`' '`*, **`n_workers`**=*`16`*, **`mark_fields`**=*`None`*, **`res_col_name`**=*`'text'`*, **\\*\\*`kwargs`**)\n",
       "\n",
       "Creates a HF_TextBlock via a pandas DataFrame"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(HF_TextBlock.from_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, we've only implemented building this block from a pandas DataFrame.  It handles single and multiple text inputs so that it can be used out-of-the-box against any model in the huggingface arsenal (e.g. sequence classification, question-answer, summarization, token classification, etc...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence classification (e.g., models that require a single text input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "\n",
    "model_path = Path('models')\n",
    "imdb_df = pd.read_csv(path/'texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.&lt;br /&gt;&lt;br /&gt;But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.&lt;br /&gt;&lt;br /&gt;Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.&lt;br /&gt;&lt;br /&gt;Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie \"Duty, Honor, Country\" are not just mere words blathered from the lips of a high-brassed offic...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  \\\n",
       "0  negative   \n",
       "1  positive   \n",
       "2  negative   \n",
       "3  positive   \n",
       "4  negative   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0                                                                                                                                                                                                    Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!   \n",
       "1  This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.<br /><br />But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...   \n",
       "2  Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.<br /><br />Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...   \n",
       "3  Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.<br /><br />Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie \"Duty, Honor, Country\" are not just mere words blathered from the lips of a high-brassed offic...   \n",
       "4  This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...   \n",
       "\n",
       "   is_valid  \n",
       "0     False  \n",
       "1     False  \n",
       "2     False  \n",
       "3     False  \n",
       "4     False  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a bunch of ways we can get at the four huggingface elements we need (e.g., architecture name, tokenizer, config, and model).  We can just create them directly, or we can use one of the helper methods available via `BLURR_MODEL_HELPER`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = HF_TASKS_AUTO.ForSequenceClassification\n",
    "\n",
    "pretrained_model_name = \"roberta-base\" # \"distilbert-base-uncased\" \"bert-base-uncased\"\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "\n",
    "hf_arch, hf_tokenizer, hf_config, hf_model = BLURR_MODEL_HELPER.get_auto_hf_objects(pretrained_model_name, \n",
    "                                                                                    task=task, \n",
    "                                                                                    config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have those elements, you can create your `DataBlock` as simple as the below. Note that you can use multiple columns in your DataFrame to make up the *single* text element required by `HF_TextBlock` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single input\n",
    "blocks = (\n",
    "    HF_TextBlock.from_df(text_cols_lists=[['text']], hf_arch=hf_arch, hf_tokenizer=hf_tokenizer),\n",
    "    CategoryBlock\n",
    ")\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, \n",
    "                   get_x=lambda x: x.text0,\n",
    "                   get_y=ColReader('label'), \n",
    "                   splitter=ColSplitter(col='is_valid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dblock.summary(imdb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls = dblock.dataloaders(imdb_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 4)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch(); len(b), len(b[0]), len(b[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 512]),\n",
       " torch.Size([4, 1]),\n",
       " torch.Size([4, 512]),\n",
       " torch.Size([4]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0][0].shape, b[0][1].shape, b[0][2].shape, b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_batch(x:HF_BaseInput, y, samples, hf_tokenizer, ctxs=None, max_n=6, **kwargs):        \n",
    "    if ctxs is None: ctxs = get_empty_df(min(len(samples), max_n))\n",
    "        \n",
    "    samples = samples = L((TitledStr(hf_tokenizer.decode(inp).replace(hf_tokenizer.pad_token, '')),*s[1:]) \n",
    "                          for inp, s in zip(x[0], samples))\n",
    "    \n",
    "    ctxs = show_batch[object](x, y, samples, max_n=max_n, ctxs=ctxs, **kwargs)\n",
    "\n",
    "    display_df(pd.DataFrame(ctxs))\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;Raising Victor Vargas: A Review&lt;br /&gt;&lt;br /&gt;You know, Raising Victor Vargas is like sticking your hands into a big, steaming bowl of oatmeal. It's warm and gooey, but you're not sure if it feels right. Try as I might, no matter how warm and gooey Raising Victor Vargas became I was always aware that something didn't quite feel right. Victor Vargas suffers from a certain overconfidence on the director's part. Apparently, the director thought that the ethnic backdrop of a Latino family on the lower east side, and an idyllic storyline would make the film critic proof. He was right, but it didn't fool me. Raising Victor Vargas is the story about a seventeen-year old boy called, you guessed it, Victor Vargas (Victor Rasuk) who lives his teenage years chasing more skirt than the Rolling Stones could do in all the years they've toured. The movie starts off in `Ugly Fat' Donna's bedroom where Victor is sure to seduce her, but a cry from outside disrupts his plans when his best-friend Harold (Kevin Rivera) comes-a-looking for him. Caught in the attempt by Harold and his sister, Victor Vargas runs off for damage control. Yet even with the embarrassing implication that he's been boffing the homeliest girl in the neighborhood, nothing dissuades young Victor from going off on the hunt for more fresh meat. On a hot, New York City day they make way to the local public swimming pool where Victor's eyes catch a glimpse of the lovely young nymph Judy (Judy Marte), who's not just pretty, but a strong and independent too. The relationship that develops between Victor and Judy becomes the focus of the film. The story also focuses on Victor's family that is comprised of his grandmother or abuelita (Altagracia Guzman), his brother Nino (also played by real life brother to Victor, Silvestre Rasuk) and his sister Vicky (Krystal Rodriguez). The action follows Victor between scenes with Judy and scenes with his family. Victor tries to cope with being an oversexed pimp-daddy, his feelings for Judy and his grandmother's conservative Catholic upbringing.&lt;br /&gt;&lt;br /&gt;The problems that arise from Raising Victor Vargas are a few, but glaring errors. Throughout the film you get to know certain characters like Vicky, Nino, Grandma,&lt;/s&gt;</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;Many neglect that this isn't just a classic due to the fact that it's the first 3D game, or even the first shoot-'em-up. It's also one of the first stealth games, one of the only(and definitely the first) truly claustrophobic games, and just a pretty well-rounded gaming experience in general. With graphics that are terribly dated today, the game thrusts you into the role of B.J.(don't even *think* I'm going to attempt spelling his last name!), an American P.O.W. caught in an underground bunker. You fight and search your way through tunnels in order to achieve different objectives for the six episodes(but, let's face it, most of them are just an excuse to hand you a weapon, surround you with Nazis and send you out to waste one of the Nazi leaders). The graphics are, as I mentioned before, quite dated and very simple. The least detailed of basically any 3D game released by a professional team of creators. If you can get over that, however(and some would suggest that this simplicity only adds to the effect the game has on you), then you've got one heck of a good shooter/sneaking game. The game play consists of searching for keys, health and ammo, blasting enemies(aforementioned Nazis, and a \"boss enemy\" per chapter) of varying difficulty(which, of course, grows as you move further in the game), unlocking doors and looking for secret rooms. There is a bonus count after each level is beaten... it goes by how fast you were(basically, if you beat the 'par time', which is the time it took a tester to go through the same level; this can be quite fun to try and beat, and with how difficult the levels are to find your way in, they are even challenging after many play-throughs), how much Nazi gold(treasure) you collected and how many bad guys you killed. Basically, if you got 100% of any of aforementioned, you get a bonus, helping you reach the coveted high score placings. The game (mostly, but not always) allows for two contrastingly different methods of playing... stealthily or gunning down anything and everything you see. You can either run or walk, and amongst your weapons is also a knife... running is heard instantly the moment you enter the same room as the guard, as is gunshots. Many guards are found standing with their backs turned to you&lt;/s&gt;</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(hf_tokenizer=hf_tokenizer, max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answering (e.g., models that require two text inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've provided a simple subset of a pre-processed SQUADv2 dataset below just for demonstration purposes. There is a lot that can be done to make this much better and more fully functional.  The idea here is just to show you how things can work for tasks beyond sequence classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path('./')\n",
    "squad_df = pd.read_csv(path/'squad_sample.csv'); len(squad_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question_id</th>\n",
       "      <th>question_text</th>\n",
       "      <th>is_impossible</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New_York_City</td>\n",
       "      <td>The New York City Fire Department (FDNY), provides fire protection, technical rescue, primary response to biological, chemical, and radioactive hazards, and emergency medical services for the five boroughs of New York City. The New York City Fire Department is the largest municipal fire department in the United States and the second largest in the world after the Tokyo Fire Department. The FDNY employs approximately 11,080 uniformed firefighters and over 3,300 uniformed EMTs and paramedics. The FDNY's motto is New York's Bravest.</td>\n",
       "      <td>56d1076317492d1400aab78c</td>\n",
       "      <td>What does FDNY stand for?</td>\n",
       "      <td>False</td>\n",
       "      <td>New York City Fire Department</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cyprus</td>\n",
       "      <td>Following the death in 1473 of James II, the last Lusignan king, the Republic of Venice assumed control of the island, while the late king's Venetian widow, Queen Catherine Cornaro, reigned as figurehead. Venice formally annexed the Kingdom of Cyprus in 1489, following the abdication of Catherine. The Venetians fortified Nicosia by building the Venetian Walls, and used it as an important commercial hub. Throughout Venetian rule, the Ottoman Empire frequently raided Cyprus. In 1539 the Ottomans destroyed Limassol and so fearing the worst, the Venetians also fortified Famagusta and Kyrenia.</td>\n",
       "      <td>572e7f8003f98919007566df</td>\n",
       "      <td>In what year did the Ottomans destroy Limassol?</td>\n",
       "      <td>False</td>\n",
       "      <td>1539</td>\n",
       "      <td>481</td>\n",
       "      <td>485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           title  \\\n",
       "0  New_York_City   \n",
       "1         Cyprus   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               context  \\\n",
       "0                                                              The New York City Fire Department (FDNY), provides fire protection, technical rescue, primary response to biological, chemical, and radioactive hazards, and emergency medical services for the five boroughs of New York City. The New York City Fire Department is the largest municipal fire department in the United States and the second largest in the world after the Tokyo Fire Department. The FDNY employs approximately 11,080 uniformed firefighters and over 3,300 uniformed EMTs and paramedics. The FDNY's motto is New York's Bravest.   \n",
       "1  Following the death in 1473 of James II, the last Lusignan king, the Republic of Venice assumed control of the island, while the late king's Venetian widow, Queen Catherine Cornaro, reigned as figurehead. Venice formally annexed the Kingdom of Cyprus in 1489, following the abdication of Catherine. The Venetians fortified Nicosia by building the Venetian Walls, and used it as an important commercial hub. Throughout Venetian rule, the Ottoman Empire frequently raided Cyprus. In 1539 the Ottomans destroyed Limassol and so fearing the worst, the Venetians also fortified Famagusta and Kyrenia.   \n",
       "\n",
       "                question_id                                    question_text  \\\n",
       "0  56d1076317492d1400aab78c                        What does FDNY stand for?   \n",
       "1  572e7f8003f98919007566df  In what year did the Ottomans destroy Limassol?   \n",
       "\n",
       "   is_impossible                    answer_text  answer_start  answer_end  \n",
       "0          False  New York City Fire Department             4          33  \n",
       "1          False                           1539           481         485  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len= 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_df = squad_df[(squad_df.answer_end < max_seq_len) & (squad_df.is_impossible == False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = HF_TASKS_AUTO.ForQuestionAnswering\n",
    "\n",
    "pretrained_model_name = \"roberta-base\"\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "\n",
    "hf_arch, hf_tokenizer, hf_config, hf_model = BLURR_MODEL_HELPER.get_auto_hf_objects(pretrained_model_name, \n",
    "                                                                                    task=task, \n",
    "                                                                                    config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dict(enumerate(range(max_seq_len)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we utilize the @typedispatch decorator to completely change how we'll tokenize the data for the `ForQuestionAnsweringTask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def build_hf_input(task:ForQuestionAnsweringTask, tokenizer, \n",
    "                   a_tok_ids, b_tok_ids=None, targets=None,\n",
    "                   max_length=512, pad_to_max_length=True, truncation_strategy=None):\n",
    "    \n",
    "    if (truncation_strategy is None):\n",
    "        truncation_strategy = \"only_second\" if tokenizer.padding_side == \"right\" else \"only_first\"\n",
    "\n",
    "    res = tokenizer.prepare_for_model(a_tok_ids if tokenizer.padding_side == \"right\" else b_tok_ids, \n",
    "                                      b_tok_ids if tokenizer.padding_side == \"right\" else a_tok_ids,\n",
    "                                      max_length=max_length, \n",
    "                                      pad_to_max_length=pad_to_max_length,\n",
    "                                      truncation_strategy=truncation_strategy, \n",
    "                                      return_tensors='pt')\n",
    "    \n",
    "    input_ids = res['input_ids'][0]\n",
    "    token_type_ids = res['token_type_ids'][0] if ('token_type_ids' in res) else torch.tensor([-9999]) \n",
    "    attention_mask = res['attention_mask'][0] if ('attention_mask' in res) else torch.tensor([-9999]) \n",
    "    \n",
    "    return HF_BaseInput([input_ids, token_type_ids, attention_mask]), targets\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we demonstrate some more of the extensibility bits of the framework, by passing in our own instance of `HF_BatchTransform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional): override HF_BatchTransform defaults\n",
    "hf_batch_tfm = HF_BatchTransform(hf_arch, hf_tokenizer, task=ForQuestionAnsweringTask(),\n",
    "                                 max_seq_len=128, truncation_strategy=None)\n",
    "\n",
    "blocks = (\n",
    "    HF_TextBlock.from_df(text_cols_lists=[['question_text'],['context']],\n",
    "                         hf_arch=hf_arch, \n",
    "                         hf_tokenizer=hf_tokenizer, \n",
    "                         hf_batch_tfm=hf_batch_tfm),\n",
    "    CategoryBlock(vocab=vocab),\n",
    "    CategoryBlock(vocab=vocab)\n",
    ")\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, \n",
    "                   get_x=lambda x: (x.text0, x.text1),\n",
    "                   get_y=[ColReader('answer_start'), ColReader('answer_end')],\n",
    "                   splitter=RandomSplitter(),\n",
    "                   n_inp=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dblock.summary(squad_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls = dblock.dataloaders(squad_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 4, 4)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch(); len(b), len(b[0]), len(b[1]), len(b[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 128]),\n",
       " torch.Size([4, 1]),\n",
       " torch.Size([4, 128]),\n",
       " torch.Size([4]),\n",
       " torch.Size([4]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0][0].shape, b[0][1].shape, b[0][2].shape, b[1].shape, b[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>category_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;What on behalf of a state cannot invalidate that state's consent to a treaty?&lt;/s&gt;&lt;/s&gt;A state's consent may be invalidated if there was an erroneous understanding of a fact or situation at the time of conclusion, which formed the \"essential basis\" of the state's consent. Consent will not be invalidated if the misunderstanding was due to the state's own conduct, or if the truth should have been evident.&lt;/s&gt;</td>\n",
       "      <td>265</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;When did Beyonce become the first African American woman to win Pop songwriter of the year?&lt;/s&gt;&lt;/s&gt;In 2001, she became the first African-American woman and second woman songwriter to win the Pop Songwriter of the Year award at the American Society of Composers, Authors, and Publishers Pop Music Awards. Beyonc√© was the third woman to have writing credits on three number one songs (\"Irreplaceable\", \"Grillz\" and \"Check on It\") in the same year, after Carole King in 1971 and Mariah Carey in 1991. She is tied with American songwriter Diane Warren at third with nine song&lt;/s&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(hf_tokenizer=hf_tokenizer, max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token classification (e.g., NER tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>token</th>\n",
       "      <th>tag1</th>\n",
       "      <th>tag2</th>\n",
       "      <th>ds_type</th>\n",
       "      <th>seq_id</th>\n",
       "      <th>n_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Schartau</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>O</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>sagte</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>dem</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>\"</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Tagesspiegel</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>O</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pos         token   tag1 tag2 ds_type  seq_id  n_tokens\n",
       "0    1      Schartau  B-PER    O   train       1         3\n",
       "1    2         sagte      O    O   train       1         1\n",
       "2    3           dem      O    O   train       1         1\n",
       "3    4             \"      O    O   train       1         1\n",
       "4    5  Tagesspiegel  B-ORG    O   train       1         3"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "germ_eval_df = pd.read_csv('./germeval2014_sample.csv')\n",
    "germ_eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bert',\n",
       " transformers.tokenization_bert.BertTokenizer,\n",
       " transformers.configuration_bert.BertConfig,\n",
       " transformers.modeling_bert.BertForTokenClassification)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = HF_TASKS_AUTO.ForTokenClassification\n",
    "\n",
    "pretrained_model_name = \"bert-base-multilingual-cased\"\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "\n",
    "hf_arch, hf_tokenizer, hf_config, hf_model = BLURR_MODEL_HELPER.get_auto_hf_objects(pretrained_model_name, \n",
    "                                                                                    task=task, \n",
    "                                                                                    config=config)\n",
    "hf_arch, type(hf_tokenizer), type(hf_config), type(hf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_config.max_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-LOC', 'B-LOCderiv', 'B-LOCpart', 'B-ORG', 'B-ORGpart', 'B-OTH', 'B-PER', 'B-PERpart', 'I-LOC', 'I-LOCderiv', 'I-ORG', 'I-ORGpart', 'I-OTH', 'I-PER', 'O']\n"
     ]
    }
   ],
   "source": [
    "labels = sorted(germ_eval_df.tag1.unique())\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_id</th>\n",
       "      <th>pos</th>\n",
       "      <th>token</th>\n",
       "      <th>tag1</th>\n",
       "      <th>tag2</th>\n",
       "      <th>ds_type</th>\n",
       "      <th>n_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]</td>\n",
       "      <td>[Schartau, sagte, dem, \", Tagesspiegel, \", vom, Freitag, ,, Fischer, sei, \", in, einer, Weise, aufgetreten, ,, die, alles, andere, als, √ºberzeugend, war, \", .]</td>\n",
       "      <td>[B-PER, O, O, O, B-ORG, O, O, O, O, B-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train]</td>\n",
       "      <td>[3, 1, 1, 1, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]</td>\n",
       "      <td>[Firmengr√ºnder, Wolf, Peter, Bree, arbeitete, Anfang, der, siebziger, Jahre, als, M√∂belvertreter, ,, als, er, einen, fliegenden, H√§ndler, aus, dem, Libanon, traf, .]</td>\n",
       "      <td>[O, B-PER, I-PER, I-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train]</td>\n",
       "      <td>[3, 1, 1, 2, 1, 1, 1, 3, 1, 1, 4, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]</td>\n",
       "      <td>[Ob, sie, dabei, nach, dem, Runden, Tisch, am, 23., April, in, Berlin, durch, ein, p√§dagogisches, Konzept, unterst√ºtzt, wird, ,, ist, allerdings, zu, bezweifeln, .]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 3, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>[Bayern, M√ºnchen, ist, wieder, alleiniger, Top-, Favorit, auf, den, Gewinn, der, deutschen, Fu√üball-Meisterschaft, .]</td>\n",
       "      <td>[B-ORG, I-ORG, O, O, O, O, O, O, O, O, O, B-LOCderiv, O, O]</td>\n",
       "      <td>[B-LOC, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[train, train, train, train, train, train, train, train, train, train, train, train, train, train]</td>\n",
       "      <td>[1, 1, 1, 1, 2, 2, 3, 1, 1, 1, 1, 1, 3, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]</td>\n",
       "      <td>[Dabei, h√§tte, der, tapfere, Schlussmann, allen, Grund, gehabt, ,, sich, viel, fr√ºher, aufzuregen, .]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[train, train, train, train, train, train, train, train, train, train, train, train, train, train]</td>\n",
       "      <td>[1, 1, 1, 2, 2, 1, 1, 3, 1, 1, 1, 1, 3, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seq_id  \\\n",
       "0       1   \n",
       "1       2   \n",
       "2       3   \n",
       "3       4   \n",
       "4       5   \n",
       "\n",
       "                                                                                           pos  \\\n",
       "0  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]   \n",
       "1              [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]   \n",
       "2      [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]   \n",
       "3                                              [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "4                                              [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]   \n",
       "\n",
       "                                                                                                                                                                   token  \\\n",
       "0        [Schartau, sagte, dem, \", Tagesspiegel, \", vom, Freitag, ,, Fischer, sei, \", in, einer, Weise, aufgetreten, ,, die, alles, andere, als, √ºberzeugend, war, \", .]   \n",
       "1  [Firmengr√ºnder, Wolf, Peter, Bree, arbeitete, Anfang, der, siebziger, Jahre, als, M√∂belvertreter, ,, als, er, einen, fliegenden, H√§ndler, aus, dem, Libanon, traf, .]   \n",
       "2   [Ob, sie, dabei, nach, dem, Runden, Tisch, am, 23., April, in, Berlin, durch, ein, p√§dagogisches, Konzept, unterst√ºtzt, wird, ,, ist, allerdings, zu, bezweifeln, .]   \n",
       "3                                                  [Bayern, M√ºnchen, ist, wieder, alleiniger, Top-, Favorit, auf, den, Gewinn, der, deutschen, Fu√üball-Meisterschaft, .]   \n",
       "4                                                                  [Dabei, h√§tte, der, tapfere, Schlussmann, allen, Grund, gehabt, ,, sich, viel, fr√ºher, aufzuregen, .]   \n",
       "\n",
       "                                                                                      tag1  \\\n",
       "0  [B-PER, O, O, O, B-ORG, O, O, O, O, B-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "1       [O, B-PER, I-PER, I-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O]   \n",
       "2             [O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "3                              [B-ORG, I-ORG, O, O, O, O, O, O, O, O, O, B-LOCderiv, O, O]   \n",
       "4                                               [O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "\n",
       "                                                                          tag2  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "1           [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "2     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "3                           [B-LOC, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "4                                   [O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "\n",
       "                                                                                                                                                                           ds_type  \\\n",
       "0  [train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train]   \n",
       "1                       [train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train]   \n",
       "2         [train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train, train]   \n",
       "3                                                                               [train, train, train, train, train, train, train, train, train, train, train, train, train, train]   \n",
       "4                                                                               [train, train, train, train, train, train, train, train, train, train, train, train, train, train]   \n",
       "\n",
       "                                                                      n_tokens  \n",
       "0  [3, 1, 1, 1, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1]  \n",
       "1           [3, 1, 1, 2, 1, 1, 1, 3, 1, 1, 4, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1]  \n",
       "2     [1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 3, 1]  \n",
       "3                                   [1, 1, 1, 1, 2, 2, 3, 1, 1, 1, 1, 1, 3, 1]  \n",
       "4                                   [1, 1, 1, 2, 2, 1, 1, 3, 1, 1, 1, 1, 3, 1]  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "germ_eval_df = germ_eval_df.groupby(by='seq_id').agg(list).reset_index()\n",
    "germ_eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_TokenCategorize(Transform):\n",
    "    \"Reversible transform of a list of category string to `vocab` id\"\n",
    "    \n",
    "    loss_func, order=CrossEntropyLossFlat(), 1\n",
    "    \n",
    "    def __init__(self, vocab=None, ignore_token=None, ignore_token_id=None):\n",
    "        self.vocab = None if vocab is None else CategoryMap(vocab)\n",
    "        self.ignore_token = '[xIGNx]' if ignore_token is None else ignore_token\n",
    "        self.ignore_token_id = CrossEntropyLossFlat().ignore_index if ignore_token_id is None else ignore_token_id\n",
    "\n",
    "    def setups(self, dsets):\n",
    "        if self.vocab is None and dsets is not None: self.vocab = CategoryMap(dsets)\n",
    "        self.c = len(self.vocab)\n",
    "\n",
    "    def encodes(self, labels):\n",
    "        ids = [[self.vocab.o2i[lbl]] + [self.ignore_token_id]*(n_subtoks-1) for lbl, n_subtoks in labels] \n",
    "        return TensorCategory(reduce(operator.concat, ids))\n",
    "    \n",
    "    def decodes(self, encoded_labels): \n",
    "        \n",
    "        return Category([(self.vocab[lbl_id]) for lbl_id in encoded_labels if lbl_id != self.ignore_token_id ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`HF_TokenCategorize` modifies the fastai `Categorize` transform in a couple of ways.  First, it allows your targets to consist of a `Category` ***per*** token, and second, it uses the idea of an `ignore_token` to mask subtokens that don't need a prediction.  For example, the target of special tokens (e.g., pad, cls, sep) are set to `ignore_token` as are subsequent sub-tokens of a given token should more than 1 sub-token make it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def HF_TokenCategoryBlock(vocab=None, ignore_token=None, ignore_token_id=None):\n",
    "    \"`TransformBlock` for single-label categorical targets\"\n",
    "    \n",
    "    return TransformBlock(type_tfms=HF_TokenCategorize(vocab=vocab, \n",
    "                                                       ignore_token=ignore_token,\n",
    "                                                       ignore_token_id=ignore_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"HF_TokenCategoryBlock\" class=\"doc_header\"><code>HF_TokenCategoryBlock</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>HF_TokenCategoryBlock</code>(**`vocab`**=*`None`*, **`ignore_token`**=*`None`*, **`ignore_token_id`**=*`None`*)\n",
       "\n",
       "`TransformBlock` for single-label categorical targets"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(HF_TokenCategoryBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def build_hf_input(task:ForTokenClassificationTask, tokenizer, a_tok_ids, b_tok_ids=None, targets=None,\n",
    "                   max_length=512, pad_to_max_length=True, truncation_strategy='longest_first'):\n",
    "\n",
    "    res = tokenizer.prepare_for_model(a_tok_ids, b_tok_ids, \n",
    "                                      max_length=max_length, \n",
    "                                      pad_to_max_length=pad_to_max_length,\n",
    "                                      truncation_strategy=truncation_strategy, \n",
    "                                      return_special_tokens_mask=True,\n",
    "                                      return_tensors='pt')\n",
    "\n",
    "    input_ids = res['input_ids'][0]\n",
    "    token_type_ids = res['token_type_ids'][0] if ('token_type_ids' in res) else torch.tensor([-9999]) \n",
    "    attention_mask = res['attention_mask'][0] if ('attention_mask' in res) else torch.tensor([-9999]) \n",
    "    \n",
    "    # we assume that first target = the categories we want to predict for each token\n",
    "    target_cls = type(targets[0])\n",
    "    idx_first_input_id = res['special_tokens_mask'].index(0)\n",
    "    targ_ids = target_cls([ el*-100 if (el == 1) else targets[0][idx-idx_first_input_id].item() \n",
    "                for idx, el in enumerate(res['special_tokens_mask']) ])\n",
    "    \n",
    "    # just in case there are other targets, we modify the first with the padded targ_ids\n",
    "    updated_targets = list(targets)\n",
    "    updated_targets[0] = targ_ids\n",
    "    \n",
    "    return HF_BaseInput([input_ids, token_type_ids, attention_mask]), tuple(updated_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a custom `build_hf_input` because we need to align the target tokens with the input tokens (e.g., if there are 512 input tokens there need to be 512 targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single input\n",
    "blocks = (\n",
    "    HF_TextBlock.from_df(text_cols_lists=[['token']], \n",
    "                         hf_arch=hf_arch, \n",
    "                         hf_tokenizer=hf_tokenizer, \n",
    "                         tok_func_mode='list', \n",
    "                         task=ForTokenClassificationTask()),\n",
    "    HF_TokenCategoryBlock(vocab=labels)\n",
    ")\n",
    "\n",
    "def get_y(inp):\n",
    "    return [ (label, len(hf_tokenizer.tokenize(str(entity)))) for entity, label in zip(inp.token, inp.tag1) ]\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, \n",
    "                   get_x=lambda x: x.text0,\n",
    "                   get_y=get_y,\n",
    "                   splitter=RandomSplitter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note in the example above we had to define a `get_y` in order to return both the entity we want to predict a category for, as well as, how many subtokens are used by the `hf_tokenizer` to represent it.  This is necessary for the input/target alignment discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dblock.summary(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls = dblock.dataloaders(germ_eval_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([4, 512]), torch.Size([4, 512]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b), b[0][0].shape, b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[CLS] Das SS - Freiwilligen - Grenadier - Regiment 88 wurde im M√§rz aus einer Kampfgruppe der SS - F√ºhrerschule des Wirtschafts - und Verwaltungsdienstes und Teilen des I. / SS - Polizei - Regiments 34 der Ordnungspolizei, Heeresangeh√∂rigen und Volkssturm gebildet. [SEP]</td>\n",
       "      <td>['O', 'B-ORGpart', 'I-ORGpart', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORGpart', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORGpart', 'I-ORGpart', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[CLS] Scenes of a Sexual Nature ( GB 2006 ) - Regie : Ed Blum Shortbus ( USA 2006 ) - Regie : John Cameron Mitchell : Film √ºber den gleichnamigen New Yorker Club, der verschiedensten Paaren eine Plattform zur Aufarbeitung ihrer Probleme bietet. [SEP]</td>\n",
       "      <td>['B-OTH', 'I-OTH', 'I-OTH', 'I-OTH', 'I-OTH', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'B-OTH', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'B-LOCderiv', 'I-LOCderiv', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(hf_tokenizer=hf_tokenizer, max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_data.ipynb.\n",
      "Converted 02_modeling.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp text.data.seq2seq.core\n",
    "#|default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| nbflags skip_exec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text.data.seq2seq.core\n",
    "\n",
    "> This module contains the core seq2seq (e.g., language modeling, summarization, translation) bits required to use the fastai DataBlock API and/or mid-level data processing pipelines to organize your data in a way modelable by Hugging Face transformer implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import warnings\n",
    "from typing import Optional\n",
    "\n",
    "from fastai.imports import *\n",
    "from fastai.losses import CrossEntropyLossFlat\n",
    "from fastai.text.data import SortedDL\n",
    "from fastai.torch_core import *\n",
    "from fastai.torch_imports import *\n",
    "from fastcore.all import *\n",
    "from transformers import PretrainedConfig, PreTrainedTokenizerBase, PreTrainedModel\n",
    "from transformers.utils import logging as hf_logging\n",
    "\n",
    "from blurr.text.data.core import BatchDecodeTransform, BatchTokenizeTransform, Preprocessor, TextBlock, TextInput, first_blurr_tfm\n",
    "from blurr.text.utils import get_hf_objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we're running with at the time this documentation was generated:\n",
      "torch: 1.10.1+cu111\n",
      "fastai: 2.5.6\n",
      "transformers: 4.16.2\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "import pdb\n",
    "\n",
    "from fastai.data.transforms import *\n",
    "from fastcore.test import *\n",
    "from nbdev import nbdev_export\n",
    "from nbdev.showdoc import show_doc\n",
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "from blurr.utils import print_versions\n",
    "from blurr.text.utils import BlurrText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "# silence all the HF warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "hf_logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "NLP = BlurrText()\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(\"What we're running with at the time this documentation was generated:\")\n",
    "print_versions(\"torch fastai transformers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "#|cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f\"Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bart',\n",
       " transformers.models.bart.configuration_bart.BartConfig,\n",
       " transformers.models.bart.tokenization_bart_fast.BartTokenizerFast,\n",
       " transformers.models.bart.modeling_bart.BartForConditionalGeneration)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model_name = \"facebook/bart-large-cnn\"\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(pretrained_model_name, model_cls=BartForConditionalGeneration)\n",
    "hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Starting with version 2.0, BLURR provides a preprocessing base class that can be used to build seq2seq preprocessed datasets from pandas DataFrames or Hugging Face Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Seq2SeqPreprocessor(Preprocessor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # The number of examples to process at a time\n",
    "        batch_size: int = 1000,\n",
    "        # The attribute holding the text\n",
    "        text_attr: str = \"text\",\n",
    "        # The maximum length (# of tokens) allowed for inputs. Will default to the max length allowed\n",
    "        # by the model if not provided\n",
    "        max_input_tok_length: Optional[int] = None,\n",
    "        # The attribute holding the summary\n",
    "        target_text_attr: str = \"summary\",\n",
    "        # The maximum length (# of tokens) allowed for targets\n",
    "        max_target_tok_length: Optional[int] = None,\n",
    "        # The attribute that should be created if your are processing individual training and validation\n",
    "        # datasets into a single dataset, and will indicate to which each example is associated\n",
    "        is_valid_attr: Optional[str] = \"is_valid\",\n",
    "        # Tokenization kwargs that will be applied with calling the tokenizer\n",
    "        tok_kwargs: dict = {},\n",
    "    ):\n",
    "        # remove \"max_length\" if set on tok_kwargs as this is set differently for inputs and targets\n",
    "        tok_kwargs.pop(\"max_length\", None)\n",
    "\n",
    "        super().__init__(hf_tokenizer, batch_size, text_attr, is_valid_attr, tok_kwargs=tok_kwargs)\n",
    "\n",
    "        # inputs\n",
    "        self.max_input_tok_length = max_input_tok_length if max_input_tok_length is not None else hf_tokenizer.model_max_length\n",
    "\n",
    "        # targets\n",
    "        self.target_text_attr = target_text_attr\n",
    "        self.max_target_tok_length = max_target_tok_length\n",
    "\n",
    "    def _tokenize_function(self, example):\n",
    "        # tokenize inputs\n",
    "        inputs = self.hf_tokenizer(example[self.text_attr], max_length=self.max_input_tok_length, **self.tok_kwargs)\n",
    "        # tokenize targets\n",
    "        with self.hf_tokenizer.as_target_tokenizer():\n",
    "            targets = self.hf_tokenizer(example[self.target_text_attr], max_length=self.max_target_tok_length, **self.tok_kwargs)\n",
    "\n",
    "        return (inputs, targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid-level API\n",
    "\n",
    "Base tokenization, batch transform, and DataBlock methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Seq2SeqTextInput` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Seq2SeqTextInput(TextInput):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Seq2SeqTextInput` object is returned from the decodes method of `Seq2SeqBatchTokenizeTransform` as a means to customize `@typedispatch`ed functions like `DataLoaders.show_batch` and `Learner.show_results`. The value will the your \"input_ids\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Seq2SeqBatchTokenizeTransform` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000, -0.8626,  0.1536],\n",
       "        [ 1.0000,  0.4264, -1.6034],\n",
       "        [ 1.0000,  0.1050,  0.9895]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "t = torch.randn((3, 3))\n",
    "\n",
    "F.pad(t, pad=(1, 0), value=1)[:, :-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Seq2SeqBatchTokenizeTransform(BatchTokenizeTransform):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (e.b., bert, bart, etc..)\n",
    "        hf_arch: str,\n",
    "        # A specific configuration instance you want to use\n",
    "        hf_config: PretrainedConfig,\n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # A Hugging Face model\n",
    "        hf_model: PreTrainedModel,\n",
    "        # To control whether the \"labels\" are included in your inputs. If they are, the loss will be calculated in\n",
    "        # the model's forward function and you can simply use `PreCalculatedLoss` as your `Learner`'s loss function to use it\n",
    "        include_labels: bool = True,\n",
    "        # The token ID that should be ignored when calculating the loss\n",
    "        ignore_token_id: int = CrossEntropyLossFlat().ignore_index,\n",
    "        # To control the length of the padding/truncation of the input sequence. It can be an integer or None,\n",
    "        # in which case it will default to the maximum length the model can accept. If the model has no\n",
    "        # specific maximum input length, truncation/padding to max_length is deactivated.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        max_length: int = None,\n",
    "        # To control the length of the padding/truncation of the target sequence. It can be an integer or None,\n",
    "        # in which case it will default to the maximum length the model can accept. If the model has no\n",
    "        # specific maximum input length, truncation/padding to max_length is deactivated.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        max_target_length: int = None,\n",
    "        # To control the `padding` applied to your `hf_tokenizer` during tokenization. If None, will default to\n",
    "        # `False` or `'do_not_pad'.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        padding: Union[bool, str] = True,\n",
    "        # To control `truncation` applied to your `hf_tokenizer` during tokenization. If None, will default to\n",
    "        # `False` or `do_not_truncate`.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        truncation: Union[bool, str] = True,\n",
    "        # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. Set this to `True`\n",
    "        # if your inputs are pre-tokenized (not numericalized)\n",
    "        is_split_into_words: bool = False,\n",
    "        # Any other keyword arguments you want included when using your `hf_tokenizer` to tokenize your inputs\n",
    "        tok_kwargs={},\n",
    "        # Any keyword arguments to pass to the `hf_model.generate` method\n",
    "        text_gen_kwargs={},\n",
    "        # Keyword arguments to apply to `BatchTokenizeTransform`\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            hf_arch,\n",
    "            hf_config,\n",
    "            hf_tokenizer,\n",
    "            hf_model,\n",
    "            include_labels=include_labels,\n",
    "            max_length=max_length,\n",
    "            padding=padding,\n",
    "            truncation=truncation,\n",
    "            is_split_into_words=False,\n",
    "            tok_kwargs=tok_kwargs.copy(),\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        store_attr()\n",
    "\n",
    "    def encodes(self, samples):\n",
    "        samples = L(samples)\n",
    "\n",
    "        # tokenize\n",
    "        src_texts = samples.itemgot(0).items\n",
    "        tgt_texts = samples.itemgot(1).items if (len(samples[0]) > 1) else None\n",
    "\n",
    "        # input text\n",
    "        inputs = self.hf_tokenizer(\n",
    "            src_texts, max_length=self.max_length, padding=self.padding, truncation=self.truncation, return_tensors=\"pt\", **self.tok_kwargs\n",
    "        )\n",
    "\n",
    "        # target text\n",
    "        targ_ids = [[]] * len(samples)\n",
    "        if tgt_texts:\n",
    "            with self.hf_tokenizer.as_target_tokenizer():\n",
    "                targ_inputs = self.hf_tokenizer(\n",
    "                    tgt_texts,\n",
    "                    max_length=self.max_target_length,\n",
    "                    padding=self.padding,\n",
    "                    truncation=self.truncation,\n",
    "                    return_tensors=\"pt\",\n",
    "                    **self.tok_kwargs\n",
    "                )\n",
    "\n",
    "                # padding tokens should be be changed to ignore_token_id so not factored into loss calculation\n",
    "                targ_inputs[\"input_ids\"].masked_fill_(targ_inputs[\"input_ids\"] == self.hf_tokenizer.pad_token_id, self.ignore_token_id)\n",
    "\n",
    "                # set targets to target input_ids (req. if calculating loss in fastai training loop and for show methods)\n",
    "                targ_ids = targ_inputs[\"input_ids\"].clone()\n",
    "\n",
    "                # if we want hugging face to calculate loss, set the inputs \"labels\" = the target \"input_ids\" ... including the labels\n",
    "                # will also tell the model to properly build the input's \"decoder_input_ids\" (right-shifted labels where the first token\n",
    "                # is [PAD] or something similar)\n",
    "                if self.include_labels:\n",
    "                    inputs[\"labels\"] = targ_inputs[\"input_ids\"]\n",
    "                else:\n",
    "                    decoder_start_tok_id = self.hf_config.get(\"decoder_start_token_id\", self.hf_config.pad_token_id)\n",
    "                    inputs[\"decoder_input_ids\"] = F.pad(targ_inputs[\"input_ids\"].clone(), pad=(1, 0), value=decoder_start_tok_id)[:, :-1]\n",
    "                    inputs[\"decoder_input_ids\"].masked_fill_(\n",
    "                        inputs[\"decoder_input_ids\"] == self.ignore_token_id, self.hf_tokenizer.pad_token_id\n",
    "                    )\n",
    "\n",
    "        # update samples with tokenized inputs (e.g. input_ids, attention_mask, etc...)\n",
    "        d_keys = inputs.keys()\n",
    "        updated_samples = [\n",
    "            (*[{k: inputs[k][idx] for k in d_keys}], *tuplify(targ_ids[idx]), *sample[2:]) for idx, sample in enumerate(samples)\n",
    "        ]\n",
    "\n",
    "        return updated_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a subclass of `BatchTokenizeTransform` for summarization tasks to add `decoder_input_ids` and `labels` (if we want Hugging Face to calculate the loss for us) to our inputs during training. See [here](https://huggingface.co/transformers/glossary.html#labels) and [here](https://huggingface.co/transformers/glossary.html#decoder-input-ids) for more information on these additional inputs used in summarization, translation, and conversational training tasks. How they should look for particular architectures can be found by looking at those model's `forward` function's docs (See [here](https://huggingface.co/transformers/model_doc/bart.html#transformers.BartModel.forward) for BART for example)\n",
    "\n",
    "Note also that `labels` is simply target_ids shifted to the right by one since the task to is to predict the next token based on the current (and all previous) `decoder_input_ids`.\n",
    "\n",
    "And lastly, we also update our targets to just be the `input_ids` of our target sequence so that fastai's `Learner.show_results` works (again, almost all the fastai bits require returning a single tensor to work)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Seq2SeqBatchDecodeTransform` -\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Seq2SeqBatchDecodeTransform(BatchDecodeTransform):\n",
    "    def decodes(self, encoded_samples):\n",
    "        input_ids = encoded_samples[\"input_ids\"] if (isinstance(encoded_samples, dict)) else encoded_samples\n",
    "        return self.input_return_type(input_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Seq2SeqTextBlock` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def default_text_gen_kwargs(hf_config, hf_model, task=None):\n",
    "    text_gen_kwargs = {}\n",
    "    hf_config_dict = hf_config.to_dict()\n",
    "\n",
    "    generate_func_args = list(inspect.signature(hf_model.generate).parameters.keys())\n",
    "    for k in generate_func_args:\n",
    "        if k in hf_config_dict:\n",
    "            text_gen_kwargs.update({k: hf_config_dict[k]})\n",
    "\n",
    "    # not all configs even have a task_specific_params property\n",
    "    if task is not None:\n",
    "        try:\n",
    "            text_gen_kwargs = {**text_gen_kwargs, **hf_config.task_specific_params[task]}\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return text_gen_kwargs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(default_text_gen_kwargs, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_length': 142,\n",
       " 'min_length': 56,\n",
       " 'do_sample': False,\n",
       " 'early_stopping': True,\n",
       " 'num_beams': 4,\n",
       " 'temperature': 1.0,\n",
       " 'top_k': 50,\n",
       " 'top_p': 1.0,\n",
       " 'repetition_penalty': 1.0,\n",
       " 'bad_words_ids': None,\n",
       " 'bos_token_id': 0,\n",
       " 'pad_token_id': 1,\n",
       " 'eos_token_id': 2,\n",
       " 'length_penalty': 2.0,\n",
       " 'no_repeat_ngram_size': 3,\n",
       " 'encoder_no_repeat_ngram_size': 0,\n",
       " 'num_return_sequences': 1,\n",
       " 'decoder_start_token_id': 2,\n",
       " 'use_cache': True,\n",
       " 'num_beam_groups': 1,\n",
       " 'diversity_penalty': 0.0,\n",
       " 'output_attentions': False,\n",
       " 'output_hidden_states': False,\n",
       " 'output_scores': False,\n",
       " 'return_dict_in_generate': False,\n",
       " 'forced_bos_token_id': 0,\n",
       " 'forced_eos_token_id': 2,\n",
       " 'remove_invalid_values': False}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_text_gen_kwargs(hf_config, hf_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Seq2SeqTextBlock(TextBlock):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (not required if passing in an\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_arch: str = None,\n",
    "        # A Hugging Face configuration object (not required if passing in an\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_config: PretrainedConfig = None,\n",
    "        # A Hugging Face tokenizer (not required if passing in an\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_tokenizer: PreTrainedTokenizerBase = None,\n",
    "        # A Hugging Face model (not required if passing in an\n",
    "        # instance of `BatchTokenizeTransform` to `before_batch_tfm`)\n",
    "        hf_model: PreTrainedModel = None,\n",
    "        # The before_batch_tfm you want to use to tokenize your raw data on the fly\n",
    "        # (defaults to an instance of `BatchTokenizeTransform`)\n",
    "        batch_tokenize_tfm: Optional[BatchTokenizeTransform] = None,\n",
    "        # The batch_tfm you want to decode your inputs into a type that can be used in the fastai show methods,\n",
    "        # (defaults to BatchDecodeTransform)\n",
    "        batch_decode_tfm: Optional[BatchDecodeTransform] = None,\n",
    "        # To control the length of the padding/truncation for the input sequence. It can be an integer or None,\n",
    "        # in which case it will default to the maximum length the model can accept. If the model has no\n",
    "        # specific maximum input length, truncation/padding to max_length is deactivated.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        max_length: int = None,\n",
    "        # To control the length of the padding/truncation for the target sequence. It can be an integer or None,\n",
    "        # in which case it will default to the maximum length the model can accept. If the model has no\n",
    "        # specific maximum input length, truncation/padding to max_length is deactivated.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-y\n",
    "        max_target_length=None,\n",
    "        # To control the `padding` applied to your `hf_tokenizer` during tokenization. If None, will default to\n",
    "        # `False` or `'do_not_pad'.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        padding: Union[bool, str] = True,\n",
    "        # To control `truncation` applied to your `hf_tokenizer` during tokenization. If None, will default to\n",
    "        # `False` or `do_not_truncate`.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        truncation: Union[bool, str] = True,\n",
    "        # The return type your decoded inputs should be cast too (used by methods such as `show_batch`)\n",
    "        input_return_type=Seq2SeqTextInput,\n",
    "        # The type of `DataLoader` you want created (defaults to `SortedDL`)\n",
    "        dl_type=SortedDL,\n",
    "        # Any keyword arguments you want applied to your `batch_tokenize_tfm`\n",
    "        batch_tokenize_kwargs: dict = {},\n",
    "        # Any keyword arguments you want applied to your `batch_decode_tfm` (will be set as a fastai `batch_tfms`)\n",
    "        batch_decode_kwargs: dict = {},\n",
    "        # Any keyword arguments you want your Hugging Face tokenizer to use during tokenization\n",
    "        tok_kwargs={},\n",
    "        # Any keyword arguments you want to have applied with generating text\n",
    "        # (default: default_text_gen_kwargs)\n",
    "        text_gen_kwargs={},\n",
    "        # Any keyword arguments you want applied to `TextBlock`\n",
    "        **kwargs\n",
    "    ):\n",
    "        # we need to pass text_gen_kwargs into our Seq2SeqBatchTokenizeTransform (use default unless specified)\n",
    "        if len(text_gen_kwargs) == 0:\n",
    "            if hf_config is None:\n",
    "                hf_config = batch_tokenize_tfm.hf_config\n",
    "            if hf_model is None:\n",
    "                hf_model = batch_tokenize_tfm.hf_model\n",
    "            self.text_gen_kwargs = default_text_gen_kwargs(hf_config, hf_model)\n",
    "        else:\n",
    "            self.text_gen_kwargs = text_gen_kwargs.copy()\n",
    "\n",
    "        # construct our before_batch and after_batch tfms as usual\n",
    "        if batch_tokenize_tfm is None:\n",
    "            batch_tokenize_tfm = Seq2SeqBatchTokenizeTransform(\n",
    "                hf_arch,\n",
    "                hf_config,\n",
    "                hf_tokenizer,\n",
    "                hf_model,\n",
    "                max_length=max_length,\n",
    "                max_target_length=max_target_length,\n",
    "                padding=padding,\n",
    "                truncation=truncation,\n",
    "                tok_kwargs=tok_kwargs.copy(),\n",
    "                text_gen_kwargs=text_gen_kwargs,\n",
    "                **batch_tokenize_kwargs.copy()\n",
    "            )\n",
    "\n",
    "        if batch_decode_tfm is None:\n",
    "            hf_tokenizer = hf_tokenizer if (hf_tokenizer is not None) else batch_tokenize_tfm.hf_tokenizer\n",
    "            batch_decode_tfm = Seq2SeqBatchDecodeTransform(input_return_type, **batch_decode_kwargs.copy())\n",
    "\n",
    "        return super().__init__(\n",
    "            batch_tokenize_tfm=batch_tokenize_tfm,\n",
    "            batch_decode_tfm=batch_decode_tfm,\n",
    "            max_length=max_length,\n",
    "            padding=padding,\n",
    "            truncation=truncation,\n",
    "            is_split_into_words=False,\n",
    "            input_return_type=input_return_type,\n",
    "            dl_type=dl_type,\n",
    "            tok_kwargs=tok_kwargs,\n",
    "            before_batch_kwargs=batch_tokenize_kwargs,\n",
    "            after_batch_kwargs=batch_decode_kwargs,\n",
    "            **kwargs\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `show_batch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@typedispatch\n",
    "def show_batch(\n",
    "    # This typedispatched `show_batch` will be called for `Seq2SeqTextInput` typed inputs\n",
    "    x: Seq2SeqTextInput,\n",
    "    # Your targets\n",
    "    y,\n",
    "    # Your raw inputs/targets\n",
    "    samples,\n",
    "    # Your `DataLoaders`. This is required so as to get at the Hugging Face objects for\n",
    "    # decoding them into something understandable\n",
    "    dataloaders,\n",
    "    # Your `show_batch` context\n",
    "    ctxs=None,\n",
    "    # The maximum number of items to show\n",
    "    max_n=6,\n",
    "    # Any truncation your want applied to your decoded inputs\n",
    "    input_trunc_at=None,\n",
    "    # Any truncation your want applied to your decoded targets\n",
    "    target_trunc_at=None,\n",
    "    # Any other keyword arguments you want applied to `show_batch`\n",
    "    **kwargs\n",
    "):\n",
    "    # grab our tokenizer and ignore token to decode\n",
    "    tfm = first_blurr_tfm(dataloaders)\n",
    "    hf_tokenizer = tfm.hf_tokenizer\n",
    "    ignore_token_id = tfm.ignore_token_id\n",
    "\n",
    "    res = L(\n",
    "        [\n",
    "            (\n",
    "                hf_tokenizer.decode(s[0], skip_special_tokens=False)[:input_trunc_at],\n",
    "                hf_tokenizer.decode(s[1][s[1] != ignore_token_id], skip_special_tokens=True)[:target_trunc_at],\n",
    "            )\n",
    "            for s in samples\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    display_df(pd.DataFrame(res, columns=[\"text\", \"target\"])[:max_n])\n",
    "    return ctxs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_callbacks.ipynb.\n",
      "Converted 00_utils.ipynb.\n",
      "Converted 01_text-callbacks.ipynb.\n",
      "Converted 01_text-utils.ipynb.\n",
      "Converted 11_text-data-core.ipynb.\n",
      "Converted 11_text-modeling-core.ipynb.\n",
      "Converted 12_text-data-language-modeling.ipynb.\n",
      "Converted 12_text-modeling-language-modeling.ipynb.\n",
      "Converted 13_text-data-token-classification.ipynb.\n",
      "Converted 13_text-modeling-token-classification.ipynb.\n",
      "Converted 14_text-data-question-answering.ipynb.\n",
      "Converted 14_text-modeling-question-answering.ipynb.\n",
      "Converted 20_text-data-seq2seq-core.ipynb.\n",
      "Converted 20_text-modeling-seq2seq-core.ipynb.\n",
      "Converted 21_text-data-seq2seq-summarization.ipynb.\n",
      "Converted 21_text-modeling-seq2seq-summarization.ipynb.\n",
      "Converted 22_text-data-seq2seq-translation.ipynb.\n",
      "Converted 22_text-modeling-seq2seq-translation.ipynb.\n",
      "Converted 99a_text-examples-high-level-api.ipynb.\n",
      "Converted 99b_text-examples-glue.ipynb.\n",
      "Converted 99c_text-examples-glue-plain-pytorch.ipynb.\n",
      "Converted 99d_text-examples-multilabel.ipynb.\n",
      "Converted 99e_text-examples-causal-lm-gpt2.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "nbdev_export()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('blurr')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

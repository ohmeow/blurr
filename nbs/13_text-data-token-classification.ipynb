{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp text.data.token_classification\n",
    "# |default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | nbflags skip_exec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "> The `text.data.token_classification` module contains the bits required to use the fastai DataBlock API and/or mid-level data processing pipelines to organize your data for token classification tasks (e.g., Named entity recognition (NER), Part-of-speech tagging (POS), etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import ast, os, warnings\n",
    "from typing import Callable, List, Tuple\n",
    "\n",
    "from datasets import Dataset\n",
    "from fastcore.all import *\n",
    "from fastai.data.block import TransformBlock, Category, CategoryMap\n",
    "from fastai.imports import *\n",
    "from fastai.losses import CrossEntropyLossFlat\n",
    "from fastai.torch_core import *\n",
    "from fastai.torch_imports import *\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    PretrainedConfig,\n",
    "    PreTrainedTokenizerBase,\n",
    "    PreTrainedModel,\n",
    ")\n",
    "from transformers.utils import logging as hf_logging\n",
    "\n",
    "from blurr.text.data.core import (\n",
    "    Preprocessor,\n",
    "    TextInput,\n",
    "    BatchTokenizeTransform,\n",
    "    first_blurr_tfm,\n",
    ")\n",
    "from blurr.text.utils import get_hf_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we're running with at the time this documentation was generated:\n",
      "torch: 1.9.0+cu102\n",
      "fastai: 2.7.9\n",
      "transformers: 4.21.2\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "import pdb\n",
    "\n",
    "from datasets import load_dataset\n",
    "from fastai.data.block import DataBlock, ColReader, ColSplitter\n",
    "from fastai.data.core import DataLoader, DataLoaders, TfmdDL\n",
    "from fastai.data.external import untar_data, URLs\n",
    "from fastai.data.transforms import *\n",
    "from fastcore.test import *\n",
    "from nbdev import nbdev_export\n",
    "from nbdev.showdoc import show_doc\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from blurr.utils import print_versions\n",
    "from blurr.text.data.core import TextBlock\n",
    "from blurr.text.utils import BlurrText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "# silence all the HF warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "hf_logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "NLP = BlurrText()\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(\"What we're running with at the time this documentation was generated:\")\n",
    "print_versions(\"torch fastai transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "# |hide\n",
    "# |cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f\"Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll use a subset of `conll2003` to demonstrate how to configure your blurr code for token classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/home/wgilliam/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.021427631378173828,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "055eed9a2dc04a23932e9cfd3e7b6eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"conll2003\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get a list of the distinct entities we want to predict. If they are represented as list in their raw/readable form in another attribute/column in our dataset, we could use something like this to build a sorted list of distinct values as such: `labels = sorted(list(set([lbls for sublist in germ_eval_df.labels.tolist() for lbls in sublist])))`.\n",
    "\n",
    "Fortunately, the `conll2003` dataset allows us to get at this list directly using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP']\n",
      "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
      "['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS']\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets[\"train\"].features[\"chunk_tags\"].feature.names[:20])\n",
    "print(raw_datasets[\"train\"].features[\"ner_tags\"].feature.names[:20])\n",
    "print(raw_datasets[\"train\"].features[\"pos_tags\"].feature.names[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
      "[3, 0, 7, 0, 0, 0, 7, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# |hide\n",
    "print(raw_datasets[\"train\"][0][\"tokens\"])\n",
    "print(raw_datasets[\"train\"][0][\"ner_tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll2003_df = pd.DataFrame(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('roberta',\n",
       " transformers.models.roberta.configuration_roberta.RobertaConfig,\n",
       " transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast,\n",
       " transformers.models.roberta.modeling_roberta.RobertaForTokenClassification)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cls = AutoModelForTokenClassification\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "pretrained_model_name = \"roberta-base\"  # \"bert-base-multilingual-cased\"\n",
    "n_labels = len(labels)\n",
    "\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(\n",
    "    pretrained_model_name, model_cls=model_cls, config_kwargs={\"num_labels\": n_labels}\n",
    ")\n",
    "\n",
    "hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Starting with version 2.0, `BLURR` provides a token classification preprocessing class that can be used to preprocess DataFrames or Hugging Face Datasets. We also introduce a novel way of handling long documents for this task that ensures tokens associated to a word is not split up in \"chunked\" documents.  See below for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class TokenClassPreprocessor(Preprocessor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # Set to `True` if the preprocessor should chunk examples that exceed `max_length`\n",
    "        chunk_examples: bool = False,\n",
    "        # Like \"stride\" except for words (not tokens)\n",
    "        word_stride: int = 2,\n",
    "        # The token ID that should be ignored when calculating the loss\n",
    "        ignore_token_id: int = CrossEntropyLossFlat().ignore_index,\n",
    "        # The label names (if not specified, will build from DataFrame)\n",
    "        label_names: Optional[List[str]] = None,\n",
    "        # The number of examples to process at a time\n",
    "        batch_size: int = 1000,\n",
    "        # The unique identifier in the dataset\n",
    "        id_attr: Optional[str] = None,\n",
    "        # The attribute holding the list of words\n",
    "        word_list_attr: str = \"tokens\",\n",
    "        # The attribute holding the list of labels (one for each word in `word_list_attr`)\n",
    "        label_list_attr: str = \"labels\",\n",
    "        # The attribute that should be created if your are processing individual training and validation\n",
    "        # datasets into a single dataset, and will indicate to which each example is associated\n",
    "        is_valid_attr: Optional[str] = \"is_valid\",\n",
    "        # If using a slow tokenizer, users will need to prove a `slow_word_ids_func` that accepts a\n",
    "        # tokenizzer, example index, and a batch encoding as arguments and in turn returnes the\n",
    "        # equavlient of fast tokenizer's `word_ids`\n",
    "        slow_word_ids_func: Optional[Callable] = None,\n",
    "        # Tokenization kwargs that will be applied with calling the tokenizer\n",
    "        tok_kwargs: dict = {},\n",
    "    ):\n",
    "        # tokenizer requires this kwargs when tokenizing text\n",
    "        tok_kwargs = {**tok_kwargs, **{\"is_split_into_words\": True}}\n",
    "\n",
    "        super().__init__(\n",
    "            hf_tokenizer, batch_size, text_attr=word_list_attr, tok_kwargs=tok_kwargs\n",
    "        )\n",
    "\n",
    "        self.id_attr = id_attr\n",
    "        self.label_list_attr = label_list_attr\n",
    "        self.is_valid_attr = is_valid_attr\n",
    "        self.label_names = label_names\n",
    "        self.chunk_examples, self.word_stride = chunk_examples, word_stride\n",
    "\n",
    "        self.slow_word_ids_func = slow_word_ids_func\n",
    "\n",
    "    def process_df(\n",
    "        self, training_df: pd.DataFrame, validation_df: Optional[pd.DataFrame] = None\n",
    "    ):\n",
    "        df = super().process_df(training_df, validation_df)\n",
    "\n",
    "        # convert even single \"labels\" to a list to make things easier\n",
    "        if self.label_names is None:\n",
    "            self.label_names = sorted(\n",
    "                list(\n",
    "                    set(\n",
    "                        [\n",
    "                            lbls\n",
    "                            for sublist in df[self.label_list_attr].tolist()\n",
    "                            for lbls in sublist\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if self.chunk_examples:\n",
    "            # \"pop\" off the max_length so we can manually chunk long documents\n",
    "            max_length = self.tok_kwargs.pop(\n",
    "                \"max_length\", self.hf_tokenizer.model_max_length\n",
    "            )\n",
    "            # a unique Id for each example is required to properly score question answering results when chunking long docs\n",
    "            if self.id_attr is None:\n",
    "                df.insert(0, \"_id\", range(len(df)))\n",
    "        else:\n",
    "            # if we're not chunking, just \"get\" the max_length\n",
    "            max_length = self.tok_kwargs.get(\n",
    "                \"max_length\", self.hf_tokenizer.model_max_length\n",
    "            )\n",
    "\n",
    "        # process df in mini-batches\n",
    "        final_df = pd.DataFrame()\n",
    "        for g, batch_df in df.groupby(np.arange(len(df)) // self.batch_size):\n",
    "            final_df = final_df.append(\n",
    "                self._process_df_batch(batch_df, self.chunk_examples, max_length)\n",
    "            )\n",
    "\n",
    "        final_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # move the processed bits up to the front\n",
    "        col = final_df.pop(f\"proc_{self.text_attr}\")\n",
    "        final_df.insert(0, col.name, col)\n",
    "        col = final_df.pop(f\"proc_{self.label_list_attr}\")\n",
    "        final_df.insert(1, col.name, col)\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def process_hf_dataset(\n",
    "        self, training_ds: Dataset, validation_ds: Optional[Dataset] = None\n",
    "    ):\n",
    "        ds = super().process_hf_dataset(training_ds, validation_ds)\n",
    "        return Dataset.from_pandas(self.process_df(pd.DataFrame(ds)))\n",
    "\n",
    "    # ----- utility methods -----\n",
    "    def _process_df_batch(self, batch_df, is_chunked, max_length):\n",
    "        batch_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # grab our inputs\n",
    "        if not is_chunked:\n",
    "            # token classification works with lists of words, so if not listy we resort to splitting by spaces\n",
    "            batch_df[self.text_attr] = batch_df[self.text_attr].apply(\n",
    "                lambda v: v if is_listy(v) else v.split()\n",
    "            )\n",
    "            inputs = self._tokenize_function(batch_df.to_dict(orient=\"list\"))\n",
    "\n",
    "            proc_toks, proc_labels = [], []\n",
    "            for idx in range(len(inputs[\"input_ids\"])):\n",
    "                word_ids = (\n",
    "                    inputs.word_ids(idx)\n",
    "                    if self.hf_tokenizer.is_fast\n",
    "                    else self.slow_word_ids_func(self.hf_tokenizer, idx, inputs)\n",
    "                )\n",
    "                non_special_word_ids = set(\n",
    "                    [word_id for word_id in word_ids if word_id is not None]\n",
    "                )\n",
    "                proc_toks.append(\n",
    "                    [\n",
    "                        batch_df.iloc[idx][self.text_attr][word_id]\n",
    "                        for word_id in non_special_word_ids\n",
    "                    ]\n",
    "                )\n",
    "                proc_labels.append(\n",
    "                    [\n",
    "                        batch_df.iloc[idx][self.label_list_attr][word_id]\n",
    "                        for word_id in non_special_word_ids\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "            batch_df[f\"proc_{self.text_attr}\"] = pd.Series(proc_toks)\n",
    "            batch_df[f\"proc_{self.label_list_attr}\"] = pd.Series(proc_labels)\n",
    "            return batch_df\n",
    "\n",
    "        # if we get here, we need create \"chunked\" inputs/labels from the existing input/label ensuring that\n",
    "        # words are *not* broken up between chunks\n",
    "        proc_data = []\n",
    "        for row_idx, row in batch_df.iterrows():\n",
    "            # fetch word list and words' label list (there should be 1 label per word)\n",
    "            words = (\n",
    "                row[self.text_attr]\n",
    "                if is_listy(row[self.text_attr])\n",
    "                else row[self.text_attr].split()\n",
    "            )\n",
    "            word_labels = row[self.label_list_attr]\n",
    "\n",
    "            inputs = hf_tokenizer(words, **self.tok_kwargs)\n",
    "            word_ids = (\n",
    "                inputs.word_ids()\n",
    "                if self.hf_tokenizer.is_fast\n",
    "                else self.slow_word_ids_func(self.hf_tokenizer, 0, inputs)\n",
    "            )\n",
    "\n",
    "            non_special_word_ids = [id for id in word_ids if id is not None]\n",
    "            max_chunk_length = (\n",
    "                max_length - self.hf_tokenizer.num_special_tokens_to_add()\n",
    "            )\n",
    "\n",
    "            start_idx, current_word_id, current_chunk_length = 0, 0, 0\n",
    "            chunks = []\n",
    "            while True:\n",
    "                last_idx = (\n",
    "                    len(non_special_word_ids)\n",
    "                    - 1\n",
    "                    - non_special_word_ids[::-1].index(current_word_id)\n",
    "                )\n",
    "                current_chunk_length = len(\n",
    "                    non_special_word_ids[start_idx : last_idx + 1]\n",
    "                )\n",
    "\n",
    "                if current_chunk_length >= max_chunk_length:\n",
    "                    # we need to add a chunk\n",
    "                    if current_chunk_length > max_chunk_length:\n",
    "                        # only when the current chunk in > the max chunk length do we want to modify the \"last_indx\" (if\n",
    "                        # equal then we want to use the current value)\n",
    "                        last_idx = (\n",
    "                            len(non_special_word_ids)\n",
    "                            - 1\n",
    "                            - non_special_word_ids[::-1].index(\n",
    "                                max(0, current_word_id - 1)\n",
    "                            )\n",
    "                        )\n",
    "                    chunks.append(non_special_word_ids[start_idx : last_idx + 1])\n",
    "\n",
    "                    # start a new chunk\n",
    "                    current_chunk_length = 0\n",
    "\n",
    "                    if (\n",
    "                        self.word_stride == 0\n",
    "                        or non_special_word_ids.index(\n",
    "                            max(0, current_word_id - self.word_stride)\n",
    "                        )\n",
    "                        <= start_idx\n",
    "                    ):\n",
    "                        # if \"word_stride\" = 0 or going back \"word_stride\" would lead to infinite recurssion because it would go\n",
    "                        # back beyond the start of the last chunk, we don't \"word_stride\" ... we just move to next token\n",
    "                        start_idx = last_idx + 1\n",
    "                    else:\n",
    "                        current_word_id -= self.word_stride - 1\n",
    "                        start_idx = non_special_word_ids.index(current_word_id)\n",
    "\n",
    "                current_word_id += 1\n",
    "\n",
    "                if current_word_id >= max(non_special_word_ids):\n",
    "                    # add any inprogress chunk\n",
    "                    if current_chunk_length > 0:\n",
    "                        chunks.append(non_special_word_ids[start_idx:])\n",
    "                    break\n",
    "\n",
    "            for chunk in chunks:\n",
    "                overflow_row = row.copy()\n",
    "                overflow_row[f\"proc_{self.text_attr}\"] = [\n",
    "                    words[word_id] for word_id in list(set(chunk))\n",
    "                ]\n",
    "                overflow_row[f\"proc_{self.label_list_attr}\"] = [\n",
    "                    word_labels[word_id] for word_id in list(set(chunk))\n",
    "                ]\n",
    "                proc_data.append(overflow_row)\n",
    "\n",
    "        return pd.DataFrame(proc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### labels are Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61298\n",
      "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proc_tokens</th>\n",
       "      <th>proc_ner_tags</th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>chunk_tags</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[EU, rejects, German, call, to, boycott]</td>\n",
       "      <td>[3, 0, 7, 0, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>[EU, rejects, German, call, to, boycott, British, lamb, .]</td>\n",
       "      <td>[22, 42, 16, 21, 35, 37, 16, 21, 7]</td>\n",
       "      <td>[11, 21, 11, 12, 21, 22, 11, 12, 0]</td>\n",
       "      <td>[3, 0, 7, 0, 0, 0, 7, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[to, boycott, British, lamb, .]</td>\n",
       "      <td>[0, 0, 7, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>[EU, rejects, German, call, to, boycott, British, lamb, .]</td>\n",
       "      <td>[22, 42, 16, 21, 35, 37, 16, 21, 7]</td>\n",
       "      <td>[11, 21, 11, 12, 21, 22, 11, 12, 0]</td>\n",
       "      <td>[3, 0, 7, 0, 0, 0, 7, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Peter, Blackburn]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>1</td>\n",
       "      <td>[Peter, Blackburn]</td>\n",
       "      <td>[22, 22]</td>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>[1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[BRUSSELS, 1996-08-22]</td>\n",
       "      <td>[5, 0]</td>\n",
       "      <td>2</td>\n",
       "      <td>[BRUSSELS, 1996-08-22]</td>\n",
       "      <td>[22, 11]</td>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>[5, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                proc_tokens       proc_ner_tags id  \\\n",
       "0  [EU, rejects, German, call, to, boycott]  [3, 0, 7, 0, 0, 0]  0   \n",
       "1           [to, boycott, British, lamb, .]     [0, 0, 7, 0, 0]  0   \n",
       "2                        [Peter, Blackburn]              [1, 2]  1   \n",
       "3                    [BRUSSELS, 1996-08-22]              [5, 0]  2   \n",
       "\n",
       "                                                       tokens  \\\n",
       "0  [EU, rejects, German, call, to, boycott, British, lamb, .]   \n",
       "1  [EU, rejects, German, call, to, boycott, British, lamb, .]   \n",
       "2                                          [Peter, Blackburn]   \n",
       "3                                      [BRUSSELS, 1996-08-22]   \n",
       "\n",
       "                              pos_tags                           chunk_tags  \\\n",
       "0  [22, 42, 16, 21, 35, 37, 16, 21, 7]  [11, 21, 11, 12, 21, 22, 11, 12, 0]   \n",
       "1  [22, 42, 16, 21, 35, 37, 16, 21, 7]  [11, 21, 11, 12, 21, 22, 11, 12, 0]   \n",
       "2                             [22, 22]                             [11, 12]   \n",
       "3                             [22, 11]                             [11, 12]   \n",
       "\n",
       "                      ner_tags  \n",
       "0  [3, 0, 7, 0, 0, 0, 7, 0, 0]  \n",
       "1  [3, 0, 7, 0, 0, 0, 7, 0, 0]  \n",
       "2                       [1, 2]  \n",
       "3                       [5, 0]  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = TokenClassPreprocessor(\n",
    "    hf_tokenizer,\n",
    "    chunk_examples=True,\n",
    "    word_stride=2,\n",
    "    label_names=labels,\n",
    "    id_attr=\"id\",\n",
    "    word_list_attr=\"tokens\",\n",
    "    label_list_attr=\"ner_tags\",\n",
    "    tok_kwargs={\"max_length\": 8},\n",
    ")\n",
    "proc_df = preprocessor.process_df(conll2003_df)\n",
    "\n",
    "print(len(proc_df))\n",
    "print(preprocessor.label_names)\n",
    "proc_df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### labels are entity names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>chunk_tags</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[EU, rejects, German, call, to, boycott, British, lamb, .]</td>\n",
       "      <td>[22, 42, 16, 21, 35, 37, 16, 21, 7]</td>\n",
       "      <td>[11, 21, 11, 12, 21, 22, 11, 12, 0]</td>\n",
       "      <td>[B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Peter, Blackburn]</td>\n",
       "      <td>[22, 22]</td>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>[B-PER, I-PER]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[BRUSSELS, 1996-08-22]</td>\n",
       "      <td>[22, 11]</td>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>[B-LOC, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[The, European, Commission, said, on, Thursday, it, disagreed, with, German, advice, to, consumers, to, shun, British, lamb, until, scientists, determine, whether, mad, cow, disease, can, be, transmitted, to, sheep, .]</td>\n",
       "      <td>[12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 35, 24, 35, 37, 16, 21, 15, 24, 41, 15, 16, 21, 21, 20, 37, 40, 35, 21, 7]</td>\n",
       "      <td>[11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 13, 11, 21, 22, 11, 12, 17, 11, 21, 17, 11, 12, 12, 21, 22, 22, 13, 11, 0]</td>\n",
       "      <td>[O, B-ORG, I-ORG, O, O, O, O, O, O, B-MISC, O, O, O, O, O, B-MISC, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[Germany, 's, representative, to, the, European, Union, 's, veterinary, committee, Werner, Zwingmann, said, on, Wednesday, consumers, should, buy, sheepmeat, from, countries, other, than, Britain, until, the, scientific, advice, was, clearer, .]</td>\n",
       "      <td>[22, 27, 21, 35, 12, 22, 22, 27, 16, 21, 22, 22, 38, 15, 22, 24, 20, 37, 21, 15, 24, 16, 15, 22, 15, 12, 16, 21, 38, 17, 7]</td>\n",
       "      <td>[11, 11, 12, 13, 11, 12, 12, 11, 12, 12, 12, 12, 21, 13, 11, 12, 21, 22, 11, 13, 11, 1, 13, 11, 17, 11, 12, 12, 21, 1, 0]</td>\n",
       "      <td>[B-LOC, O, O, O, O, B-ORG, I-ORG, O, O, O, B-PER, I-PER, O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id  \\\n",
       "0  0   \n",
       "1  1   \n",
       "2  2   \n",
       "3  3   \n",
       "4  4   \n",
       "\n",
       "                                                                                                                                                                                                                                                  tokens  \\\n",
       "0                                                                                                                                                                                             [EU, rejects, German, call, to, boycott, British, lamb, .]   \n",
       "1                                                                                                                                                                                                                                     [Peter, Blackburn]   \n",
       "2                                                                                                                                                                                                                                 [BRUSSELS, 1996-08-22]   \n",
       "3                             [The, European, Commission, said, on, Thursday, it, disagreed, with, German, advice, to, consumers, to, shun, British, lamb, until, scientists, determine, whether, mad, cow, disease, can, be, transmitted, to, sheep, .]   \n",
       "4  [Germany, 's, representative, to, the, European, Union, 's, veterinary, committee, Werner, Zwingmann, said, on, Wednesday, consumers, should, buy, sheepmeat, from, countries, other, than, Britain, until, the, scientific, advice, was, clearer, .]   \n",
       "\n",
       "                                                                                                                      pos_tags  \\\n",
       "0                                                                                          [22, 42, 16, 21, 35, 37, 16, 21, 7]   \n",
       "1                                                                                                                     [22, 22]   \n",
       "2                                                                                                                     [22, 11]   \n",
       "3      [12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 35, 24, 35, 37, 16, 21, 15, 24, 41, 15, 16, 21, 21, 20, 37, 40, 35, 21, 7]   \n",
       "4  [22, 27, 21, 35, 12, 22, 22, 27, 16, 21, 22, 22, 38, 15, 22, 24, 20, 37, 21, 15, 24, 16, 15, 22, 15, 12, 16, 21, 38, 17, 7]   \n",
       "\n",
       "                                                                                                                  chunk_tags  \\\n",
       "0                                                                                        [11, 21, 11, 12, 21, 22, 11, 12, 0]   \n",
       "1                                                                                                                   [11, 12]   \n",
       "2                                                                                                                   [11, 12]   \n",
       "3    [11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 13, 11, 21, 22, 11, 12, 17, 11, 21, 17, 11, 12, 12, 21, 22, 22, 13, 11, 0]   \n",
       "4  [11, 11, 12, 13, 11, 12, 12, 11, 12, 12, 12, 12, 21, 13, 11, 12, 21, 22, 11, 13, 11, 1, 13, 11, 17, 11, 12, 12, 21, 1, 0]   \n",
       "\n",
       "                                                                                                                ner_tags  \n",
       "0                                                                              [B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]  \n",
       "1                                                                                                         [B-PER, I-PER]  \n",
       "2                                                                                                             [B-LOC, O]  \n",
       "3           [O, B-ORG, I-ORG, O, O, O, O, O, O, B-MISC, O, O, O, O, O, B-MISC, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "4  [B-LOC, O, O, O, O, B-ORG, I-ORG, O, O, O, B-PER, I-PER, O, O, O, O, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O]  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll2003_labeled_df = conll2003_df.copy()\n",
    "conll2003_labeled_df.ner_tags = conll2003_labeled_df.ner_tags.apply(\n",
    "    lambda v: [labels[lbl_id] for lbl_id in v]\n",
    ")\n",
    "conll2003_labeled_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14041\n",
      "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proc_tokens</th>\n",
       "      <th>proc_ner_tags</th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>chunk_tags</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[EU, rejects, German, call, to, boycott]</td>\n",
       "      <td>[B-ORG, O, B-MISC, O, O, O]</td>\n",
       "      <td>0</td>\n",
       "      <td>[EU, rejects, German, call, to, boycott, British, lamb, .]</td>\n",
       "      <td>[22, 42, 16, 21, 35, 37, 16, 21, 7]</td>\n",
       "      <td>[11, 21, 11, 12, 21, 22, 11, 12, 0]</td>\n",
       "      <td>[B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Peter, Blackburn]</td>\n",
       "      <td>[B-PER, I-PER]</td>\n",
       "      <td>1</td>\n",
       "      <td>[Peter, Blackburn]</td>\n",
       "      <td>[22, 22]</td>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>[B-PER, I-PER]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[BRUSSELS, 1996-08-22]</td>\n",
       "      <td>[B-LOC, O]</td>\n",
       "      <td>2</td>\n",
       "      <td>[BRUSSELS, 1996-08-22]</td>\n",
       "      <td>[22, 11]</td>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>[B-LOC, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The, European, Commission, said, on, Thursday]</td>\n",
       "      <td>[O, B-ORG, I-ORG, O, O, O]</td>\n",
       "      <td>3</td>\n",
       "      <td>[The, European, Commission, said, on, Thursday, it, disagreed, with, German, advice, to, consumers, to, shun, British, lamb, until, scientists, determine, whether, mad, cow, disease, can, be, transmitted, to, sheep, .]</td>\n",
       "      <td>[12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 35, 24, 35, 37, 16, 21, 15, 24, 41, 15, 16, 21, 21, 20, 37, 40, 35, 21, 7]</td>\n",
       "      <td>[11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 13, 11, 21, 22, 11, 12, 17, 11, 21, 17, 11, 12, 12, 21, 22, 22, 13, 11, 0]</td>\n",
       "      <td>[O, B-ORG, I-ORG, O, O, O, O, O, O, B-MISC, O, O, O, O, O, B-MISC, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       proc_tokens  \\\n",
       "0         [EU, rejects, German, call, to, boycott]   \n",
       "1                               [Peter, Blackburn]   \n",
       "2                           [BRUSSELS, 1996-08-22]   \n",
       "3  [The, European, Commission, said, on, Thursday]   \n",
       "\n",
       "                 proc_ner_tags id  \\\n",
       "0  [B-ORG, O, B-MISC, O, O, O]  0   \n",
       "1               [B-PER, I-PER]  1   \n",
       "2                   [B-LOC, O]  2   \n",
       "3   [O, B-ORG, I-ORG, O, O, O]  3   \n",
       "\n",
       "                                                                                                                                                                                                                       tokens  \\\n",
       "0                                                                                                                                                                  [EU, rejects, German, call, to, boycott, British, lamb, .]   \n",
       "1                                                                                                                                                                                                          [Peter, Blackburn]   \n",
       "2                                                                                                                                                                                                      [BRUSSELS, 1996-08-22]   \n",
       "3  [The, European, Commission, said, on, Thursday, it, disagreed, with, German, advice, to, consumers, to, shun, British, lamb, until, scientists, determine, whether, mad, cow, disease, can, be, transmitted, to, sheep, .]   \n",
       "\n",
       "                                                                                                                  pos_tags  \\\n",
       "0                                                                                      [22, 42, 16, 21, 35, 37, 16, 21, 7]   \n",
       "1                                                                                                                 [22, 22]   \n",
       "2                                                                                                                 [22, 11]   \n",
       "3  [12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 35, 24, 35, 37, 16, 21, 15, 24, 41, 15, 16, 21, 21, 20, 37, 40, 35, 21, 7]   \n",
       "\n",
       "                                                                                                                chunk_tags  \\\n",
       "0                                                                                      [11, 21, 11, 12, 21, 22, 11, 12, 0]   \n",
       "1                                                                                                                 [11, 12]   \n",
       "2                                                                                                                 [11, 12]   \n",
       "3  [11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 13, 11, 21, 22, 11, 12, 17, 11, 21, 17, 11, 12, 12, 21, 22, 22, 13, 11, 0]   \n",
       "\n",
       "                                                                                                       ner_tags  \n",
       "0                                                                     [B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]  \n",
       "1                                                                                                [B-PER, I-PER]  \n",
       "2                                                                                                    [B-LOC, O]  \n",
       "3  [O, B-ORG, I-ORG, O, O, O, O, O, O, B-MISC, O, O, O, O, O, B-MISC, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = TokenClassPreprocessor(\n",
    "    hf_tokenizer,\n",
    "    label_names=labels,\n",
    "    id_attr=\"id\",\n",
    "    word_list_attr=\"tokens\",\n",
    "    label_list_attr=\"ner_tags\",\n",
    "    tok_kwargs={\"max_length\": 8},\n",
    ")\n",
    "proc_df = preprocessor.process_df(conll2003_labeled_df)\n",
    "\n",
    "print(len(proc_df))\n",
    "print(preprocessor.label_names)\n",
    "proc_df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `BaseLabelingStrategy` and implementations -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class BaseLabelingStrategy:\n",
    "    def __init__(\n",
    "        self,\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        label_names: Optional[List[str]],\n",
    "        non_entity_label: str = \"O\",\n",
    "        ignore_token_id: int = CrossEntropyLossFlat().ignore_index,\n",
    "    ) -> None:\n",
    "        self.hf_tokenizer = hf_tokenizer\n",
    "        self.ignore_token_id = ignore_token_id\n",
    "        self.label_names = label_names\n",
    "        self.non_entity_label = non_entity_label\n",
    "\n",
    "    def align_labels_with_tokens(self, word_ids, word_labels):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we include a `BaseLabelingStrategy` abstract class and several different strategies for assigning labels to your tokenized inputs. The \"only first token\" and \"B/I\" labeling strategies are discussed in the [\"Token Classification\"](https://huggingface.co/course/chapter7/2?fw=pt) section in part 7 of the Hugging Face's Transformers course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class OnlyFirstTokenLabelingStrategy(BaseLabelingStrategy):\n",
    "    \"\"\"\n",
    "    Only the first token of word is associated with the label (all other subtokens with the `ignore_index_id`). Works where labels\n",
    "    are Ids or strings (in the later case we'll use the `label_names` to look up it's Id)\n",
    "    \"\"\"\n",
    "\n",
    "    def align_labels_with_tokens(self, word_ids, word_labels):\n",
    "        new_labels = []\n",
    "        current_word = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id != current_word:\n",
    "                # start of a new word\n",
    "                current_word = word_id\n",
    "                label = (\n",
    "                    self.ignore_token_id if word_id is None else word_labels[word_id]\n",
    "                )\n",
    "                new_labels.append(\n",
    "                    label if isinstance(label, int) else self.label_names.index(label)\n",
    "                )\n",
    "            else:\n",
    "                # special token or another subtoken of current word\n",
    "                new_labels.append(self.ignore_token_id)\n",
    "\n",
    "        return new_labels\n",
    "\n",
    "\n",
    "class SameLabelLabelingStrategy(BaseLabelingStrategy):\n",
    "    \"\"\"\n",
    "    Every token associated with a given word is associated with the word's label. Works where labels\n",
    "    are Ids or strings (in the later case we'll use the `label_names` to look up it's Id)\n",
    "    \"\"\"\n",
    "\n",
    "    def align_labels_with_tokens(self, word_ids, word_labels):\n",
    "        new_labels = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id == None:\n",
    "                new_labels.append(self.ignore_token_id)\n",
    "            else:\n",
    "                label = word_labels[word_id]\n",
    "                new_labels.append(\n",
    "                    label if isinstance(label, int) else self.label_names.index(label)\n",
    "                )\n",
    "\n",
    "        return new_labels\n",
    "\n",
    "\n",
    "class BILabelingStrategy(BaseLabelingStrategy):\n",
    "    \"\"\"\n",
    "    If using B/I labels, the first token assoicated to a given word gets the \"B\" label while all other tokens related\n",
    "    to that same word get \"I\" labels.  If \"I\" labels don't exist, this strategy behaves like the `OnlyFirstTokenLabelingStrategy`.\n",
    "    Works where labels are Ids or strings (in the later case we'll use the `label_names` to look up it's Id)\n",
    "    \"\"\"\n",
    "\n",
    "    def align_labels_with_tokens(self, word_ids, word_labels):\n",
    "        new_labels = []\n",
    "        current_word = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id != current_word:\n",
    "                # start of a new word\n",
    "                current_word = word_id\n",
    "                label = (\n",
    "                    self.ignore_token_id if word_id is None else word_labels[word_id]\n",
    "                )\n",
    "                new_labels.append(\n",
    "                    label if isinstance(label, int) else self.label_names.index(label)\n",
    "                )\n",
    "            elif word_id is None:\n",
    "                # special token\n",
    "                new_labels.append(self.ignore_token_id)\n",
    "            else:\n",
    "                # we're in the same word\n",
    "                label = word_labels[word_id]\n",
    "                label_name = (\n",
    "                    self.label_names[label] if isinstance(label, int) else label\n",
    "                )\n",
    "\n",
    "                # append the I-{ENTITY} if it exists in `labels`, else default to the `same_label` strategy\n",
    "                iLabel = f\"I-{label_name[2:]}\"\n",
    "                new_labels.append(\n",
    "                    self.label_names.index(iLabel)\n",
    "                    if iLabel in self.label_names\n",
    "                    else self.label_names.index(self.non_entity_label)\n",
    "                )\n",
    "\n",
    "        return new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstructing inputs/labels\n",
    "\n",
    "The utility methods below allow blurr users to reconstruct the original word/label associations from the input_ids/label associations.  For example, these are used in our token classification `show_batch` method below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def get_token_labels_from_input_ids(\n",
    "    # A Hugging Face tokenizer\n",
    "    hf_tokenizer: PreTrainedTokenizerBase,\n",
    "    # List of input_ids for the tokens in a single piece of processed text\n",
    "    input_ids: List[int],\n",
    "    # List of label indexs for each token\n",
    "    token_label_ids: List[int],\n",
    "    # List of label names from witch the `label` indicies can be used to find the name of the label\n",
    "    vocab: List[str],\n",
    "    # The token ID that should be ignored when calculating the loss\n",
    "    ignore_token_id: int = CrossEntropyLossFlat().ignore_index,\n",
    "    # The token used to identifiy ignored tokens (default: [xIGNx])\n",
    "    ignore_token: str = \"[xIGNx]\",\n",
    ") -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Given a list of input IDs, the label ID associated to each, and the labels vocab, this method will return a list of tuples whereby\n",
    "    each tuple defines the \"token\" and its label name. For example:\n",
    "    [('Way', B-PER), ('de', B-PER), ('Gill', I-PER), ('iam', I-PER), ('loves'), ('Hug', B-ORG), ('ging', B-ORG), ('Face', I-ORG)]\n",
    "    \"\"\"\n",
    "    # convert ids to tokens\n",
    "    toks = hf_tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    # align \"tokens\" with labels\n",
    "    tok_labels = [\n",
    "        (tok, ignore_token if label_id == ignore_token_id else vocab[label_id])\n",
    "        for tok_id, tok, label_id in zip(input_ids, toks, token_label_ids)\n",
    "        if tok_id not in hf_tokenizer.all_special_ids\n",
    "    ]\n",
    "    return tok_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTS for align_labels_with_tokens()\n",
    "for idx in range(3):\n",
    "    raw_word_list = conll2003_df.iloc[idx][\"tokens\"]\n",
    "    raw_label_list = conll2003_df.iloc[idx][\"ner_tags\"]\n",
    "\n",
    "    be = hf_tokenizer(raw_word_list, is_split_into_words=True)\n",
    "    input_ids = be[\"input_ids\"]\n",
    "    targ_ids = [\n",
    "        -100 if (word_id == None) else raw_label_list[word_id]\n",
    "        for word_id in be.word_ids()\n",
    "    ]\n",
    "\n",
    "    tok_labels = get_token_labels_from_input_ids(\n",
    "        hf_tokenizer, input_ids, targ_ids, labels\n",
    "    )\n",
    "\n",
    "    for tok_label, targ_id in zip(\n",
    "        tok_labels, [label_id for label_id in targ_ids if label_id != -100]\n",
    "    ):\n",
    "        test_eq(tok_label[1], labels[targ_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### get_token_labels_from_input_ids\n",
       "\n",
       ">      get_token_labels_from_input_ids (hf_tokenizer:transformers.tokenization_u\n",
       ">                                       tils_base.PreTrainedTokenizerBase,\n",
       ">                                       input_ids:List[int],\n",
       ">                                       token_label_ids:List[int],\n",
       ">                                       vocab:List[str],\n",
       ">                                       ignore_token_id:int=-100,\n",
       ">                                       ignore_token:str='[xIGNx]')\n",
       "\n",
       "Given a list of input IDs, the label ID associated to each, and the labels vocab, this method will return a list of tuples whereby\n",
       "each tuple defines the \"token\" and its label name. For example:\n",
       "[('Way', B-PER), ('de', B-PER), ('Gill', I-PER), ('iam', I-PER), ('loves'), ('Hug', B-ORG), ('ging', B-ORG), ('Face', I-ORG)]\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| hf_tokenizer | PreTrainedTokenizerBase |  | A Hugging Face tokenizer |\n",
       "| input_ids | typing.List[int] |  | List of input_ids for the tokens in a single piece of processed text |\n",
       "| token_label_ids | typing.List[int] |  | List of label indexs for each token |\n",
       "| vocab | typing.List[str] |  | List of label names from witch the `label` indicies can be used to find the name of the label |\n",
       "| ignore_token_id | int | -100 | The token ID that should be ignored when calculating the loss |\n",
       "| ignore_token | str | [xIGNx] | The token used to identifiy ignored tokens (default: [xIGNx]) |\n",
       "| **Returns** | **typing.List[typing.Tuple[str, str]]** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### get_token_labels_from_input_ids\n",
       "\n",
       ">      get_token_labels_from_input_ids (hf_tokenizer:transformers.tokenization_u\n",
       ">                                       tils_base.PreTrainedTokenizerBase,\n",
       ">                                       input_ids:List[int],\n",
       ">                                       token_label_ids:List[int],\n",
       ">                                       vocab:List[str],\n",
       ">                                       ignore_token_id:int=-100,\n",
       ">                                       ignore_token:str='[xIGNx]')\n",
       "\n",
       "Given a list of input IDs, the label ID associated to each, and the labels vocab, this method will return a list of tuples whereby\n",
       "each tuple defines the \"token\" and its label name. For example:\n",
       "[('Way', B-PER), ('de', B-PER), ('Gill', I-PER), ('iam', I-PER), ('loves'), ('Hug', B-ORG), ('ging', B-ORG), ('Face', I-ORG)]\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| hf_tokenizer | PreTrainedTokenizerBase |  | A Hugging Face tokenizer |\n",
       "| input_ids | typing.List[int] |  | List of input_ids for the tokens in a single piece of processed text |\n",
       "| token_label_ids | typing.List[int] |  | List of label indexs for each token |\n",
       "| vocab | typing.List[str] |  | List of label names from witch the `label` indicies can be used to find the name of the label |\n",
       "| ignore_token_id | int | -100 | The token ID that should be ignored when calculating the loss |\n",
       "| ignore_token | str | [xIGNx] | The token used to identifiy ignored tokens (default: [xIGNx]) |\n",
       "| **Returns** | **typing.List[typing.Tuple[str, str]]** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(get_token_labels_from_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def get_word_labels_from_token_labels(\n",
    "    hf_arch: str,\n",
    "    # A Hugging Face tokenizer\n",
    "    hf_tokenizer: PreTrainedTokenizerBase,\n",
    "    # A list of tuples, where each represents a token and its label (e.g., [('Hug', B-ORG), ('ging', B-ORG), ('Face', I-ORG), ...])\n",
    "    tok_labels,\n",
    ") -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Given a list of tuples where each tuple defines a token and its label, return a list of tuples whereby each tuple defines the\n",
    "    \"word\" and its label. Method assumes that model inputs are a list of words, and in conjunction with the `align_labels_with_tokens` method,\n",
    "    allows the user to reconstruct the orginal raw inputs and labels.\n",
    "    \"\"\"\n",
    "    # recreate raw words list (we assume for token classification that the input is a list of words)\n",
    "    words = hf_tokenizer.convert_tokens_to_string(\n",
    "        [tok_label[0] for tok_label in tok_labels]\n",
    "    ).split()\n",
    "\n",
    "    if hf_arch == \"canine\":\n",
    "        word_list = [f\"{word} \" for word in words]\n",
    "    else:\n",
    "        word_list = [word for word in words]\n",
    "\n",
    "    # align \"words\" with labels\n",
    "    word_labels, idx = [], 0\n",
    "    for word in word_list:\n",
    "        word_labels.append((word, tok_labels[idx][1]))\n",
    "        idx += len(hf_tokenizer.tokenize(word))\n",
    "\n",
    "    return word_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTS for align_labels_with_words()\n",
    "for idx in range(5):\n",
    "    raw_word_list = conll2003_df.iloc[idx][\"tokens\"]\n",
    "    raw_label_list = conll2003_df.iloc[idx][\"ner_tags\"]\n",
    "\n",
    "    be = hf_tokenizer(raw_word_list, is_split_into_words=True)\n",
    "    input_ids = be[\"input_ids\"]\n",
    "    targ_ids = [\n",
    "        -100 if (word_id == None) else raw_label_list[word_id]\n",
    "        for word_id in be.word_ids()\n",
    "    ]\n",
    "\n",
    "    tok_labels = get_token_labels_from_input_ids(\n",
    "        hf_tokenizer, input_ids, targ_ids, labels\n",
    "    )\n",
    "    word_labels = get_word_labels_from_token_labels(hf_arch, hf_tokenizer, tok_labels)\n",
    "\n",
    "    for word_label, raw_word, raw_label_id in zip(\n",
    "        word_labels, raw_word_list, raw_label_list\n",
    "    ):\n",
    "        test_eq(word_label[0], raw_word)\n",
    "        test_eq(word_label[1], labels[raw_label_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### get_word_labels_from_token_labels\n",
       "\n",
       ">      get_word_labels_from_token_labels (hf_arch:str, hf_tokenizer:transformers\n",
       ">                                         .tokenization_utils_base.PreTrainedTok\n",
       ">                                         enizerBase, tok_labels)\n",
       "\n",
       "Given a list of tuples where each tuple defines a token and its label, return a list of tuples whereby each tuple defines the\n",
       "\"word\" and its label. Method assumes that model inputs are a list of words, and in conjunction with the `align_labels_with_tokens` method,\n",
       "allows the user to reconstruct the orginal raw inputs and labels.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| hf_arch | str |  |\n",
       "| hf_tokenizer | PreTrainedTokenizerBase | A Hugging Face tokenizer |\n",
       "| tok_labels |  | A list of tuples, where each represents a token and its label (e.g., [('Hug', B-ORG), ('ging', B-ORG), ('Face', I-ORG), ...]) |\n",
       "| **Returns** | **typing.List[typing.Tuple[str, str]]** |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### get_word_labels_from_token_labels\n",
       "\n",
       ">      get_word_labels_from_token_labels (hf_arch:str, hf_tokenizer:transformers\n",
       ">                                         .tokenization_utils_base.PreTrainedTok\n",
       ">                                         enizerBase, tok_labels)\n",
       "\n",
       "Given a list of tuples where each tuple defines a token and its label, return a list of tuples whereby each tuple defines the\n",
       "\"word\" and its label. Method assumes that model inputs are a list of words, and in conjunction with the `align_labels_with_tokens` method,\n",
       "allows the user to reconstruct the orginal raw inputs and labels.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| hf_arch | str |  |\n",
       "| hf_tokenizer | PreTrainedTokenizerBase | A Hugging Face tokenizer |\n",
       "| tok_labels |  | A list of tuples, where each represents a token and its label (e.g., [('Hug', B-ORG), ('ging', B-ORG), ('Face', I-ORG), ...]) |\n",
       "| **Returns** | **typing.List[typing.Tuple[str, str]]** |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(get_word_labels_from_token_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Targets -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `TokenTensorCategory` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class TokenTensorCategory(TensorBase):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `TokenCategorize` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class TokenCategorize(Transform):\n",
    "    \"\"\"Reversible transform of a list of category string to `vocab` id\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # The unique list of entities (e.g., B-LOC) (default: CategoryMap(vocab))\n",
    "        vocab: List[str] = None,\n",
    "        # The token used to identifiy ignored tokens (default: xIGNx)\n",
    "        ignore_token: str = \"[xIGNx]\",\n",
    "        # The token ID that should be ignored when calculating the loss (default: CrossEntropyLossFlat().ignore_index)\n",
    "        ignore_token_id: int = CrossEntropyLossFlat().ignore_index,\n",
    "    ):\n",
    "        self.vocab = None if vocab is None else CategoryMap(vocab, sort=False)\n",
    "        self.ignore_token, self.ignore_token_id = ignore_token, ignore_token_id\n",
    "\n",
    "        self.loss_func, self.order = (\n",
    "            CrossEntropyLossFlat(ignore_index=self.ignore_token_id),\n",
    "            1,\n",
    "        )\n",
    "\n",
    "    def setups(self, dsets):\n",
    "        if self.vocab is None and dsets is not None:\n",
    "            self.vocab = CategoryMap(dsets)\n",
    "        self.c = len(self.vocab)\n",
    "\n",
    "    def encodes(self, labels):\n",
    "        # if `val` is the label name (e.g., B-PER, I-PER, etc...), lookup the corresponding index in the vocab using\n",
    "        # `self.vocab.o2i`\n",
    "        ids = [val if (isinstance(val, int)) else self.vocab.o2i[val] for val in labels]\n",
    "        return TokenTensorCategory(ids)\n",
    "\n",
    "    def decodes(self, encoded_labels):\n",
    "        return Category(\n",
    "            [\n",
    "                (self.vocab[lbl_id])\n",
    "                for lbl_id in encoded_labels\n",
    "                if lbl_id != self.ignore_token_id\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TokenCategorize` modifies the fastai `Categorize` transform in a couple of ways.  \n",
    "\n",
    "First, it allows your targets to consist of a `Category` ***per*** token, and second, it uses the idea of an `ignore_token_id` to mask subtokens that don't need a prediction.  For example, the target of special tokens (e.g., pad, cls, sep) are set to `ignore_token_id` as are subsequent sub-tokens of a given token should more than 1 sub-token make it up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `TokenCategoryBlock` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def TokenCategoryBlock(\n",
    "    # The unique list of entities (e.g., B-LOC) (default: CategoryMap(vocab))\n",
    "    vocab: Optional[List[str]] = None,\n",
    "    # The token used to identifiy ignored tokens (default: xIGNx)\n",
    "    ignore_token: str = \"[xIGNx]\",\n",
    "    # The token ID that should be ignored when calculating the loss (default: CrossEntropyLossFlat().ignore_index)\n",
    "    ignore_token_id: int = CrossEntropyLossFlat().ignore_index,\n",
    "):\n",
    "    \"\"\"`TransformBlock` for per-token categorical targets\"\"\"\n",
    "    return TransformBlock(\n",
    "        type_tfms=TokenCategorize(\n",
    "            vocab=vocab, ignore_token=ignore_token, ignore_token_id=ignore_token_id\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### TokenCategoryBlock\n",
       "\n",
       ">      TokenCategoryBlock (vocab:Optional[List[str]]=None,\n",
       ">                          ignore_token:str='[xIGNx]', ignore_token_id:int=-100)\n",
       "\n",
       "`TransformBlock` for per-token categorical targets\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| vocab | typing.Optional[typing.List[str]] | None | The unique list of entities (e.g., B-LOC) (default: CategoryMap(vocab)) |\n",
       "| ignore_token | str | [xIGNx] | The token used to identifiy ignored tokens (default: xIGNx) |\n",
       "| ignore_token_id | int | -100 | The token ID that should be ignored when calculating the loss (default: CrossEntropyLossFlat().ignore_index) |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### TokenCategoryBlock\n",
       "\n",
       ">      TokenCategoryBlock (vocab:Optional[List[str]]=None,\n",
       ">                          ignore_token:str='[xIGNx]', ignore_token_id:int=-100)\n",
       "\n",
       "`TransformBlock` for per-token categorical targets\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| vocab | typing.Optional[typing.List[str]] | None | The unique list of entities (e.g., B-LOC) (default: CategoryMap(vocab)) |\n",
       "| ignore_token | str | [xIGNx] | The token used to identifiy ignored tokens (default: xIGNx) |\n",
       "| ignore_token_id | int | -100 | The token ID that should be ignored when calculating the loss (default: CrossEntropyLossFlat().ignore_index) |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TokenCategoryBlock, title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `TokenClassTextInput` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class TokenClassTextInput(TextInput):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we define a custom class, `TokenClassTextInput`, for the `@typedispatch`ed methods to use so that we can override how token classification inputs/targets are assembled, as well as, how the data is shown via methods like `show_batch` and `show_results`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `TokenClassBatchTokenizeTransform` -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class TokenClassBatchTokenizeTransform(BatchTokenizeTransform):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # The abbreviation/name of your Hugging Face transformer architecture (e.b., bert, bart, etc..)\n",
    "        hf_arch: str,\n",
    "        # A specific configuration instance you want to use\n",
    "        hf_config: PretrainedConfig,\n",
    "        # A Hugging Face tokenizer\n",
    "        hf_tokenizer: PreTrainedTokenizerBase,\n",
    "        # A Hugging Face model\n",
    "        hf_model: PreTrainedModel,\n",
    "        # To control whether the \"labels\" are included in your inputs. If they are, the loss will be calculated in\n",
    "        # the model's forward function and you can simply use `PreCalculatedLoss` as your `Learner`'s loss function to use it\n",
    "        include_labels: bool = True,\n",
    "        # The token ID that should be ignored when calculating the loss\n",
    "        ignore_token_id: int = CrossEntropyLossFlat().ignore_index,\n",
    "        # The labeling strategy you want to apply when associating labels with word tokens\n",
    "        labeling_strategy_cls: BaseLabelingStrategy = OnlyFirstTokenLabelingStrategy,\n",
    "        # the target label names\n",
    "        target_label_names: Optional[List[str]] = None,\n",
    "        # the label for non-entity\n",
    "        non_entity_label: str = \"O\",\n",
    "        # To control the length of the padding/truncation. It can be an integer or None,\n",
    "        # in which case it will default to the maximum length the model can accept. If the model has no\n",
    "        # specific maximum input length, truncation/padding to max_length is deactivated.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        max_length: Optional[int] = None,\n",
    "        # To control the `padding` applied to your `hf_tokenizer` during tokenization. If None, will default to\n",
    "        # `False` or `'do_not_pad'.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        padding: Union[bool, str] = True,\n",
    "        # To control `truncation` applied to your `hf_tokenizer` during tokenization. If None, will default to\n",
    "        # `False` or `do_not_truncate`.\n",
    "        # See [Everything you always wanted to know about padding and truncation](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation)\n",
    "        truncation: Union[bool, str] = True,\n",
    "        # The `is_split_into_words` argument applied to your `hf_tokenizer` during tokenization. Set this to `True`\n",
    "        # if your inputs are pre-tokenized (not numericalized)\n",
    "        is_split_into_words: bool = True,\n",
    "        # If using a slow tokenizer, users will need to prove a `slow_word_ids_func` that accepts a\n",
    "        # tokenizzer, example index, and a batch encoding as arguments and in turn returnes the\n",
    "        # equavlient of fast tokenizer's `word_ids``\n",
    "        slow_word_ids_func: Optional[Callable] = None,\n",
    "        # Any other keyword arguments you want included when using your `hf_tokenizer` to tokenize your inputs\n",
    "        tok_kwargs: dict = {},\n",
    "        # Keyword arguments to apply to `TokenClassBatchTokenizeTransform`\n",
    "        **kwargs\n",
    "    ):\n",
    "\n",
    "        super().__init__(\n",
    "            hf_arch,\n",
    "            hf_config,\n",
    "            hf_tokenizer,\n",
    "            hf_model,\n",
    "            include_labels=include_labels,\n",
    "            ignore_token_id=ignore_token_id,\n",
    "            max_length=max_length,\n",
    "            padding=padding,\n",
    "            truncation=truncation,\n",
    "            is_split_into_words=is_split_into_words,\n",
    "            tok_kwargs=tok_kwargs,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        self.target_label_names = target_label_names\n",
    "        self.non_entity_label = non_entity_label\n",
    "        self.slow_word_ids_func = slow_word_ids_func\n",
    "\n",
    "        self.labeling_strategy = labeling_strategy_cls(\n",
    "            hf_tokenizer,\n",
    "            label_names=self.target_label_names,\n",
    "            non_entity_label=self.non_entity_label,\n",
    "            ignore_token_id=ignore_token_id,\n",
    "        )\n",
    "\n",
    "    def encodes(self, samples, return_batch_encoding=False):\n",
    "        encoded_samples, inputs = super().encodes(samples, return_batch_encoding=True)\n",
    "\n",
    "        # if there are no targets (e.g., when used for inference)\n",
    "        if len(encoded_samples[0]) == 1:\n",
    "            return encoded_samples\n",
    "\n",
    "        # get the type of our targets (by default will be TokenTensorCategory)\n",
    "        target_cls = type(encoded_samples[0][1])\n",
    "\n",
    "        updated_samples = []\n",
    "        for idx, s in enumerate(encoded_samples):\n",
    "            # with batch-time tokenization, we have to align each token with the correct label using the `word_ids` in the\n",
    "            # batch encoding object we get from calling our *fast* tokenizer\n",
    "            word_ids = (\n",
    "                inputs.word_ids(idx)\n",
    "                if self.hf_tokenizer.is_fast\n",
    "                else self.slow_word_ids_func(self.hf_tokenizer, idx, inputs)\n",
    "            )\n",
    "            targ_ids = target_cls(\n",
    "                self.labeling_strategy.align_labels_with_tokens(\n",
    "                    word_ids, s[-1].tolist()\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if self.include_labels and len(targ_ids) > 0:\n",
    "                s[0][\"labels\"] = targ_ids\n",
    "\n",
    "            updated_samples.append((s[0], targ_ids))\n",
    "\n",
    "        if return_batch_encoding:\n",
    "            return updated_samples, inputs\n",
    "\n",
    "        return updated_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TokenClassBatchTokenizeTransform` is used to exclude any of the target's tokens we don't want to include in the loss calcuation (e.g. padding, cls, sep, etc...). \n",
    "\n",
    "Note also that we default `is_split_into_words = True` since token classification tasks expect a list of words and labels for each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the mid-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch-Time Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Get your Hugging Face objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('roberta',\n",
       " transformers.models.roberta.configuration_roberta.RobertaConfig,\n",
       " transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast,\n",
       " transformers.models.roberta.modeling_roberta.RobertaForTokenClassification)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "pretrained_model_name = \"distilroberta-base\"\n",
    "n_labels = len(labels)\n",
    "\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(\n",
    "    pretrained_model_name,\n",
    "    model_cls=AutoModelForTokenClassification,\n",
    "    config_kwargs={\"num_labels\": n_labels},\n",
    ")\n",
    "\n",
    "hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Step 2: Create your `DataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tok_tfm = TokenClassBatchTokenizeTransform(\n",
    "    hf_arch,\n",
    "    hf_config,\n",
    "    hf_tokenizer,\n",
    "    hf_model,\n",
    "    labeling_strategy_cls=BILabelingStrategy,\n",
    "    target_label_names=labels,\n",
    ")\n",
    "blocks = (\n",
    "    TextBlock(batch_tokenize_tfm=batch_tok_tfm, input_return_type=TokenClassTextInput),\n",
    "    TokenCategoryBlock(vocab=labels),\n",
    ")\n",
    "\n",
    "dblock = DataBlock(\n",
    "    blocks=blocks,\n",
    "    get_x=ColReader(\"tokens\"),\n",
    "    get_y=ColReader(\"ner_tags\"),\n",
    "    splitter=RandomSplitter(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "# dblock.summary(conll2003_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Build your `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(conll2003_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([4, 156]), torch.Size([4, 156]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b), b[0][\"input_ids\"].shape, b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@typedispatch\n",
    "def show_batch(\n",
    "    # This typedispatched `show_batch` will be called for `TokenClassTextInput` typed inputs\n",
    "    x: TokenClassTextInput,\n",
    "    y,\n",
    "    # Your raw inputs/targets\n",
    "    samples,\n",
    "    # Your `DataLoaders`. This is required so as to get at the Hugging Face objects for\n",
    "    # decoding them into something understandable\n",
    "    dataloaders,\n",
    "    # Your `show_batch` context\n",
    "    ctxs=None,\n",
    "    # The maximum number of items to show\n",
    "    max_n=6,\n",
    "    # Any truncation your want applied to your decoded inputs\n",
    "    trunc_at=None,\n",
    "    # Any other keyword arguments you want applied to `show_batch`\n",
    "    **kwargs,\n",
    "):\n",
    "    # grab our tokenizer\n",
    "    tfm = first_blurr_tfm(dataloaders, tfms=[TokenClassBatchTokenizeTransform])\n",
    "    hf_arch, hf_tokenizer = tfm.hf_arch, tfm.hf_tokenizer\n",
    "    vocab = dataloaders.vocab\n",
    "\n",
    "    res = L()\n",
    "    for inp, trg, sample in zip(x, y, samples):\n",
    "        # align \"tokens\" with labels\n",
    "        tok_labels = get_token_labels_from_input_ids(hf_tokenizer, inp, trg, vocab)\n",
    "        # align \"words\" with labels\n",
    "        word_labels = get_word_labels_from_token_labels(\n",
    "            hf_arch, hf_tokenizer, tok_labels\n",
    "        )\n",
    "        # stringify list of (word,label) for example\n",
    "        res.append(\n",
    "            [\n",
    "                f\"{[ word_targ for idx, word_targ in enumerate(word_labels) if (trunc_at is None or idx < trunc_at) ]}\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    display_df(pd.DataFrame(res, columns=[\"word / target label\"])[:max_n])\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O'), ('TALK', 'O'), ('-', 'O'), ('USDA', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('August', 'O'), ('22', 'O'), (',', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Slough', 'B-ORG'), (\"'s\", 'O'), ('chairman', 'O'), ('Sir', 'O'), ('Nigel', 'B-PER'), ('Mobbs', 'I-PER'), ('added', 'O'), ('to', 'O'), ('the', 'O'), ('bullish', 'O'), ('mood', 'O'), ('in', 'O'), ('the', 'O'), ('sector', 'O'), (',', 'O'), ('saying', 'O'), ('in', 'O'), ('a', 'O'), ('statement', 'O'), ('that', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[('The', 'O'), ('government-owned', 'O'), ('al-Ingaz', 'B-ORG'), ('al-Watani', 'I-ORG'), ('said', 'O'), ('the', 'O'), ('smugglers', 'O'), ('were', 'O'), ('caught', 'O'), ('in', 'O'), ('Banat', 'B-LOC'), ('in', 'O'), ('the', 'O'), ('eastern', 'O'), ('state', 'O'), ('of', 'O'), ('Kassala', 'B-LOC'), (',', 'O'), ('on', 'O'), ('the', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[('\"', 'O'), ('The', 'O'), ('ultimatum', 'O'), ('(', 'O'), ('to', 'O'), ('storm', 'O'), ('Grozny', 'B-LOC'), (')', 'O'), ('is', 'O'), ('no', 'O'), ('longer', 'O'), ('an', 'O'), ('issue', 'O'), (',', 'O'), ('\"', 'O'), ('he', 'O'), ('said', 'O'), ('quoting', 'O'), ('Ischinger', 'B-PER'), (',', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=5, trunc_at=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passing extra infromation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1b: Get your Hugging Face objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('roberta',\n",
       " transformers.models.roberta.configuration_roberta.RobertaConfig,\n",
       " transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast,\n",
       " transformers.models.roberta.modeling_roberta.RobertaForTokenClassification)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "pretrained_model_name = \"distilroberta-base\"\n",
    "n_labels = len(labels)\n",
    "\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(\n",
    "    pretrained_model_name,\n",
    "    model_cls=AutoModelForTokenClassification,\n",
    "    config_kwargs={\"num_labels\": n_labels},\n",
    ")\n",
    "\n",
    "hf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1b. Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proc_tokens</th>\n",
       "      <th>proc_ner_tags</th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>chunk_tags</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[EU, rejects, German, call, to, boycott, British, lamb, .]</td>\n",
       "      <td>[3, 0, 7, 0, 0, 0, 7, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>[EU, rejects, German, call, to, boycott, British, lamb, .]</td>\n",
       "      <td>[22, 42, 16, 21, 35, 37, 16, 21, 7]</td>\n",
       "      <td>[11, 21, 11, 12, 21, 22, 11, 12, 0]</td>\n",
       "      <td>[3, 0, 7, 0, 0, 0, 7, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Peter, Blackburn]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>1</td>\n",
       "      <td>[Peter, Blackburn]</td>\n",
       "      <td>[22, 22]</td>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>[1, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  proc_tokens  \\\n",
       "0  [EU, rejects, German, call, to, boycott, British, lamb, .]   \n",
       "1                                          [Peter, Blackburn]   \n",
       "\n",
       "                 proc_ner_tags id  \\\n",
       "0  [3, 0, 7, 0, 0, 0, 7, 0, 0]  0   \n",
       "1                       [1, 2]  1   \n",
       "\n",
       "                                                       tokens  \\\n",
       "0  [EU, rejects, German, call, to, boycott, British, lamb, .]   \n",
       "1                                          [Peter, Blackburn]   \n",
       "\n",
       "                              pos_tags                           chunk_tags  \\\n",
       "0  [22, 42, 16, 21, 35, 37, 16, 21, 7]  [11, 21, 11, 12, 21, 22, 11, 12, 0]   \n",
       "1                             [22, 22]                             [11, 12]   \n",
       "\n",
       "                      ner_tags  \n",
       "0  [3, 0, 7, 0, 0, 0, 7, 0, 0]  \n",
       "1                       [1, 2]  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = TokenClassPreprocessor(\n",
    "    hf_tokenizer,\n",
    "    label_names=labels,\n",
    "    id_attr=\"id\",\n",
    "    word_list_attr=\"tokens\",\n",
    "    label_list_attr=\"ner_tags\",\n",
    "    tok_kwargs={\"max_length\": 128},\n",
    ")\n",
    "proc_df = preprocessor.process_df(conll2003_df)\n",
    "proc_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Create your `DataBlock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tok_tfm = TokenClassBatchTokenizeTransform(\n",
    "    hf_arch, hf_config, hf_tokenizer, hf_model, target_label_names=labels\n",
    ")\n",
    "blocks = (\n",
    "    TextBlock(batch_tokenize_tfm=batch_tok_tfm, input_return_type=TokenClassTextInput),\n",
    "    TokenCategoryBlock(vocab=labels),\n",
    ")\n",
    "\n",
    "\n",
    "def get_x(item):\n",
    "    return {\"id\": item.id, \"text\": item.proc_tokens}\n",
    "\n",
    "\n",
    "dblock = DataBlock(\n",
    "    blocks=blocks,\n",
    "    get_x=get_x,\n",
    "    get_y=ColReader(\"proc_ner_tags\"),\n",
    "    splitter=RandomSplitter(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Build your `DataLoaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(proc_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'id', 'labels'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "b[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([4, 130]), torch.Size([4, 130]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b), b[0][\"input_ids\"].shape, b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O'), ('TALK', 'O'), ('-', 'O'), ('USDA', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('August', 'O'), ('22', 'O'), (',', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O'), ('This', 'O'), ('finding', 'O'), ('is', 'O'), ('important', 'O'), ('because', 'O'), ('one', 'O'), ('of', 'O'), ('the', 'O'), ('jars', 'O'), ('still', 'O'), ('contains', 'O'), ('substances', 'O'), ('and', 'O'), ('materials', 'O'), ('used', 'O'), ('in', 'O'), ('the', 'O'), ('conservation', 'O'), ('of', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[('\"', 'O'), ('We', 'O'), ('have', 'O'), ('always', 'O'), ('been', 'O'), ('concerned', 'O'), ('about', 'O'), ('barter', 'O'), ('deals', 'O'), ('with', 'O'), ('other', 'O'), ('countries', 'O'), (',', 'O'), ('viewing', 'O'), ('them', 'O'), ('as', 'O'), ('a', 'O'), ('disguised', 'O'), ('kind', 'O'), ('of', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[('The', 'O'), ('officials', 'O'), ('had', 'O'), ('been', 'O'), ('positive', 'O'), ('about', 'O'), ('Kinkel', 'B-PER'), (\"'s\", 'O'), ('request', 'O'), ('on', 'O'), ('Wednesday', 'O'), ('that', 'O'), ('President', 'O'), ('Boris', 'B-PER'), ('Yeltsin', 'I-PER'), (\"'s\", 'O'), ('security', 'O'), ('chief', 'O'), ('Alexander', 'B-PER'), ('Lebed', 'I-PER')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=5, trunc_at=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "The tests below to ensure the core DataBlock code above works for **all** pretrained token classification models available in Hugging Face.  These tests are excluded from the CI workflow because of how long they would take to run and the amount of data that would be required to download.\n",
    "\n",
    "**Note**: Feel free to modify the code below to test whatever pretrained classification models you are working with ... and if any of your pretrained token classification models fail, please submit a github issue *(or a PR if you'd like to fix it yourself)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AlbertForTokenClassification',\n",
       " 'BertForTokenClassification',\n",
       " 'BigBirdForTokenClassification',\n",
       " 'BloomForTokenClassification',\n",
       " 'CamembertForTokenClassification',\n",
       " 'CanineForTokenClassification',\n",
       " 'ConvBertForTokenClassification',\n",
       " 'Data2VecTextForTokenClassification',\n",
       " 'DebertaForTokenClassification',\n",
       " 'DebertaV2ForTokenClassification',\n",
       " 'DistilBertForTokenClassification',\n",
       " 'ElectraForTokenClassification',\n",
       " 'FNetForTokenClassification',\n",
       " 'FlaubertForTokenClassification',\n",
       " 'FunnelForTokenClassification',\n",
       " 'GPT2ForTokenClassification',\n",
       " 'IBertForTokenClassification',\n",
       " 'LayoutLMForTokenClassification',\n",
       " 'LayoutLMv2ForTokenClassification',\n",
       " 'LayoutLMv3ForTokenClassification',\n",
       " 'LongformerForTokenClassification',\n",
       " 'MPNetForTokenClassification',\n",
       " 'MegatronBertForTokenClassification',\n",
       " 'MobileBertForTokenClassification',\n",
       " 'NezhaForTokenClassification',\n",
       " 'NystromformerForTokenClassification',\n",
       " 'QDQBertForTokenClassification',\n",
       " 'RemBertForTokenClassification',\n",
       " 'RoFormerForTokenClassification',\n",
       " 'RobertaForTokenClassification',\n",
       " 'SqueezeBertForTokenClassification',\n",
       " 'ViltForTokenClassification',\n",
       " 'XLMForTokenClassification',\n",
       " 'XLMRobertaForTokenClassification',\n",
       " 'XLMRobertaXLForTokenClassification',\n",
       " 'XLNetForTokenClassification',\n",
       " 'YosoForTokenClassification']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |hide\n",
    "[\n",
    "    model_type\n",
    "    for model_type in NLP.get_models(task=\"TokenClassification\")\n",
    "    if (not model_type.startswith(\"TF\"))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "pretrained_model_names = [\n",
    "    \"hf-internal-testing/tiny-albert\",\n",
    "    \"hf-internal-testing/tiny-bert\",\n",
    "    \"google/bigbird-roberta-base\",\n",
    "    \"camembert-base\",\n",
    "    # \"google/canine-s\",                                  # word_ids\n",
    "    \"YituTech/conv-bert-base\",\n",
    "    \"hf-internal-testing/tiny-deberta\",\n",
    "    # \"microsoft/deberta-v2-xlarge\",                      # word_ids\n",
    "    \"sshleifer/tiny-distilbert-base-cased\",\n",
    "    \"hf-internal-testing/tiny-electra\",\n",
    "    # \"google/fnet-base\",                               # forward() got an unexpected keyword argument 'output_attentions'\n",
    "    # \"flaubert/flaubert_small_cased\",                    # word_ids\n",
    "    \"huggingface/funnel-small-base\",\n",
    "    \"sshleifer/tiny-gpt2\",\n",
    "    \"hf-internal-testing/tiny-layoutlm\",\n",
    "    \"allenai/longformer-base-4096\",\n",
    "    \"microsoft/mpnet-base\",\n",
    "    \"kssteven/ibert-roberta-base\",\n",
    "    # \"nvidia/megatron-bert-cased-345m\",                # could not test\n",
    "    \"google/mobilebert-uncased\",\n",
    "    \"google/rembert\",\n",
    "    \"junnyu/roformer_chinese_sim_char_ft_small\",\n",
    "    \"roberta-base\",\n",
    "    \"squeezebert/squeezebert-uncased\",\n",
    "    # \"xlm-mlm-en-2048\",                                  # word_ids\n",
    "    \"xlm-roberta-base\",\n",
    "    \"xlnet-base-cased\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/home/wgilliam/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0161130428314209,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7606c60f8a574d86bb1a5c1a0a05630b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"conll2003\")\n",
    "conll2003_df = pd.DataFrame(raw_datasets[\"train\"])\n",
    "\n",
    "labels = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-albert ===\n",
      "\n",
      "architecture:\talbert\n",
      "tokenizer:\tAlbertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('market', 'O'), ('talk', 'O'), ('-', 'O'), ('usda', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('august', 'O'), ('22', 'O'), (',', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('the', 'O'), ('airbus', 'B-MISC'), ('310', 'I-MISC'), ('flight', 'B-MISC'), ('150', 'I-MISC'), (',', 'O'), ('which', 'O'), ('was', 'O'), ('hijacked', 'O'), ('on', 'O'), ('monday', 'O'), ('evening', 'O'), ('on', 'O'), ('its', 'O'), ('way', 'O'), ('from', 'O'), ('khartoum', 'B-LOC'), ('to', 'O'), ('the', 'O'), ('jordanian', 'B-MISC')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-bert ===\n",
      "\n",
      "architecture:\tbert\n",
      "tokenizer:\tBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('market', 'O'), ('talk', 'O'), ('-', 'O'), ('usda', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('august', 'O'), ('22,', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O'), ('and', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('they', 'O'), ('show', 'O'), ('that', 'O'), ('the', 'O'), ('gendarmes', 'O'), ('were', 'O'), ('aware', 'O'), ('that', 'O'), ('dutroux', 'B-PER'), ('was', 'O'), ('building', 'O'), ('cells', 'O'), ('in', 'O'), ('some', 'O'), ('of', 'O'), ('his', 'O'), ('houses', 'O'), ('for', 'O'), ('holding', 'O'), ('children,', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/bigbird-roberta-base ===\n",
      "\n",
      "architecture:\tbig_bird\n",
      "tokenizer:\tBigBirdTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O'), ('TALK', 'O'), ('-', 'O'), ('USDA', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('August', 'O'), ('22', 'O'), (',', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('The', 'O'), ('princess', 'O'), (',', 'O'), ('who', 'O'), ('has', 'O'), ('carved', 'O'), ('out', 'O'), ('a', 'O'), ('major', 'O'), ('role', 'O'), ('for', 'O'), ('herself', 'O'), ('as', 'O'), ('a', 'O'), ('helper', 'O'), ('of', 'O'), ('the', 'O'), ('sick', 'O'), ('and', 'O'), ('needy', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== camembert-base ===\n",
      "\n",
      "architecture:\tcamembert\n",
      "tokenizer:\tCamembertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O'), ('TALK', 'O'), ('-', 'O'), ('USDA', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('August', 'O'), ('22', 'O'), (',', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Despite', 'O'), ('a', 'O'), ('mood', 'O'), ('of', 'O'), ('compromise', 'O'), ('in', 'O'), ('the', 'O'), ('region', 'O'), ('after', 'O'), ('some', 'O'), ('of', 'O'), ('the', 'O'), ('worst', 'O'), ('fighting', 'O'), ('of', 'O'), ('the', 'O'), ('war', 'O'), (',', 'O'), ('Lebed', 'B-PER'), ('may', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== YituTech/conv-bert-base ===\n",
      "\n",
      "architecture:\tconvbert\n",
      "tokenizer:\tConvBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('market', 'O'), ('talk', 'O'), ('-', 'O'), ('usda', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('august', 'O'), ('22,', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O'), ('and', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('15', 'O'), ('-', 'O'), ('christian', 'B-PER'), ('cullen,', 'I-PER'), ('14', 'O'), ('-', 'O'), ('jeff', 'B-PER'), ('wilson,', 'I-PER'), ('13', 'O'), ('-', 'O'), ('walter', 'B-PER'), ('little,', 'I-PER'), ('12', 'O'), ('-', 'O'), ('frank', 'B-PER'), ('bunce,', 'I-PER'), ('11', 'O'), ('-', 'O'), ('glen', 'B-PER'), ('osborne', 'I-PER')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-deberta ===\n",
      "\n",
      "architecture:\tdeberta\n",
      "tokenizer:\tDebertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O'), ('TALK', 'O'), ('-', 'O'), ('USDA', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('August', 'O'), ('22', 'O'), (',', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('A', 'O'), ('government', 'O'), ('statement', 'O'), (',', 'O'), ('broadcast', 'O'), ('repeatedly', 'O'), ('by', 'O'), ('state', 'O'), ('radio', 'O'), (',', 'O'), ('said', 'O'), ('the', 'O'), ('two', 'O'), ('days', 'O'), ('of', 'O'), ('prayer', 'O'), ('were', 'O'), ('\"', 'O'), ('for', 'O'), ('the', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== sshleifer/tiny-distilbert-base-cased ===\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016414642333984375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading config.json",
       "rate": null,
       "total": 990,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef82d2daf8047a382a54a1d760450d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/990 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.026143312454223633,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading tokenizer_config.json",
       "rate": null,
       "total": 49,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f96653376ef4385aa6e6a44dc694984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015770912170410156,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading vocab.txt",
       "rate": null,
       "total": 213450,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a3fb5d893346a6906556511302e8ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.020140886306762695,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading special_tokens_map.json",
       "rate": null,
       "total": 112,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71673d91949d4d02be3b9be0a5d8fc48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016589641571044922,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading pytorch_model.bin",
       "rate": null,
       "total": 245840,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "669832149fc748cdbc507a74fe726169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/240k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tbert\n",
      "tokenizer:\tBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O'), ('TALK', 'O'), ('-', 'O'), ('USDA', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('August', 'O'), ('22,', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O'), ('and', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Many', 'O'), ('software', 'O'), ('developers', 'O'), ('apparently', 'O'), ('saw', 'O'), ('their', 'O'), ('crucial', 'O'), ('holiday', 'O'), ('season', 'O'), ('sales', 'O'), ('suffer', 'O'), ('last', 'O'), ('year', 'O'), ('because', 'O'), ('store', 'O'), ('shelves', 'O'), ('were', 'O'), ('jammed', 'O'), ('with', 'O'), ('blue', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-electra ===\n",
      "\n",
      "architecture:\telectra\n",
      "tokenizer:\tElectraTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('market', 'O'), ('talk', 'O'), ('-', 'O'), ('usda', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('august', 'O'), ('22,', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O'), ('and', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('people', 'O'), ('are', 'O'), ('rushing', 'O'), ('to', 'O'), ('the', 'O'), ('hospital', 'O'), ('as', 'O'), ('soon', 'O'), ('as', 'O'), ('the', 'O'), ('first', 'O'), ('symptoms', 'O'), ('appear,', 'O'), (\"that's\", 'O'), ('why', 'O'), ('we', 'O'), ('have', 'O'), ('fewer', 'O'), ('deaths,', 'O'), ('\"', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== huggingface/funnel-small-base ===\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01786494255065918,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading config.json",
       "rate": null,
       "total": 599,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d36d5983f048c383e93155688ff069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01594257354736328,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading tokenizer_config.json",
       "rate": null,
       "total": 48,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e451ab8874948d288deafdee23ca1e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.022533416748046875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading vocab.txt",
       "rate": null,
       "total": 231485,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746cd613cf5c409f8db0df8cf688adec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017462968826293945,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading special_tokens_map.json",
       "rate": null,
       "total": 153,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afdb0fb0e50d4bad855ef9ec711127e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading special_tokens_map.json:   0%|          | 0.00/153 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01610708236694336,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading pytorch_model.bin",
       "rate": null,
       "total": 462539834,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e82f6deaf5045b69f32b4cb1727d786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/441M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tfunnel\n",
      "tokenizer:\tFunnelTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('market', 'O'), ('talk', 'O'), ('-', 'O'), ('usda', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('august', 'O'), ('22,', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O'), ('and', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('in', 'O'), ('new', 'B-LOC'), ('york,', 'I-LOC'), ('wally', 'B-PER'), ('whitehurst', 'I-PER'), ('allowed', 'O'), ('two', 'O'), ('runs', 'O'), ('over', 'O'), ('seven', 'O'), ('innings', 'O'), ('for', 'O'), ('his', 'O'), ('first', 'O'), ('win', 'O'), ('in', 'O'), ('more', 'O'), ('than', 'O'), ('two', 'O'), ('years', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== sshleifer/tiny-gpt2 ===\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015985488891601562,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading config.json",
       "rate": null,
       "total": 662,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ec05594729425d8db2431758cc5ecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016889095306396484,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading tokenizer_config.json",
       "rate": null,
       "total": 26,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad95938f8e349fc94e0c7058f32d4c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0184633731842041,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading vocab.json",
       "rate": null,
       "total": 898669,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0b1cb6df5054f36b2201b4c45407e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01719975471496582,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading merges.txt",
       "rate": null,
       "total": 456318,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21368cd5feff4fc788f04a90db21d172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01923203468322754,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading special_tokens_map.json",
       "rate": null,
       "total": 90,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df7be1640cd048f29677f49f17cd35db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016243934631347656,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading pytorch_model.bin",
       "rate": null,
       "total": 2514146,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29af2528f446424e870324cbf3a6899e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/2.40M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tgpt2\n",
      "tokenizer:\tGPT2TokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('15', 'O'), ('-', 'O'), ('Christian', 'B-PER'), ('Cullen', 'I-PER'), (',', 'O'), ('14', 'O'), ('-', 'O'), ('Jeff', 'B-PER'), ('Wilson', 'I-PER'), (',', 'O'), ('13', 'O'), ('-', 'O'), ('Walter', 'B-PER'), ('Little', 'I-PER'), (',', 'O'), ('12', 'O'), ('-', 'O'), ('Frank', 'B-PER'), ('Bunce', 'I-PER'), (',', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('With', 'O'), ('their', 'O'), ('fifth', 'O'), ('straight', 'O'), ('win', 'O'), (',', 'O'), ('the', 'O'), ('Dodgers', 'B-ORG'), ('moved', 'O'), ('a', 'O'), ('half-game', 'O'), ('ahead', 'O'), ('of', 'O'), ('the', 'O'), ('Expos', 'B-ORG'), ('at', 'O'), ('the', 'O'), ('top', 'O'), ('of', 'O'), ('the', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== hf-internal-testing/tiny-layoutlm ===\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016591548919677734,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading config.json",
       "rate": null,
       "total": 745,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ce0059c52a42a2b4853621c503ca1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/745 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017308473587036133,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading tokenizer_config.json",
       "rate": null,
       "total": 525,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75e0855ad194e269047d4a1a256dd75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/525 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016907691955566406,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading vocab.txt",
       "rate": null,
       "total": 35775,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dad8d6acfa244a71b078a18a80975b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/34.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01941537857055664,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading tokenizer.json",
       "rate": null,
       "total": 71170,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f204412dd1a3485bae4e262ec3120bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/69.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015133142471313477,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading special_tokens_map.json",
       "rate": null,
       "total": 112,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e72d45aa1a4c1c953902f5953c7340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0166928768157959,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading pytorch_model.bin",
       "rate": null,
       "total": 456786,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f95ba007bba84c3cae2cfc779f679b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tlayoutlm\n",
      "tokenizer:\tLayoutLMTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('market', 'O'), ('talk', 'O'), ('-', 'O'), ('usda', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('august', 'O'), ('22,', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O'), ('and', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('he', 'O'), ('said', 'O'), ('stallone,', 'B-PER'), ('best', 'O'), ('known', 'O'), ('for', 'O'), ('the', 'O'), ('\"', 'O'), ('rocky', 'B-MISC'), ('\"', 'O'), ('and', 'O'), ('\"', 'O'), ('rambo', 'B-MISC'), ('\"', 'O'), ('movies,', 'O'), ('left', 'O'), ('the', 'O'), ('set', 'O'), ('of', 'O'), ('\"', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== allenai/longformer-base-4096 ===\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017055511474609375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading config.json",
       "rate": null,
       "total": 694,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63bdf401797045d1b9a0c2e80c11cb7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015610218048095703,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading vocab.json",
       "rate": null,
       "total": 898823,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202a425abd3b474cb6e7e98f294afd45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015609979629516602,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading merges.txt",
       "rate": null,
       "total": 456318,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87abc1db73474f01933588afc7d57c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01708531379699707,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading tokenizer.json",
       "rate": null,
       "total": 1355863,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e4502267e945adbe223cae35dfeddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0158693790435791,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading pytorch_model.bin",
       "rate": null,
       "total": 597257159,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1793e18847941988305ee87abb658eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/570M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tlongformer\n",
      "tokenizer:\tLongformerTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('15', 'O'), ('-', 'O'), ('Christian', 'B-PER'), ('Cullen', 'I-PER'), (',', 'O'), ('14', 'O'), ('-', 'O'), ('Jeff', 'B-PER'), ('Wilson', 'I-PER'), (',', 'O'), ('13', 'O'), ('-', 'O'), ('Walter', 'B-PER'), ('Little', 'I-PER'), (',', 'O'), ('12', 'O'), ('-', 'O'), ('Frank', 'B-PER'), ('Bunce', 'I-PER'), (',', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('\"', 'O'), ('The', 'O'), ('ultimatum', 'O'), ('(', 'O'), ('to', 'O'), ('storm', 'O'), ('Grozny', 'B-LOC'), (')', 'O'), ('is', 'O'), ('no', 'O'), ('longer', 'O'), ('an', 'O'), ('issue', 'O'), (',', 'O'), ('\"', 'O'), ('he', 'O'), ('said', 'O'), ('quoting', 'O'), ('Ischinger', 'B-PER'), (',', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== microsoft/mpnet-base ===\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016499042510986328,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading config.json",
       "rate": null,
       "total": 493,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56577d6de80d4a37b42eace33959c0ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/493 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02223038673400879,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading vocab.txt",
       "rate": null,
       "total": 231536,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe17d822ba844a68916e5dbb9006101e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017274141311645508,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading tokenizer.json",
       "rate": null,
       "total": 472031,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e3b8423e2f432b8c4cc321ab97e805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/461k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017993927001953125,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading pytorch_model.bin",
       "rate": null,
       "total": 532009609,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e7878c82fa244eaa7aa8e423ac41871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/507M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tmpnet\n",
      "tokenizer:\tMPNetTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('market', 'O'), ('talk', 'O'), ('-', 'O'), ('usda', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('august', 'O'), ('22,', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O'), ('and', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('workers', 'O'), ('fixing', 'O'), ('the', 'O'), ('ceiling', 'O'), ('of', 'O'), ('a', 'O'), ('tax', 'O'), ('office', 'O'), ('in', 'O'), ('paris', 'B-LOC'), ('found', 'O'), ('a', 'O'), ('dozen', 'O'), ('seven', 'O'), ('-', '[xIGNx]'), ('year', '[xIGNx]'), ('-', '[xIGNx]'), ('old', '[xIGNx]'), ('cheques', 'O'), ('for', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== kssteven/ibert-roberta-base ===\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016513347625732422,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading tokenizer.json",
       "rate": null,
       "total": 1355863,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c619ac439f48348baf85c1a0dd6468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tibert\n",
      "tokenizer:\tRobertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O'), ('TALK', 'O'), ('-', 'O'), ('USDA', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('August', 'O'), ('22', 'O'), (',', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('A', 'O'), ('super', 'O'), ('piece', 'O'), ('of', 'O'), ('fielding', 'O'), ('by', 'O'), ('Lewis', 'B-PER'), (',', 'O'), ('dropped', 'O'), ('as', 'O'), ('a', 'O'), ('disciplinary', 'O'), ('measure', 'O'), ('after', 'O'), ('arriving', 'O'), ('only', 'O'), ('35', 'O'), ('minutes', 'O'), ('before', 'O'), ('the', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/mobilebert-uncased ===\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016736268997192383,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading config.json",
       "rate": null,
       "total": 847,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bc00a8162b84e68b67d27eca8aea539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/847 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01701807975769043,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading vocab.txt",
       "rate": null,
       "total": 231508,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d676dbfebb5642db8abff1e49f659061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01730799674987793,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading tokenizer.json",
       "rate": null,
       "total": 466062,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6b1a4a63884ec0a8dc52c44e6de8e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015804290771484375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading pytorch_model.bin",
       "rate": null,
       "total": 146863759,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8631ef3055de4c43a4b4505cc1438e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/140M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tmobilebert\n",
      "tokenizer:\tMobileBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('market', 'O'), ('talk', 'O'), ('-', 'O'), ('usda', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('august', 'O'), ('22,', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O'), ('and', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('squad', 'O'), (':', 'O'), ('alan', 'B-PER'), ('kelly,', 'I-PER'), ('shay', 'B-PER'), ('given,', 'I-PER'), ('denis', 'B-PER'), ('irwin,', 'I-PER'), ('phil', 'B-PER'), ('babb,', 'I-PER'), ('jeff', 'B-PER'), ('kenna,', 'I-PER'), ('curtis', 'B-PER'), ('fleming,', 'I-PER'), ('gary', 'B-PER'), ('breen,', 'I-PER'), ('ian', 'B-PER'), ('harte,', 'I-PER'), ('kenny', 'B-PER'), ('cunningham,', 'I-PER')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/rembert ===\n",
      "\n",
      "architecture:\trembert\n",
      "tokenizer:\tRemBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O'), ('TALK', 'O'), ('-', 'O'), ('USDA', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('August', 'O'), ('22', 'O'), (',', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('In', 'O'), ('his', 'O'), ('opinion', 'O'), ('the', 'O'), ('quartering', 'O'), ('of', 'O'), ('Unita', 'B-ORG'), ('forces', 'O'), ('must', 'O'), ('be', 'O'), ('concluded', 'O'), ('in', 'O'), ('all', 'O'), ('the', 'O'), ('Angolan', 'B-MISC'), ('territory', 'O'), ('and', 'O'), ('the', 'O'), ('troops', 'O'), ('must', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== junnyu/roformer_chinese_sim_char_ft_small ===\n",
      "\n",
      "architecture:\troformer\n",
      "tokenizer:\tRoFormerTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('market', 'O'), ('talk', 'O'), ('-', 'O'), ('usda', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('august', 'O'), ('22,', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O'), ('and', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('in', 'O'), ('one', 'O'), ('letter,', 'O'), ('a', 'O'), ('black', 'O'), ('soldier', 'O'), ('heading', 'O'), ('south', 'B-LOC'), ('wrote', 'O'), ('his', 'O'), ('wife,', 'O'), ('\"', 'O'), ('though', 'O'), ('great', 'O'), ('is', 'O'), ('the', 'O'), ('present', 'O'), ('national', 'O'), ('difficulties', 'O'), ('yet', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== roberta-base ===\n",
      "\n",
      "architecture:\troberta\n",
      "tokenizer:\tRobertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O'), ('TALK', 'O'), ('-', 'O'), ('USDA', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('August', 'O'), ('22', 'O'), (',', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('A', 'O'), ('chain-smoking', 'O'), ('former', 'O'), ('paratroop', 'O'), ('general', 'O'), ('with', 'O'), ('a', 'O'), ('sharp', 'O'), ('line', 'O'), ('in', 'O'), ('deadpan', 'O'), ('putdowns', 'O'), ('and', 'O'), ('a', 'O'), ('soldier', 'O'), (\"'s\", 'O'), ('knack', 'O'), ('for', 'O'), ('making', 'O'), ('life', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== squeezebert/squeezebert-uncased ===\n",
      "\n",
      "architecture:\tsqueezebert\n",
      "tokenizer:\tSqueezeBertTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('market', 'O'), ('talk', 'O'), ('-', 'O'), ('usda', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('august', 'O'), ('22,', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O'), ('and', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('iran', 'B-LOC'), ('has', 'O'), ('warned', 'O'), ('germany', 'B-LOC'), ('that', 'O'), ('bilateral', 'O'), ('relations', 'O'), ('could', 'O'), ('suffer', 'O'), ('if', 'O'), ('it', 'O'), ('pays', 'O'), ('heed', 'O'), ('to', 'O'), ('the', 'O'), ('testimony', 'O'), ('of', 'O'), ('banisadr,', 'B-PER'), ('an', 'O'), ('architect', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlm-roberta-base ===\n",
      "\n",
      "architecture:\txlm_roberta\n",
      "tokenizer:\tXLMRobertaTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O'), ('TALK', 'O'), ('-', 'O'), ('USDA', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('August', 'O'), ('22', 'O'), (',', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('The', 'B-MISC'), ('Vermonter', 'I-MISC'), (',', 'O'), ('which', 'O'), ('runs', 'O'), ('between', 'O'), ('St.', 'B-LOC'), ('Albans', 'I-LOC'), (',', 'O'), ('Vermont', 'B-LOC'), (',', 'O'), ('near', 'O'), ('the', 'O'), ('Canadian', 'B-MISC'), ('border', 'O'), ('and', 'O'), ('Washington', 'B-LOC'), (',', 'I-LOC'), ('D.C.', 'I-LOC'), (',', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlnet-base-cased ===\n",
      "\n",
      "architecture:\txlnet\n",
      "tokenizer:\tXLNetTokenizerFast\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word / target label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('MARKET', 'O'), ('TALK', 'O'), ('-', 'O'), ('USDA', 'B-ORG'), ('net', 'O'), ('change', 'O'), ('in', 'O'), ('weekly', 'O'), ('export', 'O'), ('commitments', 'O'), ('for', 'O'), ('the', 'O'), ('week', 'O'), ('ended', 'O'), ('August', 'O'), ('22', 'O'), (',', 'O'), ('includes', 'O'), ('old', 'O'), ('crop', 'O')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('The', 'O'), ('state', 'O'), ('of', 'O'), ('health', 'O'), ('of', 'O'), ('Boris', 'B-PER'), ('Yeltsin', 'I-PER'), (',', 'O'), ('who', 'O'), ('had', 'O'), ('two', 'O'), ('heart', 'O'), ('attacks', 'O'), ('last', 'O'), ('year', 'O'), (',', 'O'), ('has', 'O'), ('been', 'O'), ('the', 'O'), ('centre', 'O')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# |hide\n",
    "model_cls = AutoModelForTokenClassification\n",
    "bsz = 2\n",
    "seq_sz = 128\n",
    "\n",
    "test_results = []\n",
    "for model_name in pretrained_model_names:\n",
    "    error = None\n",
    "\n",
    "    print(f\"=== {model_name} ===\\n\")\n",
    "\n",
    "    tok_kwargs = {\"add_prefix_space\": True} if \"deberta\" in model_name else {}\n",
    "\n",
    "    hf_arch, hf_config, hf_tokenizer, hf_model = get_hf_objects(\n",
    "        model_name, model_cls=model_cls, tokenizer_kwargs=tok_kwargs\n",
    "    )\n",
    "    print(f\"architecture:\\t{hf_arch}\\ntokenizer:\\t{type(hf_tokenizer).__name__}\\n\")\n",
    "\n",
    "    # not all architectures include a native pad_token (e.g., gpt2, ctrl, etc...), so we add one here\n",
    "    if hf_tokenizer.pad_token is None:\n",
    "        hf_tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "        hf_config.pad_token_id = hf_tokenizer.get_vocab()[\"<pad>\"]\n",
    "        hf_model.resize_token_embeddings(len(hf_tokenizer))\n",
    "\n",
    "    def get_x(r):\n",
    "        if hf_arch == \"canine\":\n",
    "            return [f\"{word} \" for word in r.tokens]\n",
    "        else:\n",
    "            return r.tokens\n",
    "\n",
    "    batch_tok_tfm = TokenClassBatchTokenizeTransform(\n",
    "        hf_arch,\n",
    "        hf_config,\n",
    "        hf_tokenizer,\n",
    "        hf_model,\n",
    "        padding=\"max_length\",\n",
    "        max_length=seq_sz,\n",
    "    )\n",
    "    blocks = (\n",
    "        TextBlock(\n",
    "            batch_tokenize_tfm=batch_tok_tfm, input_return_type=TokenClassTextInput\n",
    "        ),\n",
    "        TokenCategoryBlock(vocab=labels),\n",
    "    )\n",
    "    dblock = DataBlock(\n",
    "        blocks=blocks,\n",
    "        get_x=get_x,\n",
    "        get_y=ColReader(\"ner_tags\"),\n",
    "        splitter=RandomSplitter(),\n",
    "    )\n",
    "\n",
    "    dls = dblock.dataloaders(conll2003_df, bs=bsz)\n",
    "    b = dls.one_batch()\n",
    "\n",
    "    print(\"*** TESTING DataLoaders ***\\n\")\n",
    "    test_eq(len(b), 2)\n",
    "    test_eq(len(b[0][\"input_ids\"]), bsz)\n",
    "    test_eq(b[0][\"input_ids\"].shape, torch.Size([bsz, seq_sz]))\n",
    "    test_eq(len(b[1]), bsz)\n",
    "\n",
    "    if hasattr(hf_tokenizer, \"add_prefix_space\"):\n",
    "        test_eq(hf_tokenizer.add_prefix_space, True)\n",
    "\n",
    "    test_results.append(\n",
    "        (hf_arch, type(hf_tokenizer).__name__, model_name, \"PASSED\", \"\")\n",
    "    )\n",
    "    dls.show_batch(dataloaders=dls, max_n=2, trunc_at=20)\n",
    "\n",
    "    # except Exception as err:\n",
    "    #     test_results.append((hf_arch, type(hf_tokenizer).__name__, model_name, \"FAILED\", err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arch</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>model_name</th>\n",
       "      <th>result</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert</td>\n",
       "      <td>AlbertTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-albert</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert</td>\n",
       "      <td>BertTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-bert</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>big_bird</td>\n",
       "      <td>BigBirdTokenizerFast</td>\n",
       "      <td>google/bigbird-roberta-base</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>camembert</td>\n",
       "      <td>CamembertTokenizerFast</td>\n",
       "      <td>camembert-base</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>convbert</td>\n",
       "      <td>ConvBertTokenizerFast</td>\n",
       "      <td>YituTech/conv-bert-base</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deberta</td>\n",
       "      <td>DebertaTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-deberta</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bert</td>\n",
       "      <td>BertTokenizerFast</td>\n",
       "      <td>sshleifer/tiny-distilbert-base-cased</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>electra</td>\n",
       "      <td>ElectraTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-electra</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>funnel</td>\n",
       "      <td>FunnelTokenizerFast</td>\n",
       "      <td>huggingface/funnel-small-base</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>GPT2TokenizerFast</td>\n",
       "      <td>sshleifer/tiny-gpt2</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>layoutlm</td>\n",
       "      <td>LayoutLMTokenizerFast</td>\n",
       "      <td>hf-internal-testing/tiny-layoutlm</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>longformer</td>\n",
       "      <td>LongformerTokenizerFast</td>\n",
       "      <td>allenai/longformer-base-4096</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mpnet</td>\n",
       "      <td>MPNetTokenizerFast</td>\n",
       "      <td>microsoft/mpnet-base</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ibert</td>\n",
       "      <td>RobertaTokenizerFast</td>\n",
       "      <td>kssteven/ibert-roberta-base</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mobilebert</td>\n",
       "      <td>MobileBertTokenizerFast</td>\n",
       "      <td>google/mobilebert-uncased</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rembert</td>\n",
       "      <td>RemBertTokenizerFast</td>\n",
       "      <td>google/rembert</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>roformer</td>\n",
       "      <td>RoFormerTokenizerFast</td>\n",
       "      <td>junnyu/roformer_chinese_sim_char_ft_small</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>roberta</td>\n",
       "      <td>RobertaTokenizerFast</td>\n",
       "      <td>roberta-base</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>squeezebert</td>\n",
       "      <td>SqueezeBertTokenizerFast</td>\n",
       "      <td>squeezebert/squeezebert-uncased</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>xlm_roberta</td>\n",
       "      <td>XLMRobertaTokenizerFast</td>\n",
       "      <td>xlm-roberta-base</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>xlnet</td>\n",
       "      <td>XLNetTokenizerFast</td>\n",
       "      <td>xlnet-base-cased</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | echo: false\n",
    "test_results_df = pd.DataFrame(\n",
    "    test_results, columns=[\"arch\", \"tokenizer\", \"model_name\", \"result\", \"error\"]\n",
    ")\n",
    "display_df(test_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modeling.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling.core\n",
    "\n",
    "> This module contains core custom models, loss functions, and a default layer group splitter for use in applying discriminiative learning rates to your huggingface models trained via fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch, nlp\n",
    "from transformers import *\n",
    "\n",
    "from fastai.text.all import *\n",
    "from fastai.callback.hook import _print_shapes\n",
    "\n",
    "from blurr.utils import *\n",
    "from blurr.data.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import pdb\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "#cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f'Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base splitter, model wrapper, and model callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def hf_splitter(m):\n",
    "    \"\"\"Splits the huggingface model based on various model architecture conventions\"\"\"\n",
    "    model = m.hf_model if (hasattr(m, 'hf_model')) else m\n",
    "    root_modules = list(model.named_children())\n",
    "    top_module_name, top_module = root_modules[0]\n",
    "    \n",
    "    groups = L([ m for m_name, m in list(top_module.named_children()) ])\n",
    "    groups += L([ m for m_name, m in root_modules[1:] ])\n",
    "\n",
    "    return groups.map(params).filter(lambda el: len(el) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"hf_splitter\" class=\"doc_header\"><code>hf_splitter</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>hf_splitter</code>(**`m`**)\n",
       "\n",
       "Splits the huggingface model based on various model architecture conventions"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(hf_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_BaseModelWrapper(Module):\n",
    "    def __init__(self, hf_model):\n",
    "        super().__init__()\n",
    "        self.hf_model = hf_model.cuda() if torch.cuda.is_available() else hf_model\n",
    "        \n",
    "        n_fwd_args = self.hf_model.forward.__code__.co_argcount\n",
    "        self.hf_model_fwd_args = self.hf_model.forward.__code__.co_varnames[:n_fwd_args][1:]\n",
    "        \n",
    "    def forward(self, x): \n",
    "        for k in list(x): \n",
    "            if k not in self.hf_model_fwd_args: del x[k]         \n",
    "                \n",
    "        return self.hf_model(**x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `HF_baseModelWrapper` includes some nifty code for just passing in the things your model needs, as not all transformer architectures require/use the same information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_BaseModelCallback(Callback):\n",
    "    def after_pred(self): self.learn.pred = self.pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a `Callback` for handling what is returned from the huggingface model ... \"the huggingface model will return a tuple in outputs, with the actual predictions and some additional activations (should we want to use them is some regularization scheme)\" - from the fastai [Transformer's Tutorial](http://dev.fast.ai/tutorial.transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence classification\n",
    "\n",
    "Below demonstrates how to setup your `blurr` pipeline for a sequence classification task (e.g., a model that requires a single text input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "imdb_df = pd.read_csv(path/'texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.&lt;br /&gt;&lt;br /&gt;But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.&lt;br /&gt;&lt;br /&gt;Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.&lt;br /&gt;&lt;br /&gt;Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie \"Duty, Honor, Country\" are not just mere words blathered from the lips of a high-brassed offic...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  \\\n",
       "0  negative   \n",
       "1  positive   \n",
       "2  negative   \n",
       "3  positive   \n",
       "4  negative   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0                                                                                                                                                                                                    Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!   \n",
       "1  This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.<br /><br />But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...   \n",
       "2  Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.<br /><br />Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...   \n",
       "3  Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.<br /><br />Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie \"Duty, Honor, Country\" are not just mere words blathered from the lips of a high-brassed offic...   \n",
       "4  This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...   \n",
       "\n",
       "   is_valid  \n",
       "0     False  \n",
       "1     False  \n",
       "2     False  \n",
       "3     False  \n",
       "4     False  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#hide_output\n",
    "task = HF_TASKS_AUTO.SequenceClassification\n",
    "\n",
    "pretrained_model_name = \"roberta-base\" # \"distilbert-base-uncased\" \"bert-base-uncased\"\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(pretrained_model_name, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single input\n",
    "blocks = (HF_TextBlock(hf_arch=hf_arch, hf_tokenizer=hf_tokenizer, padding='max_length'), CategoryBlock)\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, \n",
    "                   get_x=ColReader('text'), get_y=ColReader('label'), \n",
    "                   splitter=ColSplitter(col='is_valid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(imdb_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is the kind of movie that my enemies content I watch all the time, but it's not bloody true. I only watch it once in a while to make sure that it's as bad as I first thought it was.&lt;br /&gt;&lt;br /&gt;Some kind of mobsters hijack a Boeing 747. (That, at least, is an improvement over having Boeing hijack a good part of the Pentagon.) The airplane goes down in the Bermuda triangle and sinks pressurized to the bottoms, a kind of post-facto submarine.&lt;br /&gt;&lt;br /&gt;It has one of those all-star casts, the stars either falling or barely above the horizon.&lt;br /&gt;&lt;br /&gt;\"We're on our own!\", says pilot Jack Lemon. He is so right. Except for George Kennedy. He's in all these disaster movies.&lt;br /&gt;&lt;br /&gt;Watch another movie instead. Oh, not \"Airport\" the original. That's no good either. Instead, watch a decent flick about stuck airplanes like \"Flight of the Phoenix.\"</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We'll also add in custom summary methods for blurr learners/models that work with dictionary inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HF_BaseModelWrapper(hf_model)\n",
    "\n",
    "learn = Learner(dls, \n",
    "                model,\n",
    "                opt_func=partial(Adam),\n",
    "                loss_func=CrossEntropyLossFlat(),\n",
    "                metrics=[accuracy],\n",
    "                cbs=[HF_BaseModelCallback],\n",
    "                splitter=hf_splitter)\n",
    "\n",
    "learn.create_opt()             # -> will create your layer groups based on your \"splitter\" function\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.to_fp16()` requires a GPU so had to remove for tests to run on github.  Let's check that we can get predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2033, 0.0170],\n",
       "         [0.2102, 0.0190],\n",
       "         [0.1887, 0.0251],\n",
       "         [0.1941, 0.0119]], device='cuda:1', grad_fn=<AddmmBackward>),)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model(b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def blurr_module_summary(learn, *xb):\n",
    "    \"Print a summary of `model` using `xb`\"\n",
    "    infos = layer_info(learn, *xb)\n",
    "    n,bs = 64,find_bs(xb)\n",
    "    inp_sz = _print_shapes(apply(lambda x:x.shape,  xb[0]['input_ids']), bs)\n",
    "    res = f\"{learn.model.__class__.__name__} (Input shape: {inp_sz})\\n\"\n",
    "    res += \"=\" * n + \"\\n\"\n",
    "    res += f\"{'Layer (type)':<20} {'Output Shape':<20} {'Param #':<10} {'Trainable':<10}\\n\"\n",
    "    res += \"=\" * n + \"\\n\"\n",
    "    ps,trn_ps = 0,0\n",
    "    infos = [o for o in infos if o is not None] #see comment in previous cell\n",
    "    for typ,np,trn,sz in infos:\n",
    "        if sz is None: continue\n",
    "        ps += np\n",
    "        if trn: trn_ps += np\n",
    "        res += f\"{typ:<20} {_print_shapes(sz, bs)[:19]:<20} {np:<10,} {str(trn):<10}\\n\"\n",
    "        res += \"_\" * n + \"\\n\"\n",
    "    res += f\"\\nTotal params: {ps:,}\\n\"\n",
    "    res += f\"Total trainable params: {trn_ps:,}\\n\"\n",
    "    res += f\"Total non-trainable params: {ps - trn_ps:,}\\n\\n\"\n",
    "    return PrettyString(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def blurr_summary(self:Learner):\n",
    "    \"Print a summary of the model, optimizer and loss function.\"\n",
    "    xb = self.dls.train.one_batch()[:self.dls.train.n_inp]\n",
    "    res = blurr_module_summary(self, *xb)\n",
    "    res += f\"Optimizer used: {self.opt_func}\\nLoss function: {self.loss_func}\\n\\n\"\n",
    "    if self.opt is not None:\n",
    "        res += f\"Model \" + (\"unfrozen\\n\\n\" if self.opt.frozen_idx==0 else f\"frozen up to parameter group #{self.opt.frozen_idx}\\n\\n\")\n",
    "    res += \"Callbacks:\\n\" + '\\n'.join(f\"  - {cb}\" for cb in sort_by_run(self.cbs))\n",
    "    return PrettyString(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to create our own `summary` methods above because fastai only works where things are represented by a *single tensor*.  But in the case of huggingface transformers, a *single* sequence is represented by *multiple tensors* (in a dictionary).  \n",
    "\n",
    "The change to make this work is so minor I think that the fastai library can/will hopefully be updated to support this use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.blurr_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(learn.opt.param_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(lr_min=2.7542287170945203e-07, lr_steep=0.00363078061491251)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.lr_find(suggestions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.473018</td>\n",
       "      <td>0.309353</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.262183</td>\n",
       "      <td>0.377667</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.134383</td>\n",
       "      <td>0.259639</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.fit_one_cycle(3, lr_max=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing results\n",
    "\n",
    "And here we creat a @typedispatched impelmentation of `Learner.show_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_results(x:HF_BaseInput, y, samples, outs, learner=None, ctxs=None, max_n=6, **kwargs):        \n",
    "    \n",
    "    if ctxs is None: ctxs = get_empty_df(min(len(samples), max_n))\n",
    "    ctxs = show_batch[object](x, y, samples, max_n=max_n, ctxs=ctxs, **kwargs)\n",
    "    \n",
    "    n_preds_per_input = len(outs[0])\n",
    "    if (n_preds_per_input == 1): \n",
    "        for i,ctx in enumerate(ctxs): ctx['target'] = outs[i][0]\n",
    "    else:\n",
    "        for pred_idx in range(n_preds_per_input):\n",
    "            for i,ctx in enumerate(ctxs):  ctx[f'target{pred_idx+1}'] = outs[i][pred_idx]\n",
    "\n",
    "    display_df(pd.DataFrame(ctxs))\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br /&gt;Stanley Holloway is excellent as the minor city politician who suddenly finds himself leading one of the world's tiniest nations. Dame Margaret Rutherford is a delight as the history professor who sides with Pimlico. Others in the stand-out cast include Hermione Baddeley, Paul Duplis, Naughton Wayne, Basil Radford &amp; Sir Michael Hordern.&lt;br /&gt;&lt;br /&gt;Welcome to Burgundy!</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>...that seem to be fooling people into seeing qualities in this film that are just not there.&lt;br /&gt;&lt;br /&gt;Near Dark covered the same territory but with much more class, and action.&lt;br /&gt;&lt;br /&gt;why the script kept their 'big secret' so long was a total mystery to me - I guessed it at the breakfast scene at the start of the film. By the time it was revealed to the viewer it was just a case of 'big deal, tell me something I don't know.'&lt;br /&gt;&lt;br /&gt;I found this to be pointless movie that may have challenged the genre conventions, unfortunately those conventions are that horror films are tense and packed with genuine sense of horror. This was woeful</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def blurr_predict(self:Learner, item, rm_type_tfms=None, with_input=False):\n",
    "    dl = self.dls.test_dl([item], rm_type_tfms=rm_type_tfms, num_workers=0)\n",
    "    \n",
    "    # this is where we have to change things up since a blurr \"input\" is represented by a dictionary of\n",
    "    # tensors (input_ids, attention_mask, token_type_ids, etc...) and not a single tensor (which fastai assumes\n",
    "    # in a number of places)\n",
    "    b = dl.one_batch()\n",
    "    inp = b[0]\n",
    "    preds, _, dec_preds = self.get_preds(dl=dl, with_input=False, with_decoded=True)\n",
    "    \n",
    "    i = getattr(self.dls, 'n_inp', -1)\n",
    "    inp = (inp,) if i==1 else tuplify(inp)\n",
    "    dec = self.dls.decode_batch(inp + tuplify(dec_preds))[0]\n",
    "    dec_inp,dec_targ = map(detuplify, [dec[:i],dec[i:]])\n",
    "    res = dec_targ,dec_preds[0],preds[0]\n",
    "    if with_input: res = (dec_inp,) + res\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as with `summary`, we need to replace fastai's `Learner.predict` method with the one above which is able to work with inputs that are represented by multiple tensors included in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('positive', tensor(1), tensor([0.0542, 0.9458]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.blurr_predict('I really liked the movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.172892</td>\n",
       "      <td>0.529065</td>\n",
       "      <td>0.845000</td>\n",
       "      <td>00:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.100963</td>\n",
       "      <td>0.410971</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>00:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.071700</td>\n",
       "      <td>0.283861</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>00:52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.fit_one_cycle(3, lr_max=slice(1e-6, 1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall.&lt;br /&gt;&lt;br /&gt;Stanley Holloway is excellent as the minor city politician who suddenly finds himself leading one of the world's tiniest nations. Dame Margaret Rutherford is a delight as the history professor who sides with Pimlico. Others in the stand-out cast include Hermione Baddeley, Paul Duplis, Naughton Wayne, Basil Radford &amp; Sir Michael Hordern.&lt;br /&gt;&lt;br /&gt;Welcome to Burgundy!</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>well \"Wayne's World\" is long gone and the years since then have been hard for snl off-shoot movies. from such cinematic offal as \"It's Pat\" to the recent 80 minute yawn, \"A Night at the Roxbury,\" many have, no doubt, lost faith that any other snl skit will ever make a successful transition to the silver screen. well fear not because Tim Meadows comes through in spades. the well-written plot maintains audience interest until the very end and while it remains true to the Leon Phelps character introduced in the five minute skit, the storyline allows the character to develop. the humor (consisting largely of sex jokes) is fresh and interesting and made me laugh harder than i have in any movie in recent memory. its a just great time if you don't feel like taking yourself too seriously. Tiffany-Amber Thiessen of \"Saved by the Bell\" fame, makes an appearance in the film and looks incredible. finally Billy Dee Williams, reliving his Colt 45 days, gives the movie a touch of class. and for those out there who are mindless movie quoters like myself, you will find this movie to be eminently quotable, \"ooh, it's a lady!\"</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('positive', tensor(1), tensor([0.1165, 0.8835]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.blurr_predict(\"This was a really good movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('negative', tensor(0), tensor([0.9965, 0.0035]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.blurr_predict(\"Acting was so bad it was almost funny.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export(fname='seq_class_learn_export.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('negative', tensor(0), tensor([0.9894, 0.0106]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_learn = load_learner(fname='seq_class_learn_export.pkl')\n",
    "inf_learn.blurr_predict(\"This movie should not be seen by anyone!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "The tests below to ensure the core training code above works for **all** pretrained sequence classification models available in huggingface.  These tests are excluded from the CI workflow because of how long they would take to run and the amount of data that would be required to download.\n",
    "\n",
    "**Note**: Feel free to modify the code below to test whatever pretrained classification models you are working with ... and if any of your pretrained sequence classification models fail, please submit a github issue *(or a PR if you'd like to fix it yourself)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: del learn; torch.cuda.empty_cache()\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[transformers.modeling_albert.AlbertForSequenceClassification,\n",
       " transformers.modeling_auto.AutoModelForSequenceClassification,\n",
       " transformers.modeling_bart.BartForSequenceClassification,\n",
       " transformers.modeling_bert.BertForSequenceClassification,\n",
       " transformers.modeling_camembert.CamembertForSequenceClassification,\n",
       " transformers.modeling_distilbert.DistilBertForSequenceClassification,\n",
       " transformers.modeling_electra.ElectraForSequenceClassification,\n",
       " transformers.modeling_flaubert.FlaubertForSequenceClassification,\n",
       " transformers.modeling_longformer.LongformerForSequenceClassification,\n",
       " transformers.modeling_mobilebert.MobileBertForSequenceClassification,\n",
       " transformers.modeling_roberta.RobertaForSequenceClassification,\n",
       " transformers.modeling_xlm.XLMForSequenceClassification,\n",
       " transformers.modeling_xlm_roberta.XLMRobertaForSequenceClassification,\n",
       " transformers.modeling_xlnet.XLNetForSequenceClassification]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BLURR_MODEL_HELPER.get_models(task='SequenceClassification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_names = [\n",
    "    'albert-base-v1',\n",
    "    'facebook/bart-base',\n",
    "    'bert-base-uncased',\n",
    "    'camembert-base',\n",
    "    'distilbert-base-uncased',\n",
    "    'monologg/electra-small-finetuned-imdb',\n",
    "    'flaubert/flaubert_small_cased', \n",
    "    'allenai/longformer-base-4096',\n",
    "    'google/mobilebert-uncased',\n",
    "    'roberta-base',\n",
    "    'xlm-mlm-en-2048',\n",
    "    'xlm-roberta-base',\n",
    "    'xlnet-base-cased'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "\n",
    "model_path = Path('models')\n",
    "imdb_df = pd.read_csv(path/'texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== albert-base-v1 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v1 were not used when initializing AlbertForSequenceClassification: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\talbert\n",
      "tokenizer:\tAlbertTokenizer\n",
      "model:\t\tAlbertForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.654113</td>\n",
       "      <td>0.669488</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>00:15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this very funny british comedy shows what might happen if a section of london, in this case pimlico, were to declare itself independent from the rest of the uk and its laws, taxes &amp; post-war restrictions. merry mayhem is what would happen.br /br /the explosion of a wartime bomb leads to the discovery of ancient documents which show that pimlico was ceded to the duchy of burgundy centuries ago, a small historical footnote long since forgotten. to the new burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from whitehall.br</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>december holiday specials, like the original frosty, ought to be richly-produced with quality music and a wholesome, yet lighthearted storyline. they should have a touch of the mystical magic of the holidays. basically, they should look, sound, and feel...well, \"special\" and they should have a decent and appropriate december holiday subtext.br /br /so when i saw legend of frosty the snowman in the tv listings, i got my kids (6 and 8) pumped up for it by telling them the story of the original frosty</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== facebook/bart-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/bart-base were not used when initializing BartForSequenceClassification: ['final_logits_bias']\n",
      "- This IS expected if you are initializing BartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.weight', 'classification_head.dense.bias', 'classification_head.out_proj.weight', 'classification_head.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tbart\n",
      "tokenizer:\tBartTokenizer\n",
      "model:\t\tBartForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.301635</td>\n",
       "      <td>0.460463</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A man readjusts to life alone after 45 years of marriage. He also has to solve the problem of the family milch cow, Tulip, which refuses to allow itself to be milked. Until, that is, he visualizes his wife who was the one who used to milk Tulip.&lt;br /&gt;&lt;br /&gt;Tulip is based on a real story told in Griffith's family, of her grandparents' generation. The film is a nostalgic look back at a disappearing way of life, one where people still felt some sense of responsibility for each other, set in the lush green Victorian (the Australian state,</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== bert-base-uncased ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tbert\n",
      "tokenizer:\tBertTokenizer\n",
      "model:\t\tBertForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.596631</td>\n",
       "      <td>0.597448</td>\n",
       "      <td>0.665000</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this very funny british comedy shows what might happen if a section of london, in this case pimlico, were to declare itself independent from the rest of the uk and its laws, taxes &amp; post - war restrictions. merry mayhem is what would happen. &lt; br / &gt; &lt; br / &gt; the explosion of a wartime bomb leads to the discovery of ancient documents which show that pimlico was ceded to the duchy of burgundy centuries ago, a small historical footnote long since forgotten. to the new burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from whitehall. &lt;</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>well \" wayne's world \" is long gone and the years since then have been hard for snl off - shoot movies. from such cinematic offal as \" it's pat \" to the recent 80 minute yawn, \" a night at the roxbury, \" many have, no doubt, lost faith that any other snl skit will ever make a successful transition to the silver screen. well fear not because tim meadows comes through in spades. the well - written plot maintains audience interest until the very end and while it remains true to the leon phelps character introduced in the five minute skit, the storyline</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== camembert-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tcamembert\n",
      "tokenizer:\tCamembertTokenizer\n",
      "model:\t\tCamembertForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.689090</td>\n",
       "      <td>0.692987</td>\n",
       "      <td>0.535000</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forget the campy'religious' movies that have monopolized the television/film market... this movie has a real feel to it. While it may be deemed as a movie that has cheap emotional draws, it also has that message of forgiveness, and overall good morals. However, I did not like the lighting in this movie... for a movie dealing with such subject matter, it was too bright. I felt it took away from the overall</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== distilbert-base-uncased ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tdistilbert\n",
      "tokenizer:\tDistilBertTokenizer\n",
      "model:\t\tDistilBertForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.514264</td>\n",
       "      <td>0.523394</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this very funny british comedy shows what might happen if a section of london, in this case pimlico, were to declare itself independent from the rest of the uk and its laws, taxes &amp; post - war restrictions. merry mayhem is what would happen. &lt; br / &gt; &lt; br / &gt; the explosion of a wartime bomb leads to the discovery of ancient documents which show that pimlico was ceded to the duchy of burgundy centuries ago, a small historical footnote long since forgotten. to the new burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from whitehall. &lt;</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>like tarzan the ape man ( 1932 ), only more so. there's more of everything, more animals, more varied african tribes, and scenes in which the thought must be, if this was good with three or four lions, forty would be better. tarzan wrestles with the crocodile machine spins in the water like a rolling pin, around and around, jaws flapping. tarzan can kill it with his ubiquitous knife if the blasted saurian would hold still. tarzan kills lions and rhinos and a steadily increasing number of animals. his friends are real chimps, people wearing larger ape costumes, and elephants. in fact</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== monologg/electra-small-finetuned-imdb ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/electra-small-finetuned-imdb were not used when initializing ElectraForSequenceClassification: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/electra-small-finetuned-imdb and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\telectra\n",
      "tokenizer:\tElectraTokenizer\n",
      "model:\t\tElectraForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.227259</td>\n",
       "      <td>0.316988</td>\n",
       "      <td>0.865000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this very funny british comedy shows what might happen if a section of london, in this case pimlico, were to declare itself independent from the rest of the uk and its laws, taxes &amp; post - war restrictions. merry mayhem is what would happen. &lt; br / &gt; &lt; br / &gt; the explosion of a wartime bomb leads to the discovery of ancient documents which show that pimlico was ceded to the duchy of burgundy centuries ago, a small historical footnote long since forgotten. to the new burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from whitehall. &lt;</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is a cgi animated film based upon a french 2d animated series. the series ran briefly on cartoon network, but its run was so brief that its inclusion as one of the potential oscar nominees for best animated film for this year left most people i know going \" huh? \" this is the story of lian - chu, the kind heart muscle, and gwizdo, the brains of the operation, who along with hector their fire farting dragon, he's more like a dog. travel the world offering up their services as dragon hunters but never getting paid. into their lives comes zoe, the fairy tale loving</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== flaubert/flaubert_small_cased ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at flaubert/flaubert_small_cased were not used when initializing FlaubertForSequenceClassification: ['pred_layer.proj.bias', 'pred_layer.proj.weight']\n",
      "- This IS expected if you are initializing FlaubertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing FlaubertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FlaubertForSequenceClassification were not initialized from the model checkpoint at flaubert/flaubert_small_cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tflaubert\n",
      "tokenizer:\tFlaubertTokenizer\n",
      "model:\t\tFlaubertForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.731140</td>\n",
       "      <td>0.734240</td>\n",
       "      <td>0.535000</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen. &lt; br / &gt; &lt; br / &gt; The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since for</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This movie surprised me in a good way. From the box I got the impression that it was an action thriller but it was too funny to be a thriller, even though it was somewhat exciting. &lt; br / &gt; &lt; br / &gt; There' s a lot of nice one-liners and funny situations in this movie and James Belushi was born to do Bill Manucci, he does a great job. The rest of the cast ain' t half-bad either and especially Timothy Dalton is a treat. &lt; br /</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== allenai/longformer-base-4096 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tlongformer\n",
      "tokenizer:\tLongformerTokenizer\n",
      "model:\t\tLongformerForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.410335</td>\n",
       "      <td>0.462132</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>02:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Loulou' delights in the same way an expensive, high quality French wine does. It leaves you with a very fine aftertaste.&lt;br /&gt;&lt;br /&gt;'Loulou's theme isn't new. The film doesn't carry an original plot either. Its colored picturing shows fine, but not extraordinary. Its setting is serious. Its elegant styling never and nowhere puts any weight on your mind.&lt;br /&gt;&lt;br /&gt;Whatever one further may say about 'Loulou', it's beyond doubt that this very French film stands out for its excellent acting. The three leads convincingly reflect all numerous doubts</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/mobilebert-uncased ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/mobilebert-uncased were not used when initializing MobileBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing MobileBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing MobileBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tmobilebert\n",
      "tokenizer:\tMobileBertTokenizer\n",
      "model:\t\tMobileBertForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>519718.968750</td>\n",
       "      <td>124175.046875</td>\n",
       "      <td>0.465000</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this very funny british comedy shows what might happen if a section of london, in this case pimlico, were to declare itself independent from the rest of the uk and its laws, taxes &amp; post - war restrictions. merry mayhem is what would happen. &lt; br / &gt; &lt; br / &gt; the explosion of a wartime bomb leads to the discovery of ancient documents which show that pimlico was ceded to the duchy of burgundy centuries ago, a small historical footnote long since forgotten. to the new burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from whitehall. &lt;</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this one is a little better than the first one. it still relies on a lot of its humor which basically keeps saying that the old bond movies were not realistic. that wears thin after so many parodies. the girls were more interesting in this one. &lt; br / &gt; &lt; br / &gt; there is a tremendous amount of total gross out humor. hopefully one day real comedy will come back.</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== roberta-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\troberta\n",
      "tokenizer:\tRobertaTokenizer\n",
      "model:\t\tRobertaForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.374360</td>\n",
       "      <td>0.450064</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from Whitehall</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For me the only reason for having a look at this remake was to see how bad and funny it could be. There was no doubt about it being funny and bad, because I had seen \"Voyna i mir\" (1968). Shall we begin? Here we go...&lt;br /&gt;&lt;br /&gt;Robert Dornhelm &amp; Brendan Donnison's Pierre Bezukhov - a lean fellow that lacks the depth of the original; Robert Dornhelm &amp; Brendan Donnison's Natasha Rostova - a scarecrow, her image can cause insomnia; Robert Dornhelm &amp; Brendan Donnison's Andrej Bolkonsky</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlm-mlm-en-2048 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-mlm-en-2048 were not used when initializing XLMForSequenceClassification: ['pred_layer.proj.weight', 'pred_layer.proj.bias']\n",
      "- This IS expected if you are initializing XLMForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLMForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMForSequenceClassification were not initialized from the model checkpoint at xlm-mlm-en-2048 and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\txlm\n",
      "tokenizer:\tXLMTokenizer\n",
      "model:\t\tXLMForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.846128</td>\n",
       "      <td>0.687620</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>00:47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this very funny british comedy shows what might happen if a section of london, in this case pimlico, were to declare itself independent from the rest of the uk and its laws, taxes &amp; post-war restrictions. merry mayhem is what would happen. &lt; br / &gt; &lt; br / &gt; the explosion of a wartime bomb leads to the discovery of ancient documents which show that pimlico was ceded to the duchy of burgundy centuries ago, a small historical footnote long since forgotten. to the new burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from whitehall. &lt; br /</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>one of the best tarzan films is also one of its most action packed ( and graphic ). &lt; br / &gt; &lt; br / &gt; picking up a year or so after tarzan the ape man, niel hamilton's holt has asked a rich friend to finance a safari back to the elephants graveyard to collect ivory. his friend arrives also carrying dresses and perfumes that holt hopes to use to win jane back from tarzan. before they can leave holt finds his map stolen and it becomes a mad dash to try and capture a competing expedition. when they finally over take the thieves they find the whole party dead and themselves surrounded</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlm-roberta-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\txlm_roberta\n",
      "tokenizer:\tXLMRobertaTokenizer\n",
      "model:\t\tXLMRobertaForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.579998</td>\n",
       "      <td>0.596209</td>\n",
       "      <td>0.715000</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How viewers react to this new \"adaption\" of Shirley Jackson's book, which was promoted as NOT being a remake of the original 1963 movie (true enough), will be based, I suspect, on the following: those who were big fans of either the book or original movie are not going to think much of this one...and those who have never been exposed to either, and who are big fans of Hollywood's current trend towards \"special effects\" being the first and last word in how \"good\" a film is, are going to love it.&lt;br /&gt;&lt;br /&gt;Th</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlnet-base-cased ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\txlnet\n",
      "tokenizer:\tXLNetTokenizer\n",
      "model:\t\tXLNetForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/transformers/modeling_xlnet.py:283: UserWarning: Mixed memory format inputs detected while calling the operator. The operator will output contiguous tensor even if some of the inputs are in channels_last format. (Triggered internally at  /pytorch/aten/src/ATen/native/TensorIterator.cpp:918.)\n",
      "  attn_score = (ac + bd + ef) * self.scale\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.432855</td>\n",
       "      <td>0.450354</td>\n",
       "      <td>0.845000</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This very funny British comedy shows what might happen if a section of London, in this case Pimlico, were to declare itself independent from the rest of the UK and its laws, taxes &amp; post-war restrictions. Merry mayhem is what would happen.&lt;br /&gt;&lt;br /&gt;The explosion of a wartime bomb leads to the discovery of ancient documents which show that Pimlico was ceded to the Duchy of Burgundy centuries ago, a small historical footnote long since forgotten. To the new Burgundians, however, this is an unexpected opportunity to live as they please, free from any interference from</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A compelling, honest, daring, and unforgettable psychological horror film that touches on the painful experiences of pain caused by rape - \"Descent\" is a film that went under-the-radar due to its lack of distribution because, frankly, the film is so brutal in its depictions, that if it had been released theatrically, it may have met itself to some strong biased hate.&lt;br /&gt;&lt;br /&gt;The film deserves to be discovered for, not only its dark themes, and not only for its amazing direction and authentic style - but most of all for its performances. Chad</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "#hide_output\n",
    "task = HF_TASKS_AUTO.SequenceClassification\n",
    "bsz = 2\n",
    "\n",
    "test_results = []\n",
    "for model_name in pretrained_model_names:\n",
    "    error=None\n",
    "    \n",
    "    print(f'=== {model_name} ===\\n')\n",
    "    \n",
    "    hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(model_name, \n",
    "                                                                                   task=task, \n",
    "                                                                                   config_kwargs={'num_labels': 2})\n",
    "    \n",
    "    print(f'architecture:\\t{hf_arch}\\ntokenizer:\\t{type(hf_tokenizer).__name__}\\nmodel:\\t\\t{type(hf_model).__name__}\\n')\n",
    "\n",
    "    blocks = (HF_TextBlock(hf_arch=hf_arch, hf_tokenizer=hf_tokenizer, max_length=128, padding='max_length'), \n",
    "              CategoryBlock)\n",
    "\n",
    "    dblock = DataBlock(blocks=blocks, \n",
    "                       get_x=ColReader('text'), \n",
    "                       get_y=ColReader('label'), \n",
    "                       splitter=ColSplitter(col='is_valid'))\n",
    "    \n",
    "    dls = dblock.dataloaders(imdb_df, bs=bsz)\n",
    "    \n",
    "    model = HF_BaseModelWrapper(hf_model)\n",
    "    learn = Learner(dls, \n",
    "                    model,\n",
    "                    opt_func=partial(Adam),\n",
    "                    loss_func=CrossEntropyLossFlat(),\n",
    "                    metrics=[accuracy],\n",
    "                    cbs=[HF_BaseModelCallback],\n",
    "                    splitter=hf_splitter)\n",
    "\n",
    "    learn.create_opt()             # -> will create your layer groups based on your \"splitter\" function\n",
    "    learn.freeze()\n",
    "    \n",
    "    b = dls.one_batch()\n",
    "    \n",
    "    try:\n",
    "        print('*** TESTING DataLoaders ***')\n",
    "        test_eq(len(b), bsz)\n",
    "        test_eq(len(b[0]['input_ids']), bsz)\n",
    "        test_eq(b[0]['input_ids'].shape, torch.Size([bsz, 128]))\n",
    "        test_eq(len(b[1]), bsz)\n",
    "\n",
    "        print('*** TESTING One pass through the model ***')\n",
    "        preds = learn.model(b[0])\n",
    "        test_eq(len(preds[0]), bsz)\n",
    "        test_eq(preds[0].shape, torch.Size([bsz, 2]))\n",
    "\n",
    "        print('*** TESTING Training/Results ***')\n",
    "        learn.fit_one_cycle(1, lr_max=1e-3)\n",
    "\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, type(hf_model).__name__, 'PASSED', ''))\n",
    "        learn.show_results(max_n=2)\n",
    "    except Exception as err:\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, type(hf_model).__name__, 'FAILED', err))\n",
    "    finally:\n",
    "        # cleanup\n",
    "        del learn; torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arch</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>model</th>\n",
       "      <th>result</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert</td>\n",
       "      <td>AlbertTokenizer</td>\n",
       "      <td>AlbertForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bart</td>\n",
       "      <td>BartTokenizer</td>\n",
       "      <td>BartForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bert</td>\n",
       "      <td>BertTokenizer</td>\n",
       "      <td>BertForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>camembert</td>\n",
       "      <td>CamembertTokenizer</td>\n",
       "      <td>CamembertForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>distilbert</td>\n",
       "      <td>DistilBertTokenizer</td>\n",
       "      <td>DistilBertForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>electra</td>\n",
       "      <td>ElectraTokenizer</td>\n",
       "      <td>ElectraForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>flaubert</td>\n",
       "      <td>FlaubertTokenizer</td>\n",
       "      <td>FlaubertForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>longformer</td>\n",
       "      <td>LongformerTokenizer</td>\n",
       "      <td>LongformerForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mobilebert</td>\n",
       "      <td>MobileBertTokenizer</td>\n",
       "      <td>MobileBertForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>roberta</td>\n",
       "      <td>RobertaTokenizer</td>\n",
       "      <td>RobertaForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>xlm</td>\n",
       "      <td>XLMTokenizer</td>\n",
       "      <td>XLMForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>xlm_roberta</td>\n",
       "      <td>XLMRobertaTokenizer</td>\n",
       "      <td>XLMRobertaForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>xlnet</td>\n",
       "      <td>XLNetTokenizer</td>\n",
       "      <td>XLNetForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "#hide_input\n",
    "test_results_df = pd.DataFrame(test_results, columns=['arch', 'tokenizer', 'model', 'result', 'error'])\n",
    "display_df(test_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Multi-label classification\n",
    "\n",
    "Below demonstrates how to setup your `blurr` pipeline for a multi-label classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18049"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creates a dataset with the first 10% of training set\n",
    "raw_data = nlp.load_dataset('civil_comments', split='train[:1%]') \n",
    "len(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>sexual_explicit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is such an urgent design problem; kudos to you for taking it on. Very impressive!</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is this something I'll be able to install on my site? When will you be releasing it?</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.87234</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                 text  \\\n",
       "0               This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!   \n",
       "1  Thank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!   \n",
       "2                              This is such an urgent design problem; kudos to you for taking it on. Very impressive!   \n",
       "3                                Is this something I'll be able to install on my site? When will you be releasing it?   \n",
       "4                                                                                haha you guys are a bunch of losers.   \n",
       "\n",
       "   toxicity  severe_toxicity  obscene  threat   insult  identity_attack  \\\n",
       "0  0.000000         0.000000      0.0     0.0  0.00000         0.000000   \n",
       "1  0.000000         0.000000      0.0     0.0  0.00000         0.000000   \n",
       "2  0.000000         0.000000      0.0     0.0  0.00000         0.000000   \n",
       "3  0.000000         0.000000      0.0     0.0  0.00000         0.000000   \n",
       "4  0.893617         0.021277      0.0     0.0  0.87234         0.021277   \n",
       "\n",
       "   sexual_explicit  \n",
       "0              0.0  \n",
       "1              0.0  \n",
       "2              0.0  \n",
       "3              0.0  \n",
       "4              0.0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_df = pd.DataFrame(raw_data, columns=list(raw_data.features.keys()))\n",
    "toxic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['severe_toxicity',\n",
       " 'obscene',\n",
       " 'threat',\n",
       " 'insult',\n",
       " 'identity_attack',\n",
       " 'sexual_explicit']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_cols = list(toxic_df.columns[2:]); lbl_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>sexual_explicit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is such an urgent design problem; kudos to you for taking it on. Very impressive!</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is this something I'll be able to install on my site? When will you be releasing it?</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                 text  \\\n",
       "0               This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!   \n",
       "1  Thank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!   \n",
       "2                              This is such an urgent design problem; kudos to you for taking it on. Very impressive!   \n",
       "3                                Is this something I'll be able to install on my site? When will you be releasing it?   \n",
       "4                                                                                haha you guys are a bunch of losers.   \n",
       "\n",
       "   toxicity  severe_toxicity  obscene  threat  insult  identity_attack  \\\n",
       "0  0.000000                0        0       0       0                0   \n",
       "1  0.000000                0        0       0       0                0   \n",
       "2  0.000000                0        0       0       0                0   \n",
       "3  0.000000                0        0       0       0                0   \n",
       "4  0.893617                0        0       0       1                0   \n",
       "\n",
       "   sexual_explicit  \n",
       "0                0  \n",
       "1                0  \n",
       "2                0  \n",
       "3                0  \n",
       "4                0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_df = toxic_df.round({col: 0 for col in lbl_cols})\n",
    "toxic_df = toxic_df.convert_dtypes()\n",
    "\n",
    "toxic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#hide_output\n",
    "task = HF_TASKS_AUTO.SequenceClassification\n",
    "\n",
    "pretrained_model_name = \"roberta-base\" # \"distilbert-base-uncased\" \"bert-base-uncased\"\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "config.num_labels = len(lbl_cols)\n",
    "\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(pretrained_model_name, \n",
    "                                                                               task=task, \n",
    "                                                                               config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we have to configure the `num_labels` to the number of labels we are predicting. Given that our labels are already encoded, we use a `MultiCategoryBlock` with `encoded=True` and `vocab` equal to the columns with our 1's and 0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single input\n",
    "blocks = (\n",
    "    HF_TextBlock(hf_arch=hf_arch, hf_tokenizer=hf_tokenizer), \n",
    "    MultiCategoryBlock(encoded=True, vocab=lbl_cols)\n",
    ")\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, \n",
    "                   get_x=ColReader('text'), get_y=ColReader(lbl_cols), \n",
    "                   splitter=RandomSplitter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(toxic_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([4, 150]), torch.Size([4, 6]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "len(b), b[0]['input_ids'].shape, b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>None</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>three one-hundreths of one percent of the states employment tax paid by employers during three months of every odd-numbered year   \\n\\nKey phrase:   \"paid by employers\".  \\n\\nThey have to pay all their \"employment tax\" every month or the doors  to their business are  chained shut until they do ( I saw it done in Florence years ago.).   The State of Oregon transfers the \"three one-hundreths of one percent\" from the regular Unemployment account to this BOLI account.\\n\\nReading comprehension problem or evidence of anti-business bias?    Who would believe such a thing?</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I stand corrected. Thank you.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HF_BaseModelWrapper(hf_model)\n",
    "\n",
    "learn = Learner(dls, \n",
    "                model,\n",
    "                opt_func=partial(Adam),\n",
    "                loss_func=BCEWithLogitsLossFlat(),\n",
    "                metrics=[partial(accuracy_multi, thresh=0.2)],\n",
    "                cbs=[HF_BaseModelCallback],\n",
    "                splitter=hf_splitter)\n",
    "\n",
    "learn.loss_func.thresh = 0.2\n",
    "learn.create_opt()             # -> will create your layer groups based on your \"splitter\" function\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're doing multi-label classification, we adjust our loss function to use binary cross-entropy and our metrics to use the multi-label friendly version of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(b[0])\n",
    "preds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(lr_min=0.07585775852203369, lr_steep=0.0014454397605732083)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.lr_find(suggestions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_multi</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.050442</td>\n",
       "      <td>0.044143</td>\n",
       "      <td>0.993349</td>\n",
       "      <td>03:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.033605</td>\n",
       "      <td>0.035203</td>\n",
       "      <td>0.993349</td>\n",
       "      <td>03:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.034579</td>\n",
       "      <td>0.034765</td>\n",
       "      <td>0.993349</td>\n",
       "      <td>03:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.fit_one_cycle(3, lr_max=3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>None</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To address your question, I don't think Hillary is an honest person, and I would say that Obama is largely honest, although things don't always turn out how he intends, which could certainly come across as dishonest. To the greater point about Bernie, and socialism in general, is that it's complicated. Socialism isn't a universal failure. There are European countries where it has worked, and there are asian countries (such as china) where it has, largely, worked. The answer isn't a blanket statement of \"does socialism work or not,\" but rather one about what is the proper role of government. Unfettered capitalism has largely proven to be ineffective in a number of areas, so we as a society have decided to regulate it. I'm about as much of a capitalist as you'll find, but I think we can all point out shortcomings with the current system. Moving the top tax rate from 40% to 50% isn't the same thing as being \"Socialist\" and we are being intellectually dishonest when we portray it as such.</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Everyone can move to the new city hall, and hang-out there when its finished.</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.loss_func.thresh = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((#1) ['severe_toxicity'],\n",
       " tensor([False, False, False,  True, False, False]),\n",
       " tensor([1.2142e-06, 3.8992e-03, 6.4931e-05, 2.7668e-02, 7.8780e-04, 7.3750e-04]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment = \"\"\"\n",
    "Those damned affluent white people should only eat their own food, like cod cakes and boiled potatoes. \n",
    "No enchiladas for them!\n",
    "\"\"\"\n",
    "learn.blurr_predict(comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_data-core.ipynb.\n",
      "Converted 01a_data-token-classification.ipynb.\n",
      "Converted 01b_data-question-answering.ipynb.\n",
      "Converted 01e_data-summarization.ipynb.\n",
      "Converted 01z_data-language-modeling.ipynb.\n",
      "Converted 02_modeling-core.ipynb.\n",
      "Converted 02a_modeling-token-classification.ipynb.\n",
      "Converted 02b_modeling-question-answering.ipynb.\n",
      "Converted 02e_modeling-summarization.ipynb.\n",
      "Converted 02z_modeling-language-modeling.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

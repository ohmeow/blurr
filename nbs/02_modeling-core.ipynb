{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modeling.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling.core\n",
    "\n",
    "> This module contains core custom models, loss functions, and a default layer group splitter for use in applying discriminiative learning rates to your huggingface models trained via fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch, nlp\n",
    "from transformers import *\n",
    "\n",
    "from fastai.text.all import *\n",
    "from fastai.callback.hook import _print_shapes\n",
    "\n",
    "from blurr.utils import *\n",
    "from blurr.data.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pytorch 1.6.0\n",
      "Using fastai 2.0.13\n",
      "Using transformers 3.3.1\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "import pdb\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *\n",
    "\n",
    "from fastai import __version__ as fa_version\n",
    "from torch import __version__ as pt_version\n",
    "from transformers import __version__ as hft_version\n",
    "\n",
    "print(f'Using pytorch {pt_version}')\n",
    "print(f'Using fastai {fa_version}')\n",
    "print(f'Using transformers {hft_version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU #1: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "#cuda\n",
    "torch.cuda.set_device(1)\n",
    "print(f'Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base splitter, model wrapper, and model callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def hf_splitter(m):\n",
    "    \"\"\"Splits the huggingface model based on various model architecture conventions\"\"\"\n",
    "    model = m.hf_model if (hasattr(m, 'hf_model')) else m\n",
    "    root_modules = list(model.named_children())\n",
    "    top_module_name, top_module = root_modules[0]\n",
    "    \n",
    "    groups = L([ m for m_name, m in list(top_module.named_children()) ])\n",
    "    groups += L([ m for m_name, m in root_modules[1:] ])\n",
    "\n",
    "    return groups.map(params).filter(lambda el: len(el) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"hf_splitter\" class=\"doc_header\"><code>hf_splitter</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>hf_splitter</code>(**`m`**)\n",
       "\n",
       "Splits the huggingface model based on various model architecture conventions"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(hf_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_BaseModelWrapper(Module):\n",
    "    def __init__(self, hf_model, output_hidden_states=False, output_attentions=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        store_attr(self=self, names='output_hidden_states, output_attentions')\n",
    "        self.hf_model = hf_model.cuda() if torch.cuda.is_available() else hf_model\n",
    "        \n",
    "        n_fwd_args = self.hf_model.forward.__code__.co_argcount\n",
    "        self.hf_model_fwd_args = self.hf_model.forward.__code__.co_varnames[:n_fwd_args][1:]\n",
    "        \n",
    "    def forward(self, x): \n",
    "        for k in list(x): \n",
    "            if k not in self.hf_model_fwd_args: del x[k]       \n",
    "                \n",
    "        return self.hf_model(**x,  \n",
    "                             output_hidden_states=self.output_hidden_states, \n",
    "                             output_attentions=self.output_hidden_states,\n",
    "                             return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `HF_baseModelWrapper` includes some nifty code for just passing in the things your model needs, as not all transformer architectures require/use the same information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HF_BaseModelCallback(Callback):\n",
    "    def after_pred(self): \n",
    "        model_outputs = self.pred\n",
    "        if ('loss' in model_outputs): self.learn.loss = model_outputs.loss \n",
    "        if ('logits' in model_outputs): self.learn.pred = model_outputs.logits\n",
    "        if ('hidden_states' in model_outputs): self.learn.blurr_hidden_states = model_outputs.hidden_states\n",
    "        if ('attentions' in model_outputs): self.learn.blurr_attentions = model_outputs.attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a `Callback` for handling what is returned from the huggingface model ... \"the huggingface model will return a tuple in outputs, with the actual predictions and some additional activations (should we want to use them is some regularization scheme)\" - from the fastai [Transformer's Tutorial](http://dev.fast.ai/tutorial.transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence classification\n",
    "\n",
    "Below demonstrates how to setup your `blurr` pipeline for a sequence classification task (e.g., a model that requires a single text input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "imdb_df = pd.read_csv(path/'texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.&lt;br /&gt;&lt;br /&gt;But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.&lt;br /&gt;&lt;br /&gt;Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.&lt;br /&gt;&lt;br /&gt;Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie \"Duty, Honor, Country\" are not just mere words blathered from the lips of a high-brassed offic...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  \\\n",
       "0  negative   \n",
       "1  positive   \n",
       "2  negative   \n",
       "3  positive   \n",
       "4  negative   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0                                                                                                                                                                                                    Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!   \n",
       "1  This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.<br /><br />But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...   \n",
       "2  Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people. If I labor all my days and I can save but one soul from watching this movie, how great will be my joy.<br /><br />Where to begin my discussion of pain. For starters, there was a musical montage every five minutes. There was no character development. Every character was a stereotype. We had swearing guy, fat guy who eats donuts, goofy foreign guy, etc. The script felt as if it were being written as the movie was being shot. The production value was so incredibly low that it felt li...   \n",
       "3  Name just says it all. I watched this movie with my dad when it came out and having served in Korea he had great admiration for the man. The disappointing thing about this film is that it only concentrate on a short period of the man's life - interestingly enough the man's entire life would have made such an epic bio-pic that it is staggering to imagine the cost for production.<br /><br />Some posters elude to the flawed characteristics about the man, which are cheap shots. The theme of the movie \"Duty, Honor, Country\" are not just mere words blathered from the lips of a high-brassed offic...   \n",
       "4  This movie succeeds at being one of the most unique movies you've seen. However this comes from the fact that you can't make heads or tails of this mess. It almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid. If you don't want to feel slighted you'll sit through this horrible film and develop a real sense of pity for the actors involved, they've all seen better days, but then you realize they actually got paid quite a bit of money to do this and you'll lose pity for them just like you've alr...   \n",
       "\n",
       "   is_valid  \n",
       "0     False  \n",
       "1     False  \n",
       "2     False  \n",
       "3     False  \n",
       "4     False  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#hide_output\n",
    "task = HF_TASKS_AUTO.SequenceClassification\n",
    "\n",
    "pretrained_model_name = \"roberta-base\" # \"distilbert-base-uncased\" \"bert-base-uncased\"\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(pretrained_model_name, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single input\n",
    "blocks = (HF_TextBlock(hf_arch=hf_arch, hf_tokenizer=hf_tokenizer), CategoryBlock)\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, \n",
    "                   get_x=ColReader('text'), \n",
    "                   get_y=ColReader('label'), \n",
    "                   splitter=ColSplitter(col='is_valid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(imdb_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Raising Victor Vargas: A Review&lt;br /&gt;&lt;br /&gt;You know, Raising Victor Vargas is like sticking your hands into a big, steaming bowl of oatmeal. It's warm and gooey, but you're not sure if it feels right. Try as I might, no matter how warm and gooey Raising Victor Vargas became I was always aware that something didn't quite feel right. Victor Vargas suffers from a certain overconfidence on the director's part. Apparently, the director thought that the ethnic backdrop of a Latino family on the lower east side, and an idyllic storyline would make the film critic proof. He was right, but it didn't fool me. Raising Victor Vargas is the story about a seventeen-year old boy called, you guessed it, Victor Vargas (Victor Rasuk) who lives his teenage years chasing more skirt than the Rolling Stones could do in all the years they've toured. The movie starts off in `Ugly Fat' Donna's bedroom where Victor is sure to seduce her, but a cry from outside disrupts his plans when his best-friend Harold (Kevin Rivera) comes-a-looking for him. Caught in the attempt by Harold and his sister, Victor Vargas runs off for damage control. Yet even with the embarrassing implication that he's been boffing the homeliest girl in the neighborhood, nothing dissuades young Victor from going off on the hunt for more fresh meat. On a hot, New York City day they make way to the local public swimming pool where Victor's eyes catch a glimpse of the lovely young nymph Judy (Judy Marte), who's not just pretty, but a strong and independent too. The relationship that develops between Victor and Judy becomes the focus of the film. The story also focuses on Victor's family that is comprised of his grandmother or abuelita (Altagracia Guzman), his brother Nino (also played by real life brother to Victor, Silvestre Rasuk) and his sister Vicky (Krystal Rodriguez). The action follows Victor between scenes with Judy and scenes with his family. Victor tries to cope with being an oversexed pimp-daddy, his feelings for Judy and his grandmother's conservative Catholic upbringing.&lt;br /&gt;&lt;br /&gt;The problems that arise from Raising Victor Vargas are a few, but glaring errors. Throughout the film you get to know certain characters like Vicky, Nino, Grandma,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Well, what can I say.&lt;br /&gt;&lt;br /&gt;\"What the Bleep do we Know\" has achieved the nearly impossible - leaving behind such masterpieces of the genre as \"The Postman\", \"The Dungeon Master\", \"Merlin\", and so fourth, it will go down in history as the single worst movie I have ever seen in its entirety. And that, ladies and gentlemen, is impressive indeed, for I have seen many a bad movie.&lt;br /&gt;&lt;br /&gt;This masterpiece of modern cinema consists of two interwoven parts, alternating between a silly and contrived plot about an extremely annoying photographer, abandoned by her husband and forced to take anti-depressants to survive, and a bunch of talking heads going on about how quantum physics supposedly justifies their new-agy pseudo-philosophy. Basically, if you start your day off meditating to the likes of Enya and Kenny G, this movie is for you. If you have a sense of humor, a crowd of people who know how to have fun, and a sizable portion of good weed, then this movie is for you as well. Otherwise, stay away. Take my word for it.&lt;br /&gt;&lt;br /&gt;The first thing that struck me about \"What the Bleep do you Know\" is that is seemed to be edited and put together by the same kinds of people that shoot cheap weddings on camera, complete with pink heart effects, computer-generated sparkles across the screen, and other assorted silliness. Who let these people anywhere near a theatrical release is a mystery to me. I guess this is what too much Kenny G does to you. The movie was permeated with cheesy GCI, the likes that you or I can produce on our own computer via over-the-counter video editing software, but never would, because it's just way too ridiculous.&lt;br /&gt;&lt;br /&gt;The script was _obviously_ written by someone with no writing experience whatsoever. Not only were all the characters and conversations cumbersome and contrived beyond belief, but the \"writers\" felt like they had to shove every relevant piece of information, or rather disinformation, which is what most of this movie was all about, all the way down your throat. Well, given the target audience, that may not have been too bad of an idea. The main character, for example, spends half the movie popping pills. Apparently, though, it was deemed not convincing enough, so there are at least a couple of dialogs in throughout, which refer to her</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We'll also add in custom summary methods for blurr learners/models that work with dictionary inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HF_BaseModelWrapper(hf_model)\n",
    "\n",
    "learn = Learner(dls, \n",
    "                model,\n",
    "                opt_func=partial(Adam),\n",
    "                loss_func=CrossEntropyLossFlat(),\n",
    "                metrics=[accuracy],\n",
    "                cbs=[HF_BaseModelCallback],\n",
    "                splitter=hf_splitter)\n",
    "\n",
    "learn.create_opt()             # -> will create your layer groups based on your \"splitter\" function\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.to_fp16()` requires a GPU so had to remove for tests to run on github.  Let's check that we can get predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.1701, -0.0125],\n",
       "        [ 0.1655, -0.0182],\n",
       "        [ 0.1631, -0.0142],\n",
       "        [ 0.1629, -0.0124]], device='cuda:1', grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model(b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def blurr_module_summary(learn, *xb):\n",
    "    \"Print a summary of `model` using `xb`\"\n",
    "    infos = layer_info(learn, *xb)\n",
    "    n,bs = 64,find_bs(xb)\n",
    "    inp_sz = _print_shapes(apply(lambda x:x.shape,  xb[0]['input_ids']), bs)\n",
    "    res = f\"{learn.model.__class__.__name__} (Input shape: {inp_sz})\\n\"\n",
    "    res += \"=\" * n + \"\\n\"\n",
    "    res += f\"{'Layer (type)':<20} {'Output Shape':<20} {'Param #':<10} {'Trainable':<10}\\n\"\n",
    "    res += \"=\" * n + \"\\n\"\n",
    "    ps,trn_ps = 0,0\n",
    "    infos = [o for o in infos if o is not None] #see comment in previous cell\n",
    "    for typ,np,trn,sz in infos:\n",
    "        if sz is None: continue\n",
    "        ps += np\n",
    "        if trn: trn_ps += np\n",
    "        res += f\"{typ:<20} {_print_shapes(sz, bs)[:19]:<20} {np:<10,} {str(trn):<10}\\n\"\n",
    "        res += \"_\" * n + \"\\n\"\n",
    "    res += f\"\\nTotal params: {ps:,}\\n\"\n",
    "    res += f\"Total trainable params: {trn_ps:,}\\n\"\n",
    "    res += f\"Total non-trainable params: {ps - trn_ps:,}\\n\\n\"\n",
    "    return PrettyString(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def blurr_summary(self:Learner):\n",
    "    \"Print a summary of the model, optimizer and loss function.\"\n",
    "    xb = self.dls.train.one_batch()[:self.dls.train.n_inp]\n",
    "    res = blurr_module_summary(self, *xb)\n",
    "    res += f\"Optimizer used: {self.opt_func}\\nLoss function: {self.loss_func}\\n\\n\"\n",
    "    if self.opt is not None:\n",
    "        res += f\"Model \" + (\"unfrozen\\n\\n\" if self.opt.frozen_idx==0 else f\"frozen up to parameter group #{self.opt.frozen_idx}\\n\\n\")\n",
    "    res += \"Callbacks:\\n\" + '\\n'.join(f\"  - {cb}\" for cb in sort_by_run(self.cbs))\n",
    "    return PrettyString(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to create our own `summary` methods above because fastai only works where things are represented by a *single tensor*.  But in the case of huggingface transformers, a *single* sequence is represented by *multiple tensors* (in a dictionary).  \n",
    "\n",
    "The change to make this work is so minor I think that the fastai library can/will hopefully be updated to support this use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.blurr_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(learn.opt.param_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(lr_min=6.309573450380412e-08, lr_steep=0.04786301031708717)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.lr_find(suggestions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.316923</td>\n",
       "      <td>0.280128</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.fit_one_cycle(1, lr_max=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing results\n",
    "\n",
    "And here we creat a @typedispatched impelmentation of `Learner.show_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_results(x:HF_BaseInput, y, samples, outs, learner, ctxs=None, max_n=6, **kwargs):        \n",
    "    kwargs['hf_tokenizer'] = learner.dls.before_batch[0].hf_tokenizer\n",
    "    \n",
    "    if ctxs is None: ctxs = get_empty_df(min(len(samples), max_n))\n",
    "    ctxs = show_batch[object](x, y, samples, max_n=max_n, ctxs=ctxs, **kwargs)\n",
    "    \n",
    "    n_preds_per_input = len(outs[0])\n",
    "    if (n_preds_per_input == 1): \n",
    "        for i,ctx in enumerate(ctxs): ctx['target'] = outs[i][0]\n",
    "    else:\n",
    "        for pred_idx in range(n_preds_per_input):\n",
    "            for i,ctx in enumerate(ctxs):  ctx[f'target{pred_idx+1}'] = outs[i][pred_idx]\n",
    "\n",
    "    display_df(pd.DataFrame(ctxs))\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The trouble with the book, \"Memoirs of a Geisha\" is that it had Japanese surfaces but underneath the surfaces it was all an American man's way of thinking. Reading the book is like watching a magnificent ballet with great music, sets, and costumes yet performed by barnyard animals dressed in those costumesÂ—so far from Japanese ways of thinking were the characters.&lt;br /&gt;&lt;br /&gt;The movie isn't about Japan or real geisha. It is a story about a few American men's mistaken ideas about Japan and geisha filtered through their own ignorance and misconceptions. So what is this movie if it isn't about Japan or geisha? Is it pure fantasy as so many people have said? Yes, but then why make it into an American fantasy?&lt;br /&gt;&lt;br /&gt;There were so many missed opportunities. Imagine a culture where there are no puritanical hang-ups, no connotations of sin about sex. Sex is natural and normal. How is sex handled in this movie? Right. Like it was dirty. The closest thing to a sex scene in the movie has Sayuri wrinkling up her nose and grimacing with distaste for five seconds as if the man trying to mount her had dropped a handful of cockroaches on her crotch. &lt;br /&gt;&lt;br /&gt;Does anyone actually enjoy sex in this movie? Nope. One character is said to be promiscuous but all we see is her pushing away her lover because it looks like she doesn't want to get caught doing something dirty. Such typical American puritanism has no place in a movie about Japanese geisha.&lt;br /&gt;&lt;br /&gt;Did Sayuri enjoy her first ravishing by some old codger after her cherry was auctioned off? Nope. She lies there like a cold slab of meat on a chopping block. Of course she isn't supposed to enjoy it. And that is what I mean about this movie. Why couldn't they have given her something to enjoy? Why does all the sex have to be sinful and wrong?&lt;br /&gt;&lt;br /&gt;Behind Mameha the Chairman was Sayuri's secret patron, and as such he was behind the auction of her virginity. He could have rigged the auction and won her himself. Nobu didn't even bid. So why did the Chairman let that old codger win her and, reeking of old-man stink, get his fingers all over her naked body? Would any woman ever really forgive a man for that?&lt;br /&gt;&lt;br /&gt;Let's</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;br /&gt;&lt;br /&gt;I'm sure things didn't exactly go the same way in the real life of Homer Hickam as they did in the film adaptation of his book, Rocket Boys, but the movie \"October Sky\" (an anagram of the book's title) is good enough to stand alone. I have not read Hickam's memoirs, but I am still able to enjoy and understand their film adaptation. The film, directed by Joe Johnston and written by Lewis Colick, records the story of teenager Homer Hickam (Jake Gyllenhaal), beginning in October of 1957. It opens with the sound of a radio broadcast, bringing news of the Russian satellite Sputnik, the first artificial satellite in orbit. We see a images of a blue-gray town and its people: mostly miners working for the Olga Coal Company. One of the miners listens to the news on a hand-held radio as he enters the elevator shaft, but the signal is lost as he disappears into the darkness, losing sight of the starry sky above him. A melancholy violin tune fades with this image. We then get a jolt of Elvis on a car radio as words on the screen inform us of the setting: October 5, 1957, Coalwood, West Virginia. Homer and his buddies, Roy Lee Cook (William Lee Scott) and Sherman O'Dell (Chad Lindberg), are talking about football tryouts. Football scholarships are the only way out of the town, and working in the mines, for these boys. \"Why are the jocks the only ones who get to go to college,\" questions Homer. Roy Lee replies, \"They're also the only ones who get the girls.\" Homer doesn't make it in football like his older brother, so he is destined for the mines, and to follow in his father's footsteps as mine foreman. Until he sees the dot of light streaking across the October sky. Then he wants to build a rocket. \"I want to go into space,\" says Homer. After a disastrous attempt involving a primitive rocket and his mother's (Natalie Canerday) fence, Homer enlists the help of the nerdy Quentin Wilson (Chris Owen). Quentin asks Homer, \"What do you want to know about rockets?\" Homer quickly anwers, \"Everything.\" His science teacher at Big Creek High School, Miss Frieda Riley (Laura Dern) greatly supports Homer, and the four boys work on building rockets in Homer's basement. His father,</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(learner=learn, max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def blurr_predict(self:Learner, item, rm_type_tfms=None, with_input=False):\n",
    "    dl = self.dls.test_dl([item], rm_type_tfms=rm_type_tfms, num_workers=0)\n",
    "    \n",
    "    # this is where we have to change things up since a blurr \"input\" is represented by a dictionary of\n",
    "    # tensors (input_ids, attention_mask, token_type_ids, etc...) and not a single tensor (which fastai assumes\n",
    "    # in a number of places)\n",
    "    b = dl.one_batch()\n",
    "    inp = b[0]\n",
    "    preds, _, dec_preds = self.get_preds(dl=dl, with_input=False, with_decoded=True)\n",
    "    \n",
    "    i = getattr(self.dls, 'n_inp', -1)\n",
    "    inp = (inp,) if i==1 else tuplify(inp)\n",
    "    dec = self.dls.decode_batch(inp + tuplify(dec_preds))[0]\n",
    "    dec_inp,dec_targ = map(detuplify, [dec[:i],dec[i:]])\n",
    "    res = dec_targ,dec_preds[0],preds[0]\n",
    "    if with_input: res = (dec_inp,) + res\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as with `summary`, we need to replace fastai's `Learner.predict` method with the one above which is able to work with inputs that are represented by multiple tensors included in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('positive', tensor(1), tensor([0.1892, 0.8108]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.blurr_predict('I really liked the movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.266409</td>\n",
       "      <td>0.229690</td>\n",
       "      <td>0.905000</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.207100</td>\n",
       "      <td>0.246732</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.106141</td>\n",
       "      <td>0.289648</td>\n",
       "      <td>0.895000</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.fit_one_cycle(3, lr_max=slice(1e-7, 1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The trouble with the book, \"Memoirs of a Geisha\" is that it had Japanese surfaces but underneath the surfaces it was all an American man's way of thinking. Reading the book is like watching a magnificent ballet with great music, sets, and costumes yet performed by barnyard animals dressed in those costumesÂ—so far from Japanese ways of thinking were the characters.&lt;br /&gt;&lt;br /&gt;The movie isn't about Japan or real geisha. It is a story about a few American men's mistaken ideas about Japan and geisha filtered through their own ignorance and misconceptions. So what is this movie if it isn't about Japan or geisha? Is it pure fantasy as so many people have said? Yes, but then why make it into an American fantasy?&lt;br /&gt;&lt;br /&gt;There were so many missed opportunities. Imagine a culture where there are no puritanical hang-ups, no connotations of sin about sex. Sex is natural and normal. How is sex handled in this movie? Right. Like it was dirty. The closest thing to a sex scene in the movie has Sayuri wrinkling up her nose and grimacing with distaste for five seconds as if the man trying to mount her had dropped a handful of cockroaches on her crotch. &lt;br /&gt;&lt;br /&gt;Does anyone actually enjoy sex in this movie? Nope. One character is said to be promiscuous but all we see is her pushing away her lover because it looks like she doesn't want to get caught doing something dirty. Such typical American puritanism has no place in a movie about Japanese geisha.&lt;br /&gt;&lt;br /&gt;Did Sayuri enjoy her first ravishing by some old codger after her cherry was auctioned off? Nope. She lies there like a cold slab of meat on a chopping block. Of course she isn't supposed to enjoy it. And that is what I mean about this movie. Why couldn't they have given her something to enjoy? Why does all the sex have to be sinful and wrong?&lt;br /&gt;&lt;br /&gt;Behind Mameha the Chairman was Sayuri's secret patron, and as such he was behind the auction of her virginity. He could have rigged the auction and won her himself. Nobu didn't even bid. So why did the Chairman let that old codger win her and, reeking of old-man stink, get his fingers all over her naked body? Would any woman ever really forgive a man for that?&lt;br /&gt;&lt;br /&gt;Let's</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;br /&gt;&lt;br /&gt;I'm sure things didn't exactly go the same way in the real life of Homer Hickam as they did in the film adaptation of his book, Rocket Boys, but the movie \"October Sky\" (an anagram of the book's title) is good enough to stand alone. I have not read Hickam's memoirs, but I am still able to enjoy and understand their film adaptation. The film, directed by Joe Johnston and written by Lewis Colick, records the story of teenager Homer Hickam (Jake Gyllenhaal), beginning in October of 1957. It opens with the sound of a radio broadcast, bringing news of the Russian satellite Sputnik, the first artificial satellite in orbit. We see a images of a blue-gray town and its people: mostly miners working for the Olga Coal Company. One of the miners listens to the news on a hand-held radio as he enters the elevator shaft, but the signal is lost as he disappears into the darkness, losing sight of the starry sky above him. A melancholy violin tune fades with this image. We then get a jolt of Elvis on a car radio as words on the screen inform us of the setting: October 5, 1957, Coalwood, West Virginia. Homer and his buddies, Roy Lee Cook (William Lee Scott) and Sherman O'Dell (Chad Lindberg), are talking about football tryouts. Football scholarships are the only way out of the town, and working in the mines, for these boys. \"Why are the jocks the only ones who get to go to college,\" questions Homer. Roy Lee replies, \"They're also the only ones who get the girls.\" Homer doesn't make it in football like his older brother, so he is destined for the mines, and to follow in his father's footsteps as mine foreman. Until he sees the dot of light streaking across the October sky. Then he wants to build a rocket. \"I want to go into space,\" says Homer. After a disastrous attempt involving a primitive rocket and his mother's (Natalie Canerday) fence, Homer enlists the help of the nerdy Quentin Wilson (Chris Owen). Quentin asks Homer, \"What do you want to know about rockets?\" Homer quickly anwers, \"Everything.\" His science teacher at Big Creek High School, Miss Frieda Riley (Laura Dern) greatly supports Homer, and the four boys work on building rockets in Homer's basement. His father,</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(learner=learn, max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('positive', tensor(1), tensor([0.0123, 0.9877]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.blurr_predict(\"This was a really good movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('negative', tensor(0), tensor([0.9945, 0.0055]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.blurr_predict(\"Acting was so bad it was almost funny.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export(fname='seq_class_learn_export.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('negative', tensor(0), tensor([0.9961, 0.0039]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_learn = load_learner(fname='seq_class_learn_export.pkl')\n",
    "inf_learn.blurr_predict(\"This movie should not be seen by anyone!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "The tests below to ensure the core training code above works for **all** pretrained sequence classification models available in huggingface.  These tests are excluded from the CI workflow because of how long they would take to run and the amount of data that would be required to download.\n",
    "\n",
    "**Note**: Feel free to modify the code below to test whatever pretrained classification models you are working with ... and if any of your pretrained sequence classification models fail, please submit a github issue *(or a PR if you'd like to fix it yourself)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: del learn; torch.cuda.empty_cache()\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[transformers.modeling_albert.AlbertForSequenceClassification,\n",
       " transformers.modeling_auto.AutoModelForSequenceClassification,\n",
       " transformers.modeling_bart.BartForSequenceClassification,\n",
       " transformers.modeling_bert.BertForSequenceClassification,\n",
       " transformers.modeling_camembert.CamembertForSequenceClassification,\n",
       " transformers.modeling_distilbert.DistilBertForSequenceClassification,\n",
       " transformers.modeling_electra.ElectraForSequenceClassification,\n",
       " transformers.modeling_flaubert.FlaubertForSequenceClassification,\n",
       " transformers.modeling_funnel.FunnelForSequenceClassification,\n",
       " transformers.modeling_longformer.LongformerForSequenceClassification,\n",
       " transformers.modeling_mobilebert.MobileBertForSequenceClassification,\n",
       " transformers.modeling_reformer.ReformerForSequenceClassification,\n",
       " transformers.modeling_roberta.RobertaForSequenceClassification,\n",
       " transformers.modeling_xlm.XLMForSequenceClassification,\n",
       " transformers.modeling_xlm_roberta.XLMRobertaForSequenceClassification,\n",
       " transformers.modeling_xlnet.XLNetForSequenceClassification]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BLURR_MODEL_HELPER.get_models(task='SequenceClassification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_names = [\n",
    "    'albert-base-v1',\n",
    "    'facebook/bart-base',\n",
    "    'bert-base-uncased',\n",
    "    'camembert-base',\n",
    "    'distilbert-base-uncased',\n",
    "    'monologg/electra-small-finetuned-imdb',\n",
    "    'flaubert/flaubert_small_cased', \n",
    "    'allenai/longformer-base-4096',\n",
    "    'google/mobilebert-uncased',\n",
    "    'roberta-base',\n",
    "    'xlm-mlm-en-2048',\n",
    "    'xlm-roberta-base',\n",
    "    'xlnet-base-cased'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "\n",
    "model_path = Path('models')\n",
    "imdb_df = pd.read_csv(path/'texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== albert-base-v1 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v1 were not used when initializing AlbertForSequenceClassification: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\talbert\n",
      "tokenizer:\tAlbertTokenizer\n",
      "model:\t\tAlbertForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.686067</td>\n",
       "      <td>0.648250</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>00:15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the trouble with the book, \"memoirs of a geisha\" is that it had japanese surfaces but underneath the surfaces it was all an american man's way of thinking. reading the book is like watching a magnificent ballet with great music, sets, and costumes yet performed by barnyard animals dressed in those costumesso far from japanese ways of thinking were the characters.br /br /the movie isn't about japan or real geisha. it is a story about a few american men's mistaken ideas about japan and geisha filtered through their own ignorance and misconceptions</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>br /br /i'm sure things didn't exactly go the same way in the real life of homer hickam as they did in the film adaptation of his book, rocket boys, but the movie \"october sky\" (an anagram of the book's title) is good enough to stand alone. i have not read hickam's memoirs, but i am still able to enjoy and understand their film adaptation. the film, directed by joe johnston and written by lewis colick, records the story of teenager homer hickam (jake</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== facebook/bart-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/bart-base were not used when initializing BartForSequenceClassification: ['final_logits_bias']\n",
      "- This IS expected if you are initializing BartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.weight', 'classification_head.dense.bias', 'classification_head.out_proj.weight', 'classification_head.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tbart\n",
      "tokenizer:\tBartTokenizer\n",
      "model:\t\tBartForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.397860</td>\n",
       "      <td>0.426862</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The trouble with the book, \"Memoirs of a Geisha\" is that it had Japanese surfaces but underneath the surfaces it was all an American man's way of thinking. Reading the book is like watching a magnificent ballet with great music, sets, and costumes yet performed by barnyard animals dressed in those costumesÂ—so far from Japanese ways of thinking were the characters.&lt;br /&gt;&lt;br /&gt;The movie isn't about Japan or real geisha. It is a story about a few American men's mistaken ideas about Japan and geisha filtered through their own ignorance and misconceptions. So what is this movie if it isn't</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How viewers react to this new \"adaption\" of Shirley Jackson's book, which was promoted as NOT being a remake of the original 1963 movie (true enough), will be based, I suspect, on the following: those who were big fans of either the book or original movie are not going to think much of this one...and those who have never been exposed to either, and who are big fans of Hollywood's current trend towards \"special effects\" being the first and last word in how \"good\" a film is, are going to love it.&lt;br /&gt;&lt;br /&gt;Things I did not like about this ada</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== bert-base-uncased ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tbert\n",
      "tokenizer:\tBertTokenizer\n",
      "model:\t\tBertForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.607559</td>\n",
       "      <td>0.628680</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the trouble with the book, \" memoirs of a geisha \" is that it had japanese surfaces but underneath the surfaces it was all an american man's way of thinking. reading the book is like watching a magnificent ballet with great music, sets, and costumes yet performed by barnyard animals dressed in those far from japanese ways of thinking were the characters. &lt; br / &gt; &lt; br / &gt; the movie isn't about japan or real geisha. it is a story about a few american men's mistaken ideas about japan and geisha filtered through their own ignorance and misconceptions. so what is this movie</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt; br / &gt; &lt; br / &gt; i'm sure things didn't exactly go the same way in the real life of homer hickam as they did in the film adaptation of his book, rocket boys, but the movie \" october sky \" ( an anagram of the book's title ) is good enough to stand alone. i have not read hickam's memoirs, but i am still able to enjoy and understand their film adaptation. the film, directed by joe johnston and written by lewis colick, records the story of teenager homer hickam ( jake gyllenhaal ), beginning</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== camembert-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tcamembert\n",
      "tokenizer:\tCamembertTokenizer\n",
      "model:\t\tCamembertForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.705684</td>\n",
       "      <td>0.690840</td>\n",
       "      <td>0.535000</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The trouble with the book, \"Memoirs of a Geisha\" is that it had Japanese surfaces but underneath the surfaces it was all an American man's way of thinking. Reading the book is like watching a magnificent ballet with great music, sets, and costumes yet performed by barnyard animals dressed in those costumesso far from Japanese ways of thinking were the characters.&lt;br /&gt;&lt;br /&gt;The movie isn't about Japan or real geisha. It is a</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;br /&gt;&lt;br /&gt;I'm sure things didn't exactly go the same way in the real life of Homer Hickam as they did in the film adaptation of his book, Rocket Boys, but the movie \"October Sky\" (an anagram of the book's title) is good enough to stand alone. I have not read Hickam's memoirs, but I am still able to enjoy and understand their film adaptation. The film, directed</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== distilbert-base-uncased ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tdistilbert\n",
      "tokenizer:\tDistilBertTokenizer\n",
      "model:\t\tDistilBertForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.475335</td>\n",
       "      <td>0.483896</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the trouble with the book, \" memoirs of a geisha \" is that it had japanese surfaces but underneath the surfaces it was all an american man's way of thinking. reading the book is like watching a magnificent ballet with great music, sets, and costumes yet performed by barnyard animals dressed in those far from japanese ways of thinking were the characters. &lt; br / &gt; &lt; br / &gt; the movie isn't about japan or real geisha. it is a story about a few american men's mistaken ideas about japan and geisha filtered through their own ignorance and misconceptions. so what is this movie</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how viewers react to this new \" adaption \" of shirley jackson's book, which was promoted as not being a remake of the original 1963 movie ( true enough ), will be based, i suspect, on the following : those who were big fans of either the book or original movie are not going to think much of this one... and those who have never been exposed to either, and who are big fans of hollywood's current trend towards \" special effects \" being the first and last word in how \" good \" a film is, are going to love it. &lt; br / &gt; &lt; br / &gt; things</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== monologg/electra-small-finetuned-imdb ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/electra-small-finetuned-imdb were not used when initializing ElectraForSequenceClassification: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/electra-small-finetuned-imdb and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\telectra\n",
      "tokenizer:\tElectraTokenizer\n",
      "model:\t\tElectraForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.217937</td>\n",
       "      <td>0.363690</td>\n",
       "      <td>0.855000</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the trouble with the book, \" memoirs of a geisha \" is that it had japanese surfaces but underneath the surfaces it was all an american man's way of thinking. reading the book is like watching a magnificent ballet with great music, sets, and costumes yet performed by barnyard animals dressed in those far from japanese ways of thinking were the characters. &lt; br / &gt; &lt; br / &gt; the movie isn't about japan or real geisha. it is a story about a few american men's mistaken ideas about japan and geisha filtered through their own ignorance and misconceptions. so what is this movie</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how viewers react to this new \" adaption \" of shirley jackson's book, which was promoted as not being a remake of the original 1963 movie ( true enough ), will be based, i suspect, on the following : those who were big fans of either the book or original movie are not going to think much of this one... and those who have never been exposed to either, and who are big fans of hollywood's current trend towards \" special effects \" being the first and last word in how \" good \" a film is, are going to love it. &lt; br / &gt; &lt; br / &gt; things</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== flaubert/flaubert_small_cased ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at flaubert/flaubert_small_cased were not used when initializing FlaubertForSequenceClassification: ['pred_layer.proj.bias', 'pred_layer.proj.weight']\n",
      "- This IS expected if you are initializing FlaubertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing FlaubertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FlaubertForSequenceClassification were not initialized from the model checkpoint at flaubert/flaubert_small_cased and are newly initialized: ['transformer.position_ids', 'sequence_summary.summary.weight', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tflaubert\n",
      "tokenizer:\tFlaubertTokenizer\n",
      "model:\t\tFlaubertForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.716854</td>\n",
       "      <td>0.670829</td>\n",
       "      <td>0.565000</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The trouble with the book, \" Memoirs of a Geisha \" is that it had Japanese surfaces but underneath the surfaces it was all an American man' s way of thinking. Reading the book is like watching a magnificent ballet with great music, sets, and costumes yet performed by barnyard animals dressed in those costumesso far from Japanese ways of thinking were the characters. &lt; br / &gt; &lt; br / &gt; The movie isn' t about Japan or real geisha. It is a story about</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt; br / &gt; &lt; br / &gt; I' m sure things didn' t exactly go the same way in the real life of Homer Hickam as they did in the film adaptation of his book, Rocket Boys, but the movie \" October Sky \" ( an anagram of the book' s title ) is good enough to stand alone. I have not read Hickam' s memoirs, but I am still able to enjoy and understand their film adaptation. The film, directed by Joe Johnston and written by Lewis Coli</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== allenai/longformer-base-4096 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tlongformer\n",
      "tokenizer:\tLongformerTokenizer\n",
      "model:\t\tLongformerForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.450381</td>\n",
       "      <td>0.403549</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>01:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The trouble with the book, \"Memoirs of a Geisha\" is that it had Japanese surfaces but underneath the surfaces it was all an American man's way of thinking. Reading the book is like watching a magnificent ballet with great music, sets, and costumes yet performed by barnyard animals dressed in those costumesÂ—so far from Japanese ways of thinking were the characters.&lt;br /&gt;&lt;br /&gt;The movie isn't about Japan or real geisha. It is a story about a few American men's mistaken ideas about Japan and geisha filtered through their own ignorance and misconceptions. So what is this movie if it isn't</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How viewers react to this new \"adaption\" of Shirley Jackson's book, which was promoted as NOT being a remake of the original 1963 movie (true enough), will be based, I suspect, on the following: those who were big fans of either the book or original movie are not going to think much of this one...and those who have never been exposed to either, and who are big fans of Hollywood's current trend towards \"special effects\" being the first and last word in how \"good\" a film is, are going to love it.&lt;br /&gt;&lt;br /&gt;Things I did not like about this ada</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== google/mobilebert-uncased ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/mobilebert-uncased were not used when initializing MobileBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing MobileBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing MobileBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\tmobilebert\n",
      "tokenizer:\tMobileBertTokenizer\n",
      "model:\t\tMobileBertForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>159332.593750</td>\n",
       "      <td>37297.421875</td>\n",
       "      <td>0.465000</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the trouble with the book, \" memoirs of a geisha \" is that it had japanese surfaces but underneath the surfaces it was all an american man's way of thinking. reading the book is like watching a magnificent ballet with great music, sets, and costumes yet performed by barnyard animals dressed in those far from japanese ways of thinking were the characters. &lt; br / &gt; &lt; br / &gt; the movie isn't about japan or real geisha. it is a story about a few american men's mistaken ideas about japan and geisha filtered through their own ignorance and misconceptions. so what is this movie</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt; br / &gt; &lt; br / &gt; i'm sure things didn't exactly go the same way in the real life of homer hickam as they did in the film adaptation of his book, rocket boys, but the movie \" october sky \" ( an anagram of the book's title ) is good enough to stand alone. i have not read hickam's memoirs, but i am still able to enjoy and understand their film adaptation. the film, directed by joe johnston and written by lewis colick, records the story of teenager homer hickam ( jake gyllenhaal ), beginning</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== roberta-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\troberta\n",
      "tokenizer:\tRobertaTokenizer\n",
      "model:\t\tRobertaForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.429124</td>\n",
       "      <td>0.424677</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The trouble with the book, \"Memoirs of a Geisha\" is that it had Japanese surfaces but underneath the surfaces it was all an American man's way of thinking. Reading the book is like watching a magnificent ballet with great music, sets, and costumes yet performed by barnyard animals dressed in those costumesÂ—so far from Japanese ways of thinking were the characters.&lt;br /&gt;&lt;br /&gt;The movie isn't about Japan or real geisha. It is a story about a few American men's mistaken ideas about Japan and geisha filtered through their own ignorance and misconceptions. So what is this movie if it isn't</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;br /&gt;&lt;br /&gt;I'm sure things didn't exactly go the same way in the real life of Homer Hickam as they did in the film adaptation of his book, Rocket Boys, but the movie \"October Sky\" (an anagram of the book's title) is good enough to stand alone. I have not read Hickam's memoirs, but I am still able to enjoy and understand their film adaptation. The film, directed by Joe Johnston and written by Lewis Colick, records the story of teenager Homer Hickam (Jake Gyllenhaal), beginning in October of 1957. It opens with the</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlm-mlm-en-2048 ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-mlm-en-2048 were not used when initializing XLMForSequenceClassification: ['pred_layer.proj.weight', 'pred_layer.proj.bias']\n",
      "- This IS expected if you are initializing XLMForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLMForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMForSequenceClassification were not initialized from the model checkpoint at xlm-mlm-en-2048 and are newly initialized: ['transformer.position_ids', 'sequence_summary.summary.weight', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\txlm\n",
      "tokenizer:\tXLMTokenizer\n",
      "model:\t\tXLMForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.801137</td>\n",
       "      <td>0.692831</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>00:46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the trouble with the book, \" memoirs of a geisha \" is that it had japanese surfaces but underneath the surfaces it was all an american man's way of thinking. reading the book is like watching a magnificent ballet with great music, sets, and costumes yet performed by barnyard animals dressed in those costumesso far from japanese ways of thinking were the characters. &lt; br / &gt; &lt; br / &gt; the movie isn 't about japan or real geisha. it is a story about a few american men's mistaken ideas about japan and geisha filtered through their own ignorance and misconceptions. so what is this movie if</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to review this movie, i without any doubt would have to quote that memorable scene in tarantino's \" pulp fiction \" ( 1994 ) when jules and vincent are talking about mia wallace and what she does for a living. jules tells vincent that the \" only thing she did worthwhile was pilot. \" vincent asks \" what the hell is a pilot? \" and jules goes into a very well description of what a tv pilot is : \" well, the way they make shows is, they make one show. that show's called a'pilot '. then they show that show to the people who make shows, and on the</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlm-roberta-base ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\txlm_roberta\n",
      "tokenizer:\tXLMRobertaTokenizer\n",
      "model:\t\tXLMRobertaForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.686957</td>\n",
       "      <td>0.681423</td>\n",
       "      <td>0.535000</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The trouble with the book, \"Memoirs of a Geisha\" is that it had Japanese surfaces but underneath the surfaces it was all an American man's way of thinking. Reading the book is like watching a magnificent ballet with great music, sets, and costumes yet performed by barnyard animals dressed in those costumesÂ—so far from Japanese ways of thinking were the characters.&lt;br /&gt;&lt;br /&gt;The movie isn't about Japan or real geisha. It is a story about a few American men's mistaken ideas about Japan and geisha filtered</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;br /&gt;&lt;br /&gt;I'm sure things didn't exactly go the same way in the real life of Homer Hickam as they did in the film adaptation of his book, Rocket Boys, but the movie \"October Sky\" (an anagram of the book's title) is good enough to stand alone. I have not read Hickam's memoirs, but I am still able to enjoy and understand their film adaptation. The film, directed by Joe Johnston and written by Lewis Colick, records the story of teenager Homer Hickam (Jake Gyllen</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== xlnet-base-cased ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/transformers/configuration_xlnet.py:212: FutureWarning: This config doesn't use attention memories, a core feature of XLNet. Consider setting `men_len` to a non-zero value, for example `xlnet = XLNetLMHeadModel.from_pretrained('xlnet-base-cased'', mem_len=1024)`, for accurate training performance as well as an order of magnitude faster inference. Starting from version 3.5.0, the default parameter will be 1024, following the implementation in https://arxiv.org/abs/1906.08237\n",
      "  FutureWarning,\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture:\txlnet\n",
      "tokenizer:\tXLNetTokenizer\n",
      "model:\t\tXLNetForSequenceClassification\n",
      "\n",
      "*** TESTING DataLoaders ***\n",
      "*** TESTING One pass through the model ***\n",
      "*** TESTING Training/Results ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgilliam/anaconda3/envs/blurr/lib/python3.7/site-packages/transformers/modeling_xlnet.py:298: UserWarning: Mixed memory format inputs detected while calling the operator. The operator will output contiguous tensor even if some of the inputs are in channels_last format. (Triggered internally at  /pytorch/aten/src/ATen/native/TensorIterator.cpp:918.)\n",
      "  attn_score = (ac + bd + ef) * self.scale\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.608233</td>\n",
       "      <td>0.553650</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The trouble with the book, \"Memoirs of a Geisha\" is that it had Japanese surfaces but underneath the surfaces it was all an American man's way of thinking. Reading the book is like watching a magnificent ballet with great music, sets, and costumes yet performed by barnyard animals dressed in those costumesso far from Japanese ways of thinking were the characters.&lt;br /&gt;&lt;br /&gt;The movie isn't about Japan or real geisha. It is a story about a few American men's mistaken ideas about Japan and geisha filtered through their own ignorance and</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To review this movie, I without any doubt would have to quote that memorable scene in Tarantino's \"Pulp Fiction\" (1994) when Jules and Vincent are talking about Mia Wallace and what she does for a living. Jules tells Vincent that the \"Only thing she did worthwhile was pilot\". Vincent asks \"What the hell is a pilot?\" and Jules goes into a very well description of what a TV pilot is: \"Well, the way they make shows is, they make one show. That show's called a 'pilot'. Then they show that show to</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "#hide_output\n",
    "task = HF_TASKS_AUTO.SequenceClassification\n",
    "bsz = 2\n",
    "seq_sz = 128\n",
    "\n",
    "test_results = []\n",
    "for model_name in pretrained_model_names:\n",
    "    error=None\n",
    "    \n",
    "    print(f'=== {model_name} ===\\n')\n",
    "    \n",
    "    hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(model_name, \n",
    "                                                                                   task=task, \n",
    "                                                                                   config_kwargs={'num_labels': 2})\n",
    "    \n",
    "    print(f'architecture:\\t{hf_arch}\\ntokenizer:\\t{type(hf_tokenizer).__name__}\\nmodel:\\t\\t{type(hf_model).__name__}\\n')\n",
    "\n",
    "    blocks = (HF_TextBlock(hf_arch=hf_arch, hf_tokenizer=hf_tokenizer, max_length=seq_sz, padding='max_length'), \n",
    "              CategoryBlock)\n",
    "\n",
    "    dblock = DataBlock(blocks=blocks, \n",
    "                       get_x=ColReader('text'), \n",
    "                       get_y=ColReader('label'), \n",
    "                       splitter=ColSplitter(col='is_valid'))\n",
    "    \n",
    "    dls = dblock.dataloaders(imdb_df, bs=bsz)\n",
    "    \n",
    "    model = HF_BaseModelWrapper(hf_model)\n",
    "    learn = Learner(dls, \n",
    "                    model,\n",
    "                    opt_func=partial(Adam),\n",
    "                    loss_func=CrossEntropyLossFlat(),\n",
    "                    metrics=[accuracy],\n",
    "                    cbs=[HF_BaseModelCallback],\n",
    "                    splitter=hf_splitter)\n",
    "\n",
    "    learn.create_opt()             # -> will create your layer groups based on your \"splitter\" function\n",
    "    learn.freeze()\n",
    "    \n",
    "    b = dls.one_batch()\n",
    "    \n",
    "    try:\n",
    "        print('*** TESTING DataLoaders ***')\n",
    "        test_eq(len(b), bsz)\n",
    "        test_eq(len(b[0]['input_ids']), bsz)\n",
    "        test_eq(b[0]['input_ids'].shape, torch.Size([bsz, seq_sz]))\n",
    "        test_eq(len(b[1]), bsz)\n",
    "\n",
    "        print('*** TESTING One pass through the model ***')\n",
    "        preds = learn.model(b[0])\n",
    "        test_eq(len(preds[0]), bsz)\n",
    "        test_eq(preds[0].shape, torch.Size([bsz, 2]))\n",
    "\n",
    "        print('*** TESTING Training/Results ***')\n",
    "        learn.fit_one_cycle(1, lr_max=1e-3)\n",
    "\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, type(hf_model).__name__, 'PASSED', ''))\n",
    "        learn.show_results(learner=learn, max_n=2)\n",
    "    except Exception as err:\n",
    "        test_results.append((hf_arch, type(hf_tokenizer).__name__, type(hf_model).__name__, 'FAILED', err))\n",
    "    finally:\n",
    "        # cleanup\n",
    "        del learn; torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arch</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>model</th>\n",
       "      <th>result</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert</td>\n",
       "      <td>AlbertTokenizer</td>\n",
       "      <td>AlbertForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bart</td>\n",
       "      <td>BartTokenizer</td>\n",
       "      <td>BartForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bert</td>\n",
       "      <td>BertTokenizer</td>\n",
       "      <td>BertForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>camembert</td>\n",
       "      <td>CamembertTokenizer</td>\n",
       "      <td>CamembertForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>distilbert</td>\n",
       "      <td>DistilBertTokenizer</td>\n",
       "      <td>DistilBertForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>electra</td>\n",
       "      <td>ElectraTokenizer</td>\n",
       "      <td>ElectraForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>flaubert</td>\n",
       "      <td>FlaubertTokenizer</td>\n",
       "      <td>FlaubertForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>longformer</td>\n",
       "      <td>LongformerTokenizer</td>\n",
       "      <td>LongformerForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mobilebert</td>\n",
       "      <td>MobileBertTokenizer</td>\n",
       "      <td>MobileBertForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>roberta</td>\n",
       "      <td>RobertaTokenizer</td>\n",
       "      <td>RobertaForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>xlm</td>\n",
       "      <td>XLMTokenizer</td>\n",
       "      <td>XLMForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>xlm_roberta</td>\n",
       "      <td>XLMRobertaTokenizer</td>\n",
       "      <td>XLMRobertaForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>xlnet</td>\n",
       "      <td>XLNetTokenizer</td>\n",
       "      <td>XLNetForSequenceClassification</td>\n",
       "      <td>PASSED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "#hide_input\n",
    "test_results_df = pd.DataFrame(test_results, columns=['arch', 'tokenizer', 'model', 'result', 'error'])\n",
    "display_df(test_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Multi-label classification\n",
    "\n",
    "Below demonstrates how to setup your `blurr` pipeline for a multi-label classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18049"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creates a dataset with the first 10% of training set\n",
    "raw_data = nlp.load_dataset('civil_comments', split='train[:1%]') \n",
    "len(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>sexual_explicit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is such an urgent design problem; kudos to you for taking it on. Very impressive!</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is this something I'll be able to install on my site? When will you be releasing it?</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.87234</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                 text  \\\n",
       "0               This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!   \n",
       "1  Thank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!   \n",
       "2                              This is such an urgent design problem; kudos to you for taking it on. Very impressive!   \n",
       "3                                Is this something I'll be able to install on my site? When will you be releasing it?   \n",
       "4                                                                                haha you guys are a bunch of losers.   \n",
       "\n",
       "   toxicity  severe_toxicity  obscene  threat   insult  identity_attack  \\\n",
       "0  0.000000         0.000000      0.0     0.0  0.00000         0.000000   \n",
       "1  0.000000         0.000000      0.0     0.0  0.00000         0.000000   \n",
       "2  0.000000         0.000000      0.0     0.0  0.00000         0.000000   \n",
       "3  0.000000         0.000000      0.0     0.0  0.00000         0.000000   \n",
       "4  0.893617         0.021277      0.0     0.0  0.87234         0.021277   \n",
       "\n",
       "   sexual_explicit  \n",
       "0              0.0  \n",
       "1              0.0  \n",
       "2              0.0  \n",
       "3              0.0  \n",
       "4              0.0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_df = pd.DataFrame(raw_data, columns=list(raw_data.features.keys()))\n",
    "toxic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['severe_toxicity',\n",
       " 'obscene',\n",
       " 'threat',\n",
       " 'insult',\n",
       " 'identity_attack',\n",
       " 'sexual_explicit']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_cols = list(toxic_df.columns[2:]); lbl_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>sexual_explicit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is such an urgent design problem; kudos to you for taking it on. Very impressive!</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is this something I'll be able to install on my site? When will you be releasing it?</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                 text  \\\n",
       "0               This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!   \n",
       "1  Thank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!   \n",
       "2                              This is such an urgent design problem; kudos to you for taking it on. Very impressive!   \n",
       "3                                Is this something I'll be able to install on my site? When will you be releasing it?   \n",
       "4                                                                                haha you guys are a bunch of losers.   \n",
       "\n",
       "   toxicity  severe_toxicity  obscene  threat  insult  identity_attack  \\\n",
       "0  0.000000                0        0       0       0                0   \n",
       "1  0.000000                0        0       0       0                0   \n",
       "2  0.000000                0        0       0       0                0   \n",
       "3  0.000000                0        0       0       0                0   \n",
       "4  0.893617                0        0       0       1                0   \n",
       "\n",
       "   sexual_explicit  \n",
       "0                0  \n",
       "1                0  \n",
       "2                0  \n",
       "3                0  \n",
       "4                0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_df = toxic_df.round({col: 0 for col in lbl_cols})\n",
    "toxic_df = toxic_df.convert_dtypes()\n",
    "\n",
    "toxic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#hide_output\n",
    "task = HF_TASKS_AUTO.SequenceClassification\n",
    "\n",
    "pretrained_model_name = \"roberta-base\" # \"distilbert-base-uncased\" \"bert-base-uncased\"\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "config.num_labels = len(lbl_cols)\n",
    "\n",
    "hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(pretrained_model_name, \n",
    "                                                                               task=task, \n",
    "                                                                               config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we have to configure the `num_labels` to the number of labels we are predicting. Given that our labels are already encoded, we use a `MultiCategoryBlock` with `encoded=True` and `vocab` equal to the columns with our 1's and 0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single input\n",
    "blocks = (\n",
    "    HF_TextBlock(hf_arch=hf_arch, hf_tokenizer=hf_tokenizer), \n",
    "    MultiCategoryBlock(encoded=True, vocab=lbl_cols)\n",
    ")\n",
    "\n",
    "dblock = DataBlock(blocks=blocks, \n",
    "                   get_x=ColReader('text'), get_y=ColReader(lbl_cols), \n",
    "                   splitter=RandomSplitter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(toxic_df, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([4, 391]), torch.Size([4, 6]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dls.one_batch()\n",
    "len(b), b[0]['input_ids'].shape, b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>None</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Predatory patrol towing isn't a big subject, and there is no advocacy group that is paying any attention to it, but the City of Portland has completely backed off of enforcing state law where the towing predators are operating on private property, and this is Commissioner Novick's failure. He's in charge of towing.\\n\\nThe City has allowed Retriever Towing to operate in open violation of ADA for years at their NW Quimby lot, and there is absolutely no provision in city ordinance that takes into account when they tow a mobility-disabled person's vehicle.  Wheelchair or no wheelchair, the person has to get themselves to the tow yard (which does not have wheelchair access).\\n\\nLed by Senator Avel Gordly, the legislature enacted important new citizen protections against predatory towing effective January 1, 2008, but there is no effective way for Portlanders to learn what their rights are other than what the predatory towers themselves tell you. A description of the protections has never appeared on the City website. The version posted there has never been correct.\\n\\nIt is only in the past two years the the City began enforcing the signage requirements, and the City Towing Coordinator worked with the towers to pass the costs onto the citizens. $ 20 of every tow bill goes to the towers to \"compensate\" them for their signs, with no upper limit, and your $ 20 is likely to be paying for signs completely unrelated to the place your vehicle was towed from.\\n\\nPortland towers are allowed to operate in open violation of state law and the City's own statutes, providing towing services to property owners for free. That is specifically prohibited by state law and City ordinance.\\n\\nSenator Gordly's legislation was intended to protect people living in apartment complexes, generally low-income, minority populations, and they have no one standing up for them in City Hall. I hope you are that champion.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lynn: According to the studies I have read, about 6% of the homeless are \"homeless by choice.\"  You're quite right, this is a very small percentage given the hundreds of thousands of homeless across our country.  You and I have no debate regarding the need for assistance to those 94%.  We have no debate about the need for \"Housing First\" programs to assist them in securing safe shelter as a first step toward working their way out of homelessness.  \\n\\nHowever, that 6% is very much worth talking about.  It fact it's important, first, to point out that it IS 6% and those who cavalierly dismiss all homeless individuals as \\n\"freeloading parasites\" are very wrongheaded in doing so.  \\n\\nOn the other hand, when you include those \"travelers\" (that's the title very commonly used to refer to the voluntarily homeless) in the broad category of \"homeless and deserving our support\", I feel you seriously weaken your case for such assistance.\\n\\nWe see this particular issue very differently.  regards, Gary</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(dataloaders=dls, max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HF_BaseModelWrapper(hf_model)\n",
    "\n",
    "learn = Learner(dls, \n",
    "                model,\n",
    "                opt_func=partial(Adam),\n",
    "                loss_func=BCEWithLogitsLossFlat(),\n",
    "                metrics=[partial(accuracy_multi, thresh=0.2)],\n",
    "                cbs=[HF_BaseModelCallback],\n",
    "                splitter=hf_splitter)\n",
    "\n",
    "learn.loss_func.thresh = 0.2\n",
    "learn.create_opt()             # -> will create your layer groups based on your \"splitter\" function\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're doing multi-label classification, we adjust our loss function to use binary cross-entropy and our metrics to use the multi-label friendly version of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(b[0])\n",
    "preds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(lr_min=0.00831763744354248, lr_steep=0.0014454397605732083)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.lr_find(suggestions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_multi</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.045857</td>\n",
       "      <td>0.037238</td>\n",
       "      <td>0.992657</td>\n",
       "      <td>02:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.021411</td>\n",
       "      <td>0.040381</td>\n",
       "      <td>0.992657</td>\n",
       "      <td>02:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.025079</td>\n",
       "      <td>0.036547</td>\n",
       "      <td>0.992657</td>\n",
       "      <td>02:54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "learn.fit_one_cycle(3, lr_max=3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>None</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I understand wanting to cut rather than tax, as I'm sure it's just human nature to not want to have to pay for something when you otherwise don't have to. I think that most people would agree that there is waste in state government, just as there is in any organization of its same size and complexity. With that being said, I don't understand how so many people are drawing a \"hard line\" on only doing cuts. I personally see the problem as being a lack of taxes. Surely you would agree that it's kind of absurd to live in a state that charges no sales tax (at the state level), no income tax, and in fact even pays you 2k for each man, woman, and child in your home. Even though I feel that this is largely a revenue problem, I wouldn't have any beef with a smart compromise, that matches budget cuts with new revenue. Whatever happened to reasonable people coming together and compromising? The Governor is doing the best he can with being dealt an awful hand, and everyone seems to just trash him.</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Part III]\\n\\nSection 1. To thank Councilor Evans, along with Councilors Zelenka, Syrett, and Pryor, and Mayor Piercy for their votes on November 9 in favor of responsible planning that is both respectful to the concerns of neighbors and reflective of Oregon Statewide Planning Goals.\\n\\nSection 2. To encourage Eugene City Councilors who are members of the Democratic Party of Lane County to consider the environmental and equity ramifications that would result from Sections 2 &amp; 3 of the October 21 motion. \\n\\nSection 3. To work with community partnersâ€”such as the Eugene Area Chamber of Commerce, 1000 Friends of Oregon, the planning commission, and the Eugene Sustainability Commissionâ€”to advocate for responsible planning goals and outcomes.\\n\\nSection 4. To mail this Resolution to Mayor Piercy, Councilor Brown, Councilor Taylor, Councilor Zelenka, Councilor Poling, Councilor Evans, Councilor Syrett, Councilor Pryor, and City Manager Ruiz.\\n\\nRslved by Central Comte of DPLC.\\n\\nSigned:\\nChris Wig\\nChair</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results(learner=learn, max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.loss_func.thresh = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((#1) ['severe_toxicity'],\n",
       " tensor([False, False, False,  True, False, False]),\n",
       " tensor([3.9881e-06, 6.2288e-03, 4.5700e-04, 3.2172e-02, 2.8085e-03, 1.4909e-03]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment = \"\"\"\n",
    "Those damned affluent white people should only eat their own food, like cod cakes and boiled potatoes. \n",
    "No enchiladas for them!\n",
    "\"\"\"\n",
    "learn.blurr_predict(comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_data-core.ipynb.\n",
      "Converted 01a_data-token-classification.ipynb.\n",
      "Converted 01b_data-question-answering.ipynb.\n",
      "Converted 01e_data-summarization.ipynb.\n",
      "Converted 01z_data-language-modeling.ipynb.\n",
      "Converted 02_modeling-core.ipynb.\n",
      "Converted 02a_modeling-token-classification.ipynb.\n",
      "Converted 02b_modeling-question-answering.ipynb.\n",
      "Converted 02e_modeling-summarization.ipynb.\n",
      "Converted 02z_modeling-language-modeling.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

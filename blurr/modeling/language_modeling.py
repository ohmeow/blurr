# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02_modeling-language-modeling.ipynb (unless otherwise specified).

__all__ = ['LM_MetricsCallback', 'BlearnerForLM']

# Cell
import ast

import torch
from transformers import *
from fastai.text.all import *
from sklearn.metrics import accuracy_score

from ..utils import *
from ..data.core import *
from ..data.language_modeling import *
from .core import *

# Cell
class LM_MetricsCallback(Callback):
    """A fastai friendly metric implemented as a callback so that we can handle use cases where we don't
    want to count tokens marked to be ignored or else not count batches where there are no targs
    """
    def __init__(self, **kwargs):
        self.run_before = Recorder

        self.custom_metrics_dict = { 'lm_accuracy':None }
        self.do_setup = True

    def setup(self):
        # one time setup code here.
        if (not self.do_setup): return

        # add custom text generation specific metrics
        custom_metric_keys = self.custom_metrics_dict.keys()
        custom_metrics = L([ ValueMetric(partial(self.metric_value, metric_key=k), k) for k in custom_metric_keys ])
        self.learn.metrics = self.learn.metrics + custom_metrics

        self.do_setup = False

    def before_fit(self):
        self.setup()


    # --- batch begin/after phases ---
    def after_batch(self):
        # do this only for validation set
        if (self.training or self.learn.y is None): return

        preds = self.pred.argmax(dim=-1)
        targs = self.yb[0]               # yb is TensorText tuple, item 0 is the data

        msk = torch.where(targs != -100, 1, 0).bool()
        preds = torch.masked_select(preds, msk).cpu()
        targs = torch.masked_select(targs, msk).cpu()

        if (preds.shape[0] == 0):
            return

        self.results += [ (res[0], res[1]) for res in zip(preds, targs) ]


    # --- validation begin/after phases ---
    def before_validate(self): self.results = []

    def after_validate(self):
        if (len(self.results) < 1): return

        preds, targs = map(list, zip(*self.results))
        self.custom_metrics_dict['lm_accuracy'] = accuracy_score(targs, preds)


    # --- for ValueMetric metrics ---
    def metric_value(self, metric_key): return self.custom_metrics_dict[metric_key]

# Cell
@typedispatch
def show_results(x:HF_CausalLMInput, y, samples, outs, learner, ctxs=None, max_n=6, trunc_at=None, **kwargs):
    # grab our tokenizer and ignore token to decode
    hf_before_batch_tfm = get_blurr_tfm(learner.dls.before_batch)
    hf_config = hf_before_batch_tfm.hf_config
    hf_tokenizer = hf_before_batch_tfm.hf_tokenizer
    ignore_token_id = hf_before_batch_tfm.ignore_token_id

    res = L([(
        hf_tokenizer.decode(s[0], skip_special_tokens=True)[:trunc_at],
        hf_tokenizer.decode(s[1][s[1] != ignore_token_id], skip_special_tokens=True)[:trunc_at],
        hf_tokenizer.decode(pred[0], skip_special_tokens=True)[:trunc_at],
    ) for s, pred in zip(samples, outs) ])

    display_df(pd.DataFrame(res, columns=['text', 'target', 'prediction'])[:max_n])
    return ctxs

# Cell
@typedispatch
def show_results(x:HF_MLMInput, y, samples, outs, learner, ctxs=None, max_n=6, trunc_at=None, **kwargs):
    # grab our tokenizer and ignore token to decode
    hf_before_batch_tfm = get_blurr_tfm(learner.dls.before_batch)
    hf_config = hf_before_batch_tfm.hf_config
    hf_tokenizer = hf_before_batch_tfm.hf_tokenizer
    ignore_token_id = hf_before_batch_tfm.ignore_token_id

    # grab our mask token id and do-not-mask token ids
    mask_token_id = hf_tokenizer.mask_token_id
    dnm_tok_ids = hf_before_batch_tfm.lm_strategy.dnm_tok_ids

    res = L()
    for s, t in zip(samples, outs):
        # exclue dnm tokens from input
        inps = [ hf_tokenizer.decode(tok_id)
                if (tok_id == mask_token_id or s[1][idx] == ignore_token_id)
                else f'[{hf_tokenizer.decode(tok_id)}]'
                for idx, tok_id in enumerate(s[0]) if (tok_id not in dnm_tok_ids) ]

        # replaced masked tokens with "[{actual_token}]"
        trgs = [ hf_tokenizer.decode(s[0][idx])
                if (tok_id == ignore_token_id)
                else f'[{hf_tokenizer.decode(tok_id)}]'
                for idx, tok_id in enumerate(s[1]) if (s[0][idx] not in dnm_tok_ids) ]

        # same as above except we replace the [MASK] with the PREDICTED token
        preds = [ hf_tokenizer.decode(s[0][idx])
                 if (tok_id == ignore_token_id)
                 else f'[{hf_tokenizer.decode(t[0][idx])}]'
                 for idx, tok_id in enumerate(s[1]) if (s[0][idx] not in dnm_tok_ids) ]

        res.append((' '.join(inps[:trunc_at]).strip(),
                    ' '.join(trgs[:trunc_at]).strip(),
                    ' '.join(preds[:trunc_at]).strip()))

    display_df(pd.DataFrame(res, columns=['text', 'target', 'prediction'])[:max_n])
    return ctxs

# Cell
@patch
def blurr_fill_mask(self:Learner, inp, n_preds=1, **kwargs):
    """For MLM models"""
    # grab the Hugging Face tokenizer from the learner's dls.tfms
    hf_before_batch_tfm = get_blurr_tfm(self.dls.before_batch)
    hf_config = hf_before_batch_tfm.hf_config
    hf_tokenizer = hf_before_batch_tfm.hf_tokenizer
    tok_kwargs = hf_before_batch_tfm.tok_kwargs

    # grab the text generation kwargs
    text_gen_kwargs = hf_before_batch_tfm.text_gen_kwargs if (len(kwargs) == 0) else kwargs

    if (isinstance(inp, str)):
        input_ids = hf_tokenizer.encode(inp, padding=True, truncation=True, return_tensors='pt', **tok_kwargs)
    else:
        # note (10/30/2020): as of pytorch 1.7, this has to be a plain ol tensor (not a subclass of TensorBase)
        input_ids = inp.as_subclass(Tensor)

    input_ids = input_ids.to(self.model.hf_model.device)
    mask_token_index = torch.where(input_ids == hf_tokenizer.mask_token_id)[1]

    outputs = self.model.hf_model(input_ids)
    mask_token_logits = outputs.logits[0, mask_token_index, :]
    preds = torch.topk(mask_token_logits, n_preds, dim=-1).indices[0].tolist()

    outputs = [ inp.replace(hf_tokenizer.mask_token, hf_tokenizer.decode([tok_id]).strip())
               for tok_id in preds ]

    return outputs

# Cell
@delegates(Blearner.__init__)
class BlearnerForLM(Blearner):

    def __init__(self, dls, hf_model, **kwargs):
        kwargs['loss_func'] = HF_PreCalculatedLoss()
        super().__init__(dls, hf_model, **kwargs)

    @classmethod
    def get_model_cls(self, lm_type):
        return AutoModelForCausalLM if (lm_type == LMType.CAUSAL) else AutoModelForMaskedLM

    @classmethod
    def get_metrics_cb(self):
        return LM_MetricsCallback()

    @classmethod
    def _create_learner(cls, data, pretrained_model_name_or_path, preprocess_func,
                        lm_strategy_cls, text_attr, dblock_splitter, dl_kwargs, learner_kwargs):

        lm_type = lm_strategy_cls.get_lm_type()

        # get our hf objects
        model_cls = cls.get_model_cls(lm_type=lm_type)
        hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(pretrained_model_name_or_path,
                                                                          model_cls=model_cls)

        # if we need to preprocess the raw data before creating our DataLoaders
        if (preprocess_func):
            data = preprocess_func(data, hf_arch, hf_config, hf_tokenizer, hf_model, lm_type, lm_strategy_cls)

        # not all architectures include a native pad_token (e.g., gpt2, ctrl, etc...), so we add one here
        if (hf_tokenizer.pad_token is None):
            hf_tokenizer.add_special_tokens({'pad_token': '<pad>'})
            hf_config.pad_token_id = hf_tokenizer.get_vocab()['<pad>']
            hf_model.resize_token_embeddings(len(hf_tokenizer))

        # build getters
        if (isinstance(data, pd.DataFrame)):
            get_x = ColReader(text_attr)
        else:
            get_x = ItemGetter(text_attr)

        # build DataBlocks and DataLoaders
        bbtfm = HF_LMBeforeBatchTransform(hf_arch, hf_config, hf_tokenizer, hf_model, lm_strategy_cls=lm_strategy_cls)

        input_return_type = HF_CausalLMInput if (lm_type == LMType.CAUSAL) else HF_MLMInput
        blocks = (HF_TextBlock(before_batch_tfm=bbtfm, input_return_type=input_return_type), noop)

        dblock = DataBlock(blocks=blocks, get_x=get_x, splitter=dblock_splitter)
        dls = dblock.dataloaders(data, **dl_kwargs.copy())

        # return BLearner instance with default metrics (optional)
        learner_kwargs['metrics'] = learner_kwargs.pop('metrics', [perplexity])
        return cls(dls, hf_model, **learner_kwargs.copy())

    @classmethod
    def from_dataframe(cls, df, pretrained_model_name_or_path, preprocess_func=None,
                       lm_strategy_cls=CausalLMStrategy, text_attr='text', dblock_splitter=ColSplitter(),
                       dl_kwargs={}, learner_kwargs={}):

        return cls._create_learner(df, pretrained_model_name_or_path, preprocess_func,
                                   lm_strategy_cls, text_attr, dblock_splitter, dl_kwargs, learner_kwargs)


    @classmethod
    def from_csv(cls, csv_file, pretrained_model_name_or_path, preprocess_func=None,
                 lm_strategy_cls=CausalLMStrategy, text_attr='text', dblock_splitter=ColSplitter(),
                 dl_kwargs={}, learner_kwargs={}):

        df = pd.read_csv(csv_file)
        return cls.from_dataframe(df, pretrained_model_name_or_path, preprocess_func,
                                  lm_strategy_cls, text_attr, dblock_splitter,
                                  dl_kwargs, learner_kwargs)

    @classmethod
    def from_dictionaries(cls, ds, pretrained_model_name_or_path, preprocess_func=None,
                          lm_strategy_cls=CausalLMStrategy, text_attr='text', dblock_splitter=RandomSplitter(),
                          dl_kwargs={}, learner_kwargs={}):

        return cls._create_learner(ds, pretrained_model_name_or_path, preprocess_func,
                                   lm_strategy_cls, text_attr, dblock_splitter,
                                   dl_kwargs, learner_kwargs)
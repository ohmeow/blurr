# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02d_modeling-token-classification.ipynb (unless otherwise specified).

__all__ = []

# Cell
import ast, torch
from transformers import *
from fastai2.text.all import *

from ..data.all import *
from .core import *

# Cell
@typedispatch
def show_results(x:HF_TokenClassInput, y:HF_TokenTensorCategory, samples, outs, hf_tokenizer, skip_special_tokens=True,
                 ctxs=None, max_n=6, **kwargs):
    res = L()
    for inp, trg, sample, pred in zip(x[0], y, samples, outs):
        inp_trg_preds = [ (hf_tokenizer.ids_to_tokens[tok_id.item()], lbl_id.item(), pred_lbl)
                         for tok_id, lbl_id, pred_lbl in zip(inp, trg, ast.literal_eval(pred[0]))
                         if (tok_id not in hf_tokenizer.all_special_ids) and lbl_id != -100 ]

        res.append(f'{[ (itp[0], lbl, itp[2]) for itp, lbl in zip(inp_trg_preds, ast.literal_eval(sample[1])) ]}')

    display_df(pd.DataFrame(res, columns=['token / target label / predicted label'])[:max_n])
    return ctxs

# Cell
@patch
def predict_tokens(self:Learner, inp, **kargs):
    """Remove all the unnecessary predicted tokens after calling `Learner.predict`, so that you only
    get the predicted labels, label ids, and probabilities for what you passed into it in addition to the input
    """
    pred_lbls, pred_lbl_ids, probs = self.predict(inp)

    # grab the huggingface tokenizer from the learner's dls.tfms
    hf_textblock_tfm = self.dls.tfms[0]
    hf_tokenizer = hf_textblock_tfm.hf_tokenizer
    add_prefix_space = hf_textblock_tfm.add_prefix_space

    # grab the HF_BatchTransform as well
    learn_hf_batch_transform = learn.dls.before_batch.hf__batch_transform

    # calculate the number of subtokens per raw/input token so that we can determine what predictions to
    # return
    subtoks_per_raw_tok = [ (entity, len(hf_tokenizer.tokenize(str(entity), add_prefix_space=add_prefix_space)))
                           for entity in inp ]

    # very similar to what HF_BatchTransform does with the exception that we are also grabbing
    # the `special_tokens_mask` to help with getting rid or irelevant predicts for any special tokens
    # (e.g., [CLS], [SEP], etc...)
    txt_toks = [ sub_toks for entity in inp
                for sub_toks in hf_tokenizer.tokenize(entity, add_prefix_space=add_prefix_space) ]

    txt_tok_ids = hf_tokenizer.convert_tokens_to_ids(txt_toks)

    res = hf_tokenizer.prepare_for_model(txt_tok_ids, None,
                                         max_length=learn_hf_batch_transform.max_seq_len,
                                         pad_to_max_length=True,
                                         truncation_strategy=None,
                                         return_special_tokens_mask=True)

    special_toks_msk = L(res['special_tokens_mask'])
    actual_tok_idxs = special_toks_msk.argwhere(lambda el: el != 1)

    # using the indexes to the actual tokens, get that info from the results returned above
    pred_lbls_list = ast.literal_eval(pred_lbls)
    actual_pred_lbls = L(pred_lbls_list)[actual_tok_idxs]
    actual_pred_lbl_ids = pred_lbl_ids[actual_tok_idxs]
    actual_probs = probs[actual_tok_idxs]

    # now, because a raw token can be mapped to multiple subtokens, we need to build a list of indexes composed
    # of the *first* subtoken used to represent each raw token (that is where the prediction is)
    offset = 0
    raw_trg_idxs = []
    for idx, (raw_tok, sub_tok_count) in enumerate(subtoks_per_raw_tok):
        raw_trg_idxs.append(idx+offset)
        offset += sub_tok_count-1 if (sub_tok_count > 1) else 0

    return inp, actual_pred_lbls[raw_trg_idxs], actual_pred_lbl_ids[raw_trg_idxs], actual_probs[raw_trg_idxs]
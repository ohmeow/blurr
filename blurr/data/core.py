# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01_data-core.ipynb (unless otherwise specified).

__all__ = ['HF_TokenizerTransform', 'HF_BaseInput', 'HF_BatchTransform', 'HF_TextBlock']

# Cell
from functools import reduce

import torch, nlp
from transformers import *
from fastai2.text.all import *

from ..utils import *

# Cell
class HF_TokenizerTransform(ItemTransform):
    """huggingface friendly tokenization transfor."""
    def __init__(self, hf_arch, hf_tokenizer, max_length=None, padding='max_length', truncation=True,
                 is_pretokenized=False, tok_kwargs={}):

        # gpt2, roberta, bart (and maybe others) tokenizers require a prefix space
        if (hasattr(hf_tokenizer, 'add_prefix_space')): tok_kwargs['add_prefix_space'] = True

        store_attr(self, 'hf_arch, hf_tokenizer, is_pretokenized, max_length, padding, truncation, tok_kwargs')

    def encodes(self, inp):
        """Supports both string and list[str] inputs (the later is common for token classification tasks).
        Returns the numericalized (token_ids) of the input so no need to run this through a Numericalization
        transform."""
        inps = [inp, None] if (isinstance(inp, str) or self.is_pretokenized) else inp

        res = self.hf_tokenizer(inps[0], inps[1],
                                max_length=self.max_length,
                                padding=self.padding,
                                truncation=self.truncation,
                                is_pretokenized=self.is_pretokenized,
                                return_tensors='pt',
                                **self.tok_kwargs)

        for k in res.keys(): res[k] = res[k].squeeze(0)
        return res

    def decodes(self, encoded_inp):
        """This will get called multiple times for a given encoded input because our batch transform will add
        other elements to it (e.g., attention_mask, token_type_ids, etc...) as required by the defined huggingface
        tokenizer and model.  If it can't decode it, return None."""
        input_ids = filter(lambda el: el != self.hf_tokenizer.pad_token_id, encoded_inp[0].cpu().numpy())
        decoded_input = self.hf_tokenizer.decode(input_ids, skip_special_tokens=True)
        return TitledStr(decoded_input)


# Cell
class HF_BaseInput(list): pass

# Cell
class HF_BatchTransform(Transform):
    """Handles everything you need to assemble a mini-batch of inputs and targets"""
    def __init__(self, hf_arch, hf_tokenizer, hf_input_return_type=HF_BaseInput, **kwargs):
        store_attr(self, 'hf_arch, hf_tokenizer, hf_input_return_type, kwargs')

    def encodes(self, samples): return samples

    def decodes(self, encoded_samples):
        if (isinstance(encoded_samples, dict)): return self.hf_input_return_type([encoded_samples['input_ids']])
        return encoded_samples

# Cell
class HF_TextBlock(TransformBlock):
    def __init__(self, hf_arch, hf_tokenizer,
                 hf_tok_tfm=None, max_length=512, padding='max_length', truncation=True, is_pretokenized=False,
                 hf_batch_tfm=None, hf_input_return_type=HF_BaseInput,
                 dl_type = SortedDL, **tok_kwargs):

        if (hf_tok_tfm is None):
            hf_tok_tfm = HF_TokenizerTransform(hf_arch, hf_tokenizer, max_length,
                                               padding, truncation, is_pretokenized, **tok_kwargs)

        if (hf_batch_tfm is None):
            hf_batch_tfm = HF_BatchTransform(hf_arch, hf_tokenizer, hf_input_return_type)

        return super().__init__(type_tfms=hf_tok_tfm, dl_type=dl_type, dls_kwargs={ 'before_batch': hf_batch_tfm })

# Cell
@typedispatch
def show_batch(x:HF_BaseInput, y, samples, hf_tokenizer=None, ctxs=None, max_n=6, **kwargs):
    if ctxs is None: ctxs = get_empty_df(min(len(samples), max_n))
    ctxs = show_batch[object](x, y, samples, max_n=max_n, ctxs=ctxs, **kwargs)

    display_df(pd.DataFrame(ctxs))
    return ctxs
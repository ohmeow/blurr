# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01d_data-token-classification.ipynb (unless otherwise specified).

__all__ = ['HF_TokenTensorCategory', 'HF_TokenCategorize', 'HF_TokenCategoryBlock', 'HF_TokenClassInput',
           'HF_TokenClassBatchTransform']

# Cell
import ast
from functools import reduce

import torch
from transformers import *
from fastai2.text.all import *

from ..utils import *
from .core import *

# Cell
class HF_TokenTensorCategory(TensorBase): pass

# Cell
class HF_TokenCategorize(Transform):
    "Reversible transform of a list of category string to `vocab` id"

    def __init__(self, vocab=None, ignore_token=None, ignore_token_id=None):
        self.vocab = None if vocab is None else CategoryMap(vocab)
        self.ignore_token = '[xIGNx]' if ignore_token is None else ignore_token
        self.ignore_token_id = CrossEntropyLossFlat().ignore_index if ignore_token_id is None else ignore_token_id

        self.loss_func, self.order = CrossEntropyLossFlat(ignore_index=self.ignore_token_id), 1

    def setups(self, dsets):
        if self.vocab is None and dsets is not None: self.vocab = CategoryMap(dsets)
        self.c = len(self.vocab)

    def encodes(self, labels):
        ids = [[self.vocab.o2i[lbl]] + [self.ignore_token_id]*(n_subtoks-1) for lbl, n_subtoks in labels]
        return HF_TokenTensorCategory(reduce(operator.concat, ids))

    def decodes(self, encoded_labels):
        return Category([(self.vocab[lbl_id]) for lbl_id in encoded_labels if lbl_id != self.ignore_token_id ])

# Cell
def HF_TokenCategoryBlock(vocab=None, ignore_token=None, ignore_token_id=None):
    "`TransformBlock` for single-label categorical targets"
    return TransformBlock(type_tfms=HF_TokenCategorize(vocab=vocab,
                                                       ignore_token=ignore_token,
                                                       ignore_token_id=ignore_token_id))

# Cell
class HF_TokenClassInput(list): pass

# Cell
class HF_TokenClassBatchTransform(HF_BatchTransform):
    def __init__(self, hf_arch, hf_tokenizer, **kwargs):
        super().__init__(hf_arch, hf_tokenizer, HF_TokenClassInput, **kwargs)

    def encodes(self, samples):
        samples = super().encodes(samples)
        if (len(samples[0]) == 1): return samples

        target_cls = type(samples[0][1])
        updated_samples = []

        # we assume that first target = the categories we want to predict for each token
        for s in samples:
            idx_first_input_id = s[0]['special_tokens_mask'].tolist().index(0)
            targ_ids = target_cls([ el*-100 if (el == 1) else s[1][idx-idx_first_input_id].item()
                                   for idx, el in enumerate(s[0]['special_tokens_mask']) ])
            updated_samples.append((s[0], targ_ids))

        return updated_samples

# Cell
@typedispatch
def show_batch(x:HF_TokenClassInput, y, samples, dataloaders=None, ctxs=None, max_n=6, **kwargs):
    hf_tokenizer = dataloaders.valid.hf_tokenizer

    res = L()
    for inp, trg, sample in zip(x[0], y, samples):
        inp_targs = [ (hf_tokenizer.ids_to_tokens[tok_id.item()], lbl_id.item())
                     for tok_id, lbl_id in zip(inp, trg)
                     if (tok_id.item() not in hf_tokenizer.all_special_ids) and lbl_id != -100 ]

        res.append([f'{[ (inp_trg[0], lbl) for inp_trg, lbl in zip(inp_targs, ast.literal_eval(sample[1])) ]}'])

    display_df(pd.DataFrame(res, columns=['token / target label'])[:max_n])
    return ctxs
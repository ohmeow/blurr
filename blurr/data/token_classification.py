# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/03_data-token-classification.ipynb (unless otherwise specified).

__all__ = ['HF_TokenTensorCategory', 'HF_TokenCategorize', 'HF_TokenCategoryBlock', 'HF_TokenClassInput',
           'HF_TokenClassBeforeBatchTransform']

# Cell
import ast
from functools import reduce

import torch
from transformers import *
from fastai.text.all import *

from ..utils import *
from .core import *

logging.set_verbosity_error()

# Cell
class HF_TokenTensorCategory(TensorBase): pass

# Cell
class HF_TokenCategorize(Transform):
    "Reversible transform of a list of category string to `vocab` id"

    def __init__(self, vocab=None, ignore_token=None, ignore_token_id=None):
        self.vocab = None if vocab is None else CategoryMap(vocab)
        self.ignore_token = '[xIGNx]' if ignore_token is None else ignore_token
        self.ignore_token_id = CrossEntropyLossFlat().ignore_index if ignore_token_id is None else ignore_token_id

        self.loss_func, self.order = CrossEntropyLossFlat(ignore_index=self.ignore_token_id), 1

    def setups(self, dsets):
        if self.vocab is None and dsets is not None: self.vocab = CategoryMap(dsets)
        self.c = len(self.vocab)

    def encodes(self, labels):
        ids = [[self.vocab.o2i[lbl]] + [self.ignore_token_id]*(n_subtoks-1) for lbl, n_subtoks in labels]
        return HF_TokenTensorCategory(reduce(operator.concat, ids))

    def decodes(self, encoded_labels):
        return Category([(self.vocab[lbl_id]) for lbl_id in encoded_labels if lbl_id != self.ignore_token_id ])

# Cell
def HF_TokenCategoryBlock(vocab=None, ignore_token=None, ignore_token_id=None):
    "`TransformBlock` for single-label categorical targets"
    return TransformBlock(type_tfms=HF_TokenCategorize(vocab=vocab,
                                                       ignore_token=ignore_token,
                                                       ignore_token_id=ignore_token_id))

# Cell
class HF_TokenClassInput(HF_BaseInput): pass

# Cell
class HF_TokenClassBeforeBatchTransform(HF_BeforeBatchTransform):
    def __init__(self, hf_arch, hf_config, hf_tokenizer, hf_model,
                 ignore_token_id=CrossEntropyLossFlat().ignore_index,
                 max_length=None, padding=True, truncation=True, is_split_into_words=True,
                 tok_kwargs={}, **kwargs):

        super().__init__(hf_arch, hf_config, hf_tokenizer, hf_model,
                         max_length=max_length, padding=padding, truncation=truncation,
                         is_split_into_words=is_split_into_words, tok_kwargs=tok_kwargs, **kwargs)

        self.ignore_token_id = ignore_token_id

    def encodes(self, samples):
        samples = super().encodes(samples)
        if (len(samples[0]) == 1): return samples

        target_cls = type(samples[0][1])
        updated_samples = []

        # we assume that first target = the categories we want to predict for each token
        for s in samples:
            targ_len = len(s[1])
            idx_first_input_id = s[0]['special_tokens_mask'].tolist().index(0)
            targ_ids = target_cls([ self.ignore_token_id if (el == 1 or idx > targ_len)
                                   else s[1][idx-idx_first_input_id].item()
                                   for idx, el in enumerate(s[0]['special_tokens_mask']) ])

            updated_samples.append((s[0], targ_ids))

        return updated_samples

# Cell
@typedispatch
def show_batch(x:HF_TokenClassInput, y, samples, dataloaders, ctxs=None, max_n=6, trunc_at=None, **kwargs):
    # grab our tokenizer
    hf_before_batch_tfm = get_blurr_tfm(dataloaders.before_batch)
    hf_tokenizer = hf_before_batch_tfm.hf_tokenizer

    res = L()
    for inp, trg, sample in zip(x, y, samples):
        # recontstruct the string and split on space to get back your pre-tokenized list of tokens
        toks = hf_tokenizer.convert_ids_to_tokens(inp, skip_special_tokens=True)
        pretokenized_toks =  hf_tokenizer.convert_tokens_to_string(toks).split()

        res.append([f'{[ (tok, lbl) for idx, (tok, lbl) in enumerate(zip(pretokenized_toks, ast.literal_eval(sample[1]))) if (trunc_at is None or idx < trunc_at) ]}'])

    display_df(pd.DataFrame(res, columns=['token / target label'])[:max_n])
    return ctxs